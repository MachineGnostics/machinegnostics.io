{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Gnostics","text":"<p>Welcome to <code>Machine Gnostics</code>, an innovative Python library designed to implement the principles of Mathematical Gnostics for robust data analysis, modeling, and inference. Unlike traditional statistical approaches that depend heavily on probabilistic assumptions, Machine Gnostics harnesses deterministic algebraic and geometric structures. This unique foundation enables the library to deliver exceptional resilience against outliers, noise, and corrupted data, making it a powerful tool for challenging real-world scenarios.</p> <p> </p> <p>Machine Gnostics</p> <p>Laws of Nature, Encoded\u2014For Everyone!</p> <pre><code>graph LR\n    Entropy[\"Entropy\"]\n    Curvature[\"Space Curvature\"]\n    Bounds[\"[0, \u221e] Bounds\"]\n    Laws[\"Laws of Nature\"]\n    MG[\"Machine Gnostics\"]\n    AI[\"**AI** built with 'Laws of Nature'\"]\n    Universe[\"Universe\"]\n\n    Universe --&gt; Entropy\n    Universe --&gt; Curvature\n    Universe --&gt; Bounds\n\n    Entropy --&gt; Laws\n    Curvature --&gt; Laws\n    Bounds --&gt; Laws\n\n    Laws --&gt; MG\n    Laws -.-&gt; AI\n    MG --&gt; AI\n\n    %% Help center Laws visually\n    Entropy -.-&gt; MG\n    Curvature -.-&gt; MG\n    Bounds -.-&gt; MG\n\n    style Universe stroke-width:2px\n    style Laws stroke-width:3px\n    style MG stroke-width:2px\n    style AI stroke-width:2px</code></pre> <p>Machine Gnostics is an open-source initiative that seeks to redefine the mathematical underpinnings of machine learning. While most conventional ML libraries are grounded in probabilistic and statistical frameworks, Machine Gnostics explores alternative paradigms\u2014drawing from deterministic algebra, information theory, and geometric methods. This approach opens new avenues for building robust, interpretable, and reliable analysis tools that can withstand the limitations of traditional models.</p> <p>Machine Gnostics</p> <p>As a pioneering project, Machine Gnostics invites users to adopt a fresh perspective and develop a new understanding of machine learning. The library is currently in its infancy, and as such, some features may require refinement and fixes. We are actively working to expand its capabilities, with new models and methods planned for the near future. Community support and collaboration are essential to realizing Machine Gnostics\u2019 full potential. Together, let\u2019s build a new AI grounded in a rational and resilient paradigm.</p> <p>Machine Gnostics challenges the limitations of traditional, probabilistic models. Instead of relying on assumptions and large data samples, it encodes the very laws of nature\u2014geometry, physics, entropy\u2014into algorithms that extract truth from data, even when samples are small, noisy, or corrupted.</p>"},{"location":"#data-science-rooted-in-nature","title":"Data Science Rooted in Nature","text":"<p>Machine Gnostics challenges the limitations of traditional, probabilistic models. Instead of relying on assumptions and large data samples, it encodes the very laws of nature\u2014geometry, physics, entropy\u2014into algorithms that extract truth from data, even when samples are small, noisy, or corrupted.</p> <p>\u201cLet data speak for themselves.\u201d Machine Gnostics empowers you to uncover the real structure of your data, free from statistical dogma.</p>"},{"location":"#why-machine-gnostics","title":"Why Machine Gnostics?","text":"<ul> <li>Beyond Statistics: Move past fragile, assumption-heavy models. MG is built for the real world\u2014messy, complex, and unpredictable.</li> <li>Nature-Inspired Algorithms: Deterministic, axiomatic, and robust\u2014rooted in geometry, physics, and information theory.</li> <li>Resilient to Outliers &amp; Noise: Analyze small, corrupted, or outlier-ridden datasets with confidence.</li> <li>Universal &amp; Open:   Free, open-source, and adaptable for science, engineering, and industry.</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li>Advanced Gnostic Data Analysis:   Unlock sophisticated exploratory data analysis (EDA) with algorithms that reveal hidden structures, relationships, and patterns in your data. Designed for data scientists, analysts, and researchers, Machine Gnostics provides tools that go far beyond traditional statistics\u2014enabling deeper, more meaningful insights for both small and complex datasets.</li> <li>Industry-Ready Machine Learning:   Enjoy seamless integration with standard machine learning workflows. Machine Gnostics models support familiar <code>fit</code> and <code>predict</code> methods, making them easy to adopt in any pipeline. With built-in MLflow integration, you can track, version, and deploy models effortlessly\u2014bridging the gap between research and real-world industry applications.</li> <li>Next-Generation Deep Learning (MAGNET):   Prepare for the future with MAGNET (Machine Gnostics Networks), our upcoming deep learning framework. Rooted in the gnostic theorem and the laws of nature, MAGNET will offer a new paradigm for building robust, interpretable neural networks. Stay tuned as we develop this groundbreaking extension to the Machine Gnostics ecosystem.</li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<p>Machine Gnostics encodes the \u201cgnostic cycle\u201d of observation and feedback, letting you model uncertainty as a consequence of real, measurable conditions\u2014not just randomness. See the Concepts page for a deep dive into the science and philosophy behind MG.</p>"},{"location":"#real-world-impact","title":"Real-World Impact","text":"<ul> <li>Testimonials:Hear from scientists and engineers who have solved unsolvable problems with MG.See Testimonials &amp; History.</li> <li>Case Studies:   Explore real applications in thermodynamics, environmental science, and more.   See Examples.</li> </ul>"},{"location":"#get-involved","title":"Get Involved","text":"<p>Machine Gnostics is open source and community-driven.</p> <ul> <li>Contribute: Join us on GitHub.</li> <li>Contact: Connect with the community\u2014see Contact.</li> </ul>"},{"location":"#learn-more","title":"Learn More","text":"<ul> <li>Concept</li> <li>Principles</li> <li>Architecture</li> <li>Gnostic Distribution Functions</li> <li>Glossary</li> <li>References</li> <li>Tutorial</li> </ul>"},{"location":"#license-gnu-v30","title":"License: GNU v3.0","text":""},{"location":"contact/","title":"Contact","text":"<p>Welcome to the Machine Gnostics Community!</p> <p>We\u2019re excited to connect with fellow data enthusiasts, researchers, and learners. Whether you have a question, want to collaborate, or just want to say hello, we\u2019d love to hear from you!</p>"},{"location":"contact/#why-connect-with-us","title":"Why Connect with Us?","text":"<ul> <li>Join our Community: Get access to exclusive tutorials, early updates, and community discussions.</li> <li>Collaborate: Interested in contributing or partnering? Let\u2019s work together on innovative projects.</li> <li>Stay Updated: Subscribe to our newsletter for the latest research, features, and events.</li> </ul>"},{"location":"contact/#how-you-can-contribute","title":"How You Can Contribute","text":"<p>Share your feedback and suggestions, report bugs or request features on GitHub, write tutorials or share your use-cases, or help answer questions in our Discord or LinkedIn groups. Every contribution helps us grow!</p>"},{"location":"contact/#community-spotlight","title":"Community Spotlight","text":"<p>Biweekly, we publish informative articles that break down Machine Gnostics concepts and share them with the data science and research community. We are also open to writing collaborations and new ideas\u2014if you\u2019d like to contribute or suggest a topic, let us know!</p>"},{"location":"contact/#have-a-question-or-idea","title":"Have a Question or Idea?","text":"<p>No question is too small. We're here to help and collaborate \u2014 please reach out using one of the options below:</p>"},{"location":"contact/#social-platforms","title":"Social Platforms","text":"<p>Contact Information</p> <p>Primary Contact:</p> <p>Dr. Nirmal Parmar \ud83d\udce7 info.machinegnostics@gmail.com</p> <p>Community &amp; Social: Discord \u00a0|\u00a0  LinkedIn \u00a0|\u00a0  GitHub \u00a0|\u00a0  Instagram</p>"},{"location":"contact/#subscribe-for-newsletters-latest-updates-and-tutorials","title":"Subscribe for Newsletters, Latest Updates, and Tutorials","text":"<p>Subscribe</p> Your Email: Leave this field empty:          SUBSCRIBE"},{"location":"contact/#lets-connect","title":"Let\u2019s Connect!","text":"<ul> <li>Join our Discord for real-time discussions.</li> <li>Follow us on LinkedIn for professional updates.</li> <li>Star us on GitHub to support the project.</li> <li>Follow us on Instagram for quick updates.</li> </ul> <p>How to Get Support</p> <ul> <li>For issues or feature requests, please open a ticket on our GitHub repository.</li> <li>For consultation, learning sessions, or the latest updates, follow us on LinkedIn or join our GitHub community.</li> <li>For direct inquiries, email us at info.machinegnostics@gmail.com.</li> </ul> <p>We appreciate your interest in Machine Gnostics and look forward to collaborating with researchers, developers, and practitioners passionate about robust and interpretable machine learning.</p> <p>Let's build the future of data science together!</p>"},{"location":"installation/","title":"Installation Guide","text":"<p>Machine Gnostics is distributed as a standard Python package and is designed for easy installation and integration into your data science workflow. The library has been tested on macOS with Python 3.11 and is fully compatible with standard data science libraries.</p>"},{"location":"installation/#1-create-a-python-virtual-environment","title":"1. Create a Python Virtual Environment","text":"<p>It is best practice to use a virtual environment to manage your project dependencies and avoid conflicts with other Python packages.</p> macOS &amp; LinuxWindows <pre><code># Create a new virtual environment named 'mg-env'     \npython3 -m venv mg-env\n\n# Activate the environment     \nsource mg-env/bin/activate    \n</code></pre> <pre><code># Create a new virtual environment named 'mg-env'     \npython -m venv mg-env     \n\n# Activate the environment     \nmg-env\\Scripts\\activate     \n</code></pre>"},{"location":"installation/#2-install-machine-gnostics","title":"2. Install Machine Gnostics","text":"<p>Install the Machine Gnostics library using pip:</p> macOS &amp; LinuxWindows <pre><code>pip install machinegnostics     \n</code></pre> <pre><code>pip install machinegnostics     \n</code></pre> <p>This command will install Machine Gnostics and automatically resolve its dependencies.</p>"},{"location":"installation/#3-verify-installation","title":"3. Verify Installation","text":"<p>You can verify that Machine Gnostics and its dependencies are installed correctly by importing them in a Python session:</p> <pre><code># check import\nimport machinegnostics\nprint(\"imported successfully!\")\n</code></pre> <p>You can also check the installation with pip:</p> macOS &amp; LinuxWindows <pre><code>pip show machinegnostics     \n</code></pre> <pre><code>pip show machinegnostics     \n</code></pre>"},{"location":"installation/#4-quick-usage-example","title":"4. Quick Usage Example","text":"<p>Machine Gnostics is designed to be as simple to use as other machine learning libraries. You can call its functions and classes directly after installation.</p> <p>Gnostic Distribution Function</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF\n\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\negdf = EGDF()\negdf.fit(data)\negdf.plot()\nprint(egdf.params)\n</code></pre> <p>Polynomial Regression</p> <pre><code>import numpy as np\nfrom machinegnostics.models.regression import PolynomialRegressor\n\n# Example data\nX = np.array([0., 0.4, 0.8, 1.2, 1.6, 2. ])\ny = np.array([17.89408548, 69.61586934, -7.19890572, 9.37670866, -10.55673099, 16.57855348])\n\n# Create and fit a robust polynomial regression model\nmodel = PolynomialRegressor(degree=2)\nmodel.fit(X, y)\n\nmodel_lr = LinearRegressor()\nmodel_lr.fit(X, y)\n\n# Make predictions\ny_pred = model.predict(X)\ny_pred_lr = model_lr.predict(X)\n\nprint(\"Predictions:\", y_pred)\n\n# coefficients\nprint(\"Coefficients:\", model.coefficients)\n\n# x vs y, y_pred plot\nimport matplotlib.pyplot as plt\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X, y_pred, color='red', label='Polynomial Prediction')\nplt.plot(X, y_pred_lr, color='green', label='Linear Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Polynomial and Linear Regression')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p>Please find step by step tutorial here.</p>"},{"location":"installation/#5-platform-and-environment","title":"5. Platform and Environment","text":"<ul> <li>Operating System: Tested on macOS and Windows 11</li> <li>Python Version: 3.11 recommended</li> <li>Dependencies: Compatible with NumPy, pandas, SciPy, and other standard data science libraries</li> </ul>"},{"location":"installation/#6-troubleshooting","title":"6. Troubleshooting","text":"<ul> <li>Activate Your Environment:   Always activate your virtual environment before installing or running Machine Gnostics.</li> </ul> WindowsmacOS/Linux <pre><code>mg-env\\Scripts\\activate     \n# or for conda     \nconda activate myenv     \n</code></pre> <pre><code>source mg-env/bin/activate     \n# or for conda     \nconda activate myenv     \n</code></pre> <ul> <li>Check Your Python Version:   Ensure you are using Python 3.8 or newer.</li> </ul> WindowsmacOS/Linux <pre><code>python --version     \n</code></pre> <pre><code>python3 --version     \n</code></pre> <ul> <li>Upgrade pip:   An outdated pip can cause installation errors. Upgrade pip before installing:</li> </ul> WindowsmacOS/Linux <pre><code>pip install --upgrade pip     \n</code></pre> <pre><code>pip install --upgrade pip     \n</code></pre> <ul> <li>Install from a Clean Environment:If you encounter conflicts, try creating a fresh virtual environment and reinstalling.</li> <li>Check Your Internet Connection:Download errors often result from network issues. Make sure you are connected.</li> <li>Permission Issues:If you see permission errors, avoid using <code>sudo pip install</code>. Instead, use a virtual environment.</li> <li> <p>Still Stuck?</p> </li> <li> <p>Double-check the installation instructions.</p> </li> <li>Contact us or open an issue on GitHub.</li> </ul> <p>Machine Gnostics is designed for simplicity and reliability, making robust machine learning accessible for all Python users.</p>"},{"location":"da/cluster_analysis/","title":"ClusterAnalysis: End-to-End Clustering-Based Bound Estimation (Machine Gnostics)","text":"<p>The <code>ClusterAnalysis</code> class provides a robust, automated workflow for estimating main cluster bounds in a dataset using Gnostic Distribution Functions (GDFs) and advanced clustering analysis. It is designed for interpretable, reproducible interval estimation in scientific, engineering, and data science applications.</p>"},{"location":"da/cluster_analysis/#overview","title":"Overview","text":"<p>ClusterAnalysis orchestrates the entire process of fitting a GDF (ELDF/EGDF), assessing data homogeneity, performing cluster boundary detection, and returning interpretable lower and upper cluster bounds (LCB, UCB) for the main data cluster.</p> <ul> <li>Automated Pipeline: Integrates GDF fitting, homogeneity testing, and cluster analysis.</li> <li>Flexible: Supports both local (ELDF) and global (EGDF) GDFs.</li> <li>Robust: Handles weighted data, bounded/unbounded domains, and advanced parameterization.</li> <li>Diagnostics: Detailed error/warning logging and reproducible parameter tracking.</li> <li>Memory-Efficient: Optional flushing of intermediate results.</li> <li>Visualization: Built-in plotting for GDF and cluster analysis results.</li> </ul>"},{"location":"da/cluster_analysis/#key-features","title":"Key Features","text":"<ul> <li>End-to-end cluster-based bound estimation</li> <li>Integrates GDF fitting, homogeneity testing, and clustering</li> <li>Supports local and global GDFs</li> <li>Handles weighted, bounded, and unbounded data</li> <li>Detailed error and warning logging</li> <li>Memory-efficient operation via flushing</li> <li>Visualization of GDF and cluster analysis results</li> </ul>"},{"location":"da/cluster_analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>verbose</code> bool False Print detailed logs and progress information <code>catch</code> bool True Store intermediate results and diagnostics <code>derivative_threshold</code> float 0.01 Threshold for derivative-based cluster boundary detection <code>slope_percentile</code> int 70 Percentile for slope-based boundary detection <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower probable bound (optional) <code>UB</code> float or None None Upper probable bound (optional) <code>S</code> float or 'auto' 'auto' Scale parameter for GDF ('auto' for automatic estimation) <code>varS</code> bool False Use variable scale parameter during optimization <code>z0_optimize</code> bool True Optimize location parameter Z0 during fitting <code>tolerance</code> float 1e-5 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 1000 Number of points for GDF evaluation <code>homogeneous</code> bool True Assume data homogeneity <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'L-BFGS-B' Optimization method (scipy.optimize) <code>max_data_size</code> int 1000 Max data size for smooth GDF generation <code>flush</code> bool False Flush intermediate results after fitting to save memory"},{"location":"da/cluster_analysis/#attributes","title":"Attributes","text":"<ul> <li>LCB: <code>float or None</code>   Lower Cluster Bound (main cluster lower edge)</li> <li>UCB: <code>float or None</code>   Upper Cluster Bound (main cluster upper edge)</li> <li>params: <code>dict</code>   All parameters, intermediate results, errors, and warnings</li> <li>_fitted: <code>bool</code>   Indicates whether analysis has been completed</li> </ul>"},{"location":"da/cluster_analysis/#methods","title":"Methods","text":""},{"location":"da/cluster_analysis/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Runs the full cluster analysis pipeline on the input data.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array for interval analysis</li> <li>plot: <code>bool</code> (optional)   If True, generates plots for the fitted GDF and cluster analysis</li> </ul> <p>Returns: <code>tuple</code> \u2014 <code>(LCB, UCB)</code> as the main cluster bounds</p>"},{"location":"da/cluster_analysis/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary with the estimated bounds and key results.</p> <p>Returns: <code>dict</code> \u2014 <code>{ 'LCB': float, 'UCB': float }</code></p>"},{"location":"da/cluster_analysis/#plot","title":"<code>plot()</code>","text":"<p>Visualizes the fitted GDF and cluster analysis results (if not flushed).</p> <p>Returns: None (displays plot)</p>"},{"location":"da/cluster_analysis/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import ClusterAnalysis\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize ClusterAnalysis\nca = ClusterAnalysis(verbose=True)\n\n# Fit and get cluster bounds\nLCB, UCB = ca.fit(data)\nprint(f\"Main cluster bounds: LCB={LCB:.3f}, UCB={UCB:.3f}\")\n\n# Visualize results\nca.plot()\n\n# Access results dictionary\nresults = ca.results()\nprint(results)\n</code></pre>"},{"location":"da/cluster_analysis/#notes","title":"Notes","text":"<ul> <li>Designed for robust, interpretable cluster-based bound estimation</li> <li>Works best with local GDFs (ELDF); global GDFs (EGDF) are supported</li> <li>If <code>homogeneous=True</code> but data is heterogeneous, a warning is issued</li> <li>All intermediate parameters, errors, and warnings are tracked in <code>params</code></li> <li>For large datasets or memory-constrained environments, set <code>flush=True</code> to save memory (disables plotting)</li> </ul>"},{"location":"da/cluster_analysis/#references","title":"References","text":"<ul> <li>Gnostic Distribution Function theory and clustering methods (see mathematical gnostics literature)</li> <li>For details on underlying algorithms, see documentation for ELDF, EGDF, and DataCluster classes</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/da_models/","title":"Models - Data Analysis (Machine Gnostics)","text":""},{"location":"da/da_models/#welcome-to-machine-gnostics-models","title":"Welcome to Machine Gnostics Models","text":"<p>Machine Gnostics provides a unified framework for robust, assumption-free data analysis using advanced statistical and gnostic theory principles. The \"Models\" section is your entry point to understanding the core analytical tools available in this library.</p>"},{"location":"da/da_models/#what-are-machine-gnostics-models","title":"What Are Machine Gnostics Models?","text":"<p>Machine Gnostics models are specialized classes and algorithms designed to analyze, interpret, and diagnose data distributions. They go beyond traditional statistics by focusing on universal properties, diagnostic features, and gnostic error measures, making them suitable for a wide range of scientific, engineering, and machine learning applications.</p>"},{"location":"da/da_models/#key-data-analysis-model-categories","title":"Key Data Analysis Model Categories","text":"<ul> <li>GDF Distributions </li> <li>EGDF (Empirical Gnostics Distribution Function)  </li> <li>ELDF (Empirical Likelihood Distribution Function)  </li> <li>QGDF (Quantile Gnostics Distribution Function)  </li> <li> <p>QLDF (Quantile Likelihood Distribution Function)   These models provide flexible, non-parametric representations of data distributions, supporting both empirical and quantile-based analysis.</p> </li> <li> <p>Cluster Analysis   Tools for identifying natural groupings and structure in data, supporting unsupervised learning and exploratory analysis.</p> </li> <li> <p>Interval Analysis   Methods for estimating confidence intervals, bounds, and diagnostic regions in data, with robust handling of outliers and non-normality.</p> </li> <li> <p>Homogeneity &amp; Scedasticity   Functions for assessing data uniformity, variance, and diagnostic properties, enabling deeper understanding of data quality and structure.</p> </li> <li> <p>Membership &amp; Data Diagnostics   Algorithms for quantifying membership, relevance, and diagnostic scores for individual data points.</p> </li> </ul>"},{"location":"da/da_models/#why-use-machine-gnostics-models","title":"Why Use Machine Gnostics Models?","text":"<ul> <li>Assumption-Free: No reliance on normality, linearity, or parametric forms.</li> <li>Universal: Applicable to any data type or domain.</li> <li>Diagnostic: Built-in error estimation, entropy measures, and robust statistics.</li> <li>Extensible: Easily integrates with existing Python data science workflows.</li> </ul>"},{"location":"da/da_models/#getting-started","title":"Getting Started","text":"<p>Explore the documentation for each model to learn about their features, usage patterns, and example workflows. - EGDF - ELDF - QGDF - QLDF - Cluster Analysis - Interval Analysis - Homogeneity - Scedasticity - Membership - Data Cluster - Data Interval - Z0 Estimator </p> <p>Each page provides a detailed overview, key features, parameters, example usage, and references.</p>"},{"location":"da/da_models/#next-steps","title":"Next Steps","text":"<ul> <li>Browse individual model pages for in-depth documentation and code examples.</li> <li>Try out example notebooks in the examples folder for hands-on learning.</li> <li>Integrate models into your own analysis pipeline for robust, diagnostic data science.</li> </ul> <p>If you want this saved as <code>models.md</code> in your documentation folder, let me know!</p>"},{"location":"da/data_cluster/","title":"DataCluster: Advanced Cluster Boundary Detection for Gnostic Distribution Functions (Machine Gnostics)","text":"<p>The <code>DataCluster</code> class identifies main cluster boundaries (LCB and UCB) from probability density functions of Gnostic Distribution Functions (GDFs): ELDF, EGDF, QLDF, and QGDF. It uses normalized PDF analysis, derivative-based methods, and shape detection algorithms for robust cluster identification.</p>"},{"location":"da/data_cluster/#overview","title":"Overview","text":"<p>DataCluster provides automated, robust cluster boundary detection for GDFs. It adapts its algorithm based on the type of GDF, using derivative thresholds, valley detection, and slope analysis to find the main cluster region. The class is designed for scientific, engineering, and data science applications where interpretable cluster boundaries are needed.</p> <ul> <li>Supports All GDF Types: ELDF, EGDF, QLDF, QGDF.</li> <li>PDF Normalization: Ensures consistent analysis across distributions.</li> <li>Shape Detection: W-shape/U-shape/heterogeneous detection for QLDF.</li> <li>Derivative-Based Boundaries: Uses first and second derivatives for boundary detection.</li> <li>Fallback Strategies: Falls back to data bounds if boundary detection fails.</li> <li>Diagnostic: Tracks errors, warnings, and method details.</li> <li>Visualization: Plots PDF, boundaries, and derivative analysis.</li> </ul>"},{"location":"da/data_cluster/#key-features","title":"Key Features","text":"<ul> <li>Automated cluster boundary detection for GDFs</li> <li>PDF normalization and robust derivative analysis</li> <li>Shape-based valley detection for QLDF</li> <li>Adaptive thresholding and slope analysis</li> <li>Comprehensive error handling and diagnostics</li> <li>Visualization of PDF, boundaries, and cluster regions</li> </ul>"},{"location":"da/data_cluster/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> ELDF/EGDF/QLDF/QGDF required Fitted GDF object with <code>pdf_points</code> available <code>verbose</code> bool False Print detailed logs and diagnostics <code>catch</code> bool True Store errors, warnings, and results <code>derivative_threshold</code> float 0.01 Threshold for ELDF/EGDF boundary detection <code>slope_percentile</code> int 70 Percentile for QLDF/QGDF slope-based detection"},{"location":"da/data_cluster/#attributes","title":"Attributes","text":"<ul> <li>LCB: <code>float or None</code>Cluster Lower Boundary (left boundary of main cluster)</li> <li>UCB: <code>float or None</code>Cluster Upper Boundary (right boundary of main cluster)</li> <li>z0: <code>float or None</code>Characteristic point of the distribution</li> <li>S_opt: <code>float or None</code>Optimal scale parameter from GDF</li> <li>pdf_normalized: <code>ndarray or None</code>Min-max normalized PDF values [0,1]</li> <li>pdf_original: <code>ndarray or None</code>Original PDF values</li> <li>params: <code>dict</code>Complete analysis results, boundaries, diagnostics, and method details</li> <li>fitted: <code>bool</code>   Indicates whether clustering analysis has been completed</li> </ul>"},{"location":"da/data_cluster/#methods","title":"Methods","text":""},{"location":"da/data_cluster/#fitplotfalse","title":"<code>fit(plot=False)</code>","text":"<p>Performs cluster boundary detection analysis.</p> <ul> <li>plot: <code>bool</code> (optional)   If True, generates a plot of the PDF, detected boundaries, and derivative analysis.</li> </ul> <p>Returns: <code>Tuple[float or None, float or None]</code> \u2014 The detected LCB and UCB values. Returns None for a bound if it cannot be determined.</p>"},{"location":"da/data_cluster/#results","title":"<code>results()</code>","text":"<p>Returns a comprehensive cluster analysis results dictionary.</p> <p>Returns: <code>dict</code> \u2014 Contains LCB, UCB, cluster width, GDF type, Z0, S_opt, method details, errors, and warnings.</p>"},{"location":"da/data_cluster/#plotfigsize12-8","title":"<code>plot(figsize=(12, 8))</code>","text":"<p>Creates a visualization of the PDF, detected boundaries, and derivative analysis.</p> <ul> <li>figsize: <code>tuple</code> (default: (12, 8))   Figure size</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/data_cluster/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import ELDF, DataCluster\n\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\neldf = ELDF()\neldf.fit(data)\n\ncluster = DataCluster(gdf=eldf, verbose=True)\nCLB, CUB = cluster.fit(plot=True)\n\nresults = cluster.results()\nprint(f\"Lower boundary: {results['LCB']}\")\nprint(f\"Upper boundary: {results['UCB']}\")\nprint(f\"Cluster width: {results['cluster_width']}\")\n</code></pre>"},{"location":"da/data_cluster/#notes","title":"Notes","text":"<ul> <li>Clustering works best with local distribution functions (ELDF, QLDF).</li> <li>Global functions (EGDF, QGDF) have limited clustering effectiveness due to uniqueness constraints.</li> <li>QLDF W-shape detection is effective for central clusters between outlying regions.</li> <li>For heterogeneous data with multiple clusters, consider splitting the dataset before analysis.</li> <li>Errors and warnings are tracked in the results dictionary.</li> </ul>"},{"location":"da/data_cluster/#references","title":"References","text":"<ul> <li>Gnostic Distribution Function theory and cluster analysis methods (see mathematical gnostics literature).</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"da/data_interval/","title":"DataIntervals: Robust Interval Estimation Engine (Machine Gnostics)","text":"<p>The <code>DataIntervals</code> class provides robust, adaptive, and diagnostic interval estimation for Gnostic Distribution Function (GDF) classes such as ELDF, EGDF, QLDF, and QGDF. It estimates meaningful data intervals (tolerance, typical intervals) based on the behavior of the GDF's central parameter (Z0) as the data is extended, enforcing ordering constraints and providing detailed diagnostics.</p>"},{"location":"da/data_interval/#overview","title":"Overview","text":"<p>DataIntervals is designed for advanced interval analysis in scientific, engineering, and reliability applications. It adaptively scans the data domain, recomputes the GDF as needed, and extracts intervals that reflect the true structure of the data. The class enforces natural ordering constraints and provides comprehensive diagnostics and visualization.</p> <ul> <li>Adaptive Search: Dense scanning near Z0, sparse near boundaries for efficiency and accuracy.</li> <li>Robustness: Optional GDF recomputation and gnostic filtering for outlier resistance.</li> <li>Diagnostics: Tracks warnings, errors, and parameter settings.</li> <li>Ordering Constraint: Ensures intervals satisfy ZL &lt; Z0L &lt; Z0 &lt; Z0U &lt; ZU.</li> <li>Visualization: Plots Z0 variation, intervals, and data coverage.</li> </ul>"},{"location":"da/data_interval/#key-features","title":"Key Features","text":"<ul> <li>Adaptive interval scanning and estimation</li> <li>Robust to outliers and noise</li> <li>Supports ELDF, EGDF, QLDF, QGDF</li> <li>Ordering constraint enforcement</li> <li>Detailed diagnostics and error tracking</li> <li>Visualization of Z0 variation and intervals</li> <li>Memory-efficient with optional flushing</li> </ul>"},{"location":"da/data_interval/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> ELDF/EGDF/QLDF/QGDF required Fitted GDF object to analyze <code>n_points</code> int 100 Number of search points for interval estimation <code>dense_zone_fraction</code> float 0.4 Fraction of domain near Z0 for dense search <code>dense_points_fraction</code> float 0.7 Fraction of points in dense zone <code>convergence_window</code> int 15 Window size for convergence detection <code>convergence_threshold</code> float 1e-6 Threshold for Z0 convergence <code>min_search_points</code> int 30 Minimum search points before checking convergence <code>boundary_margin_factor</code> float 0.001 Margin to avoid searching at boundaries <code>extrema_search_tolerance</code> float 1e-6 Tolerance for detecting extrema in Z0 variation <code>gdf_recompute</code> bool False Recompute GDF for each candidate datum <code>gnostic_filter</code> bool False Apply gnostic clustering to filter outlier Z0 values <code>catch</code> bool True Store warnings/errors internally <code>verbose</code> bool False Print detailed progress and diagnostics <code>flush</code> bool False Flush memory after fitting to save resources"},{"location":"da/data_interval/#attributes","title":"Attributes","text":"<ul> <li>ZL: <code>float</code>Lower bound of the typical data interval</li> <li>Z0L: <code>float</code>Lower bound of the tolerance interval (Z0-based)</li> <li>Z0: <code>float</code>Central value (Z0) of the original GDF</li> <li>Z0U: <code>float</code>Upper bound of the tolerance interval (Z0-based)</li> <li>ZU: <code>float</code>Upper bound of the typical data interval</li> <li>tolerance_interval: <code>float</code>Width of the tolerance interval (Z0U - Z0L)</li> <li>typical_data_interval: <code>float</code>Width of the typical data interval (ZU - ZL)</li> <li>ordering_valid: <code>bool</code>Whether the ordering constraint (ZL &lt; Z0L &lt; Z0 &lt; Z0U &lt; ZU) is satisfied</li> <li>params: <code>dict</code>Dictionary of parameters, warnings, errors, and results</li> <li>search_results: <code>dict</code>   Raw search results for datum values and corresponding Z0s</li> </ul>"},{"location":"da/data_interval/#methods","title":"Methods","text":""},{"location":"da/data_interval/#fitplotfalse","title":"<code>fit(plot=False)</code>","text":"<p>Runs the interval estimation process. Optionally plots results.</p> <ul> <li>plot: <code>bool</code> (optional)   If True, automatically plot the interval analysis results after fitting.</li> </ul> <p>Returns: None (results stored in attributes and params)</p>"},{"location":"da/data_interval/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of estimated interval results and bounds.</p> <p>Returns: <code>dict</code> \u2014 Contains keys such as <code>'LB', 'LSB', 'DLB', 'LCB', 'LSD', 'ZL', 'Z0L', 'Z0', 'Z0U', 'ZU', 'USD', 'UCB', 'DUB', 'USB', 'UB'</code></p> <pre><code>-**LB**: Lower Bound\nThe practical lower limit for the interval (may be set by user or inferred).\n\n-**LSB**: Lower Sample (Membership) Bound\nThe lowest value for which data is homogeneous.\n\n-**DLB**: Data Lower Bound\nThe absolute minimum value present in the data.\n\n-**LCB**: Lower Cluster Bound\nThe lower edge of the main data cluster.\n\n-**LSD**: Lower Standard Deviation Bound\nThe lowest value as per gnostic standard deviation.\n\n-**ZL**: Z0 Lower Interval\nThe lower bound of the typical interval.\n\n-**Z0L**: Z0 Lower Bound\nThe lower bound of the tolerance interval.\n\n-**Z0**: Central Value (Gnostic Mean)\nThe central parameter of the distribution (gnostic mean).\n\n-**Z0U**: Z0 Upper Bound\nThe upper bound of the tolerance interval.\n\n-**ZU**: Z0 Upper Interval\nThe upper bound of the typical interval.\n\n-**USD**: Upper Support/Domain Bound\nThe highest value in the support or domain of the fitted distribution.\n\n-**UCB**: Upper Cluster Bound\nThe upper edge of the main data cluster.\n\n-**DUB**: Data Upper Bound\nThe absolute maximum value present in the data.\n\n-**USB**: Upper Sample (Membership) Bound\nThe highest value for which data is homogeneous (membership analysis).\n\n-**UB**: Upper Bound\nThe practical upper limit for the interval (may be set by user or inferred).\n</code></pre>"},{"location":"da/data_interval/#plot_intervalsfigsize12-8","title":"<code>plot_intervals(figsize=(12, 8))</code>","text":"<p>Plots the Z0 variation and estimated intervals.</p> <ul> <li>figsize: <code>tuple</code> (default: (12, 8))   Size of the matplotlib figure</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/data_interval/#plotfigsize12-8","title":"<code>plot(figsize=(12, 8))</code>","text":"<p>Plots the GDF, PDF, and intervals on the data domain.</p> <ul> <li>figsize: <code>tuple</code> (default: (12, 8))   Size of the matplotlib figure</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/data_interval/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import ELDF, DataIntervals\n\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# ELDF\neld = ELDF()\neld.fit(data)\n\ndi = DataIntervals(eld, n_points=200, gdf_recompute=True, gnostic_filter=True, verbose=True)\ndi.fit()\n\nprint(di.results())\ndi.plot_intervals()\ndi.plot()\n</code></pre>"},{"location":"da/data_interval/#notes","title":"Notes","text":"<ul> <li>For best results, use with ELDF or QLDF and set 'wedf=False' in the GDF.</li> <li>Increasing 'n_points' improves accuracy but increases computation time.</li> <li>Enable 'gdf_recompute' and 'gnostic_filter' for maximum robustness, especially with noisy data.</li> <li>The class is designed for research and diagnostic use; adjust parameters for your data and application.</li> <li>Ordering constraint is enforced for interval validity; warnings are issued if violated.</li> <li>All warnings and errors are stored in the <code>params</code> attribute for later inspection.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"da/egdf/","title":"EGDF: Estimating Global Distribution Function (Machine Gnostics)","text":"<p>The <code>EGDF</code> class provides robust, assumption-free global distribution estimation for real-world data using the Machine Gnostics framework. Unlike traditional parametric models, EGDF adapts directly to your data, making it ideal for noisy, uncertain, or heterogeneous datasets.</p>"},{"location":"da/egdf/#overview","title":"Overview","text":"<p>EGDF is designed for robust probability and density estimation, especially when data may contain outliers, inner noise, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Robust: Handles outliers, inner noise, and contaminated data.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Parameter Estimation: Scale and bounds inferred from data.</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for EGDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/egdf/#key-features","title":"Key Features","text":"<ul> <li>Fits a global distribution function to your data</li> <li>Robust to outliers and inner noise</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of EGDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> </ul>"},{"location":"da/egdf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter (auto-estimated or fixed value) <code>z0_optimize</code> bool True Optimize location parameter during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 500 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'L-BFGS-B' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth EGDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/egdf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/egdf/#methods","title":"Methods","text":""},{"location":"da/egdf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the EGDF to your data, estimating all relevant parameters and generating the global distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/egdf/#plotplot_smoothtrue-plotboth-boundsfalse-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=False, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted EGDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>   Plot smooth interpolated curve.</li> <li>plot: <code>str</code>   'gdf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>   Show bound lines.</li> <li>extra_df: <code>bool</code>   Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/egdf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/egdf/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize EGDF\negdf = EGDF()\n\n# Fit the model\negdf.fit(data)\n\n# Plot the results\negdf.plot()\n\n# Access fitted parameters\nresults = egdf.results()\nprint(\"Global scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/egdf/#notes","title":"Notes","text":"<ul> <li>EGDF is robust to outliers and suitable for non-Gaussian, contaminated, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., reliability, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/eldf/","title":"ELDF: Estimating Local Distribution Function (Machine Gnostics)","text":"<p>The <code>ELDF</code> class provides robust, assumption-free local distribution estimation for real-world data using the Machine Gnostics framework. ELDF is designed for detailed local analysis, peak detection, and modal characterization, making it ideal for noisy, uncertain, or heterogeneous datasets.</p>"},{"location":"da/eldf/#overview","title":"Overview","text":"<p>ELDF is optimized for local probability and density estimation, especially when data may contain outliers, inner noise, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Robust: Handles outliers, inner noise, and contaminated data.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Parameter Estimation: Scale and bounds inferred from data.</li> <li>Advanced Z0 Estimation: Finds the gnostic mean (location of maximum PDF).</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for ELDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/eldf/#key-features","title":"Key Features","text":"<ul> <li>Fits a local distribution function to your data</li> <li>Robust to outliers and inner noise</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of ELDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> <li>Variable scale parameter option for heteroscedasticity</li> </ul>"},{"location":"da/eldf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter (auto-estimated or fixed value) <code>varS</code> bool False Use variable scale parameter during optimization <code>z0_optimize</code> bool True Optimize location parameter Z0 during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 1000 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'L-BFGS-B' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth ELDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/eldf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, varS, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/eldf/#methods","title":"Methods","text":""},{"location":"da/eldf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the ELDF to your data, estimating all relevant parameters and generating the local distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/eldf/#plotplot_smoothtrue-plotboth-boundstrue-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=True, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted ELDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>   Plot smooth interpolated curve.</li> <li>plot: <code>str</code>   'eldf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>   Show bound lines.</li> <li>extra_df: <code>bool</code>   Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/eldf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/eldf/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import ELDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize ELDF\neldf = ELDF()\n\n# Fit the model\neldf.fit(data)\n\n# Plot the results\neldf.plot()\n\n# Access fitted parameters\nresults = eldf.results()\nprint(\"Local scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/eldf/#notes","title":"Notes","text":"<ul> <li>ELDF is robust to outliers and suitable for non-Gaussian, contaminated, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., clustering, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of local distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/homogeneity/","title":"DataHomogeneity: Homogeneity Analysis for EGDF (Machine Gnostics)","text":"<p>The <code>DataHomogeneity</code> class provides robust, numerical homogeneity analysis for Estimating Global Distribution Functions (EGDF) by examining the shape and characteristics of their probability density functions (PDF). It is designed to detect outliers, clusters, and non-homogeneous structure in data using gnostic theory principles.</p>"},{"location":"da/homogeneity/#overview","title":"Overview","text":"<p>DataHomogeneity analyzes the fitted EGDF's PDF to determine if the underlying data is homogeneous. Homogeneity is defined by the presence of a single global maximum (unimodal PDF) and the absence of negative density values. The class uses robust peak detection, configurable smoothing, and comprehensive diagnostics to provide reliable results.</p> <p>Gnostic vs. Statistical Homogeneity: Gnostic homogeneity analysis is based on the algebraic and geometric properties of the data and the EGDF, not on statistical or probabilistic assumptions. It is deterministic, reproducible, and sensitive to both outliers and clusters, making it fundamentally different from classical statistical homogeneity tests.</p> <ul> <li>Assumption-Free: No parametric or probabilistic assumptions.</li> <li>Numerical: Decisions are made based on numerical analysis, not visual inspection.</li> <li>Robust: Detects outliers and clusters via PDF maxima.</li> <li>Diagnostic: Tracks errors, warnings, and analysis parameters.</li> <li>Memory-Efficient: Optional flushing of large arrays after analysis.</li> <li>Visualization: Built-in plotting for PDF and detected maxima.</li> </ul>"},{"location":"da/homogeneity/#key-features","title":"Key Features","text":"<ul> <li>Automatic EGDF validation and homogeneity testing</li> <li>Robust peak detection with configurable smoothing</li> <li>Comprehensive error and warning tracking</li> <li>Memory management with optional data flushing</li> <li>Detailed visualization of analysis results</li> <li>Integration with EGDF parameter systems</li> </ul>"},{"location":"da/homogeneity/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> EGDF required Fitted EGDF object (must have catch=True and be fitted) <code>verbose</code> bool True Print detailed progress, warnings, and results <code>catch</code> bool True Store all analysis results and metadata <code>flush</code> bool False Clear large arrays after analysis to save memory <code>smoothing_sigma</code> float 1.0 Gaussian smoothing parameter for PDF preprocessing <code>min_height_ratio</code> float 0.01 Minimum relative height threshold for peak detection <code>min_distance</code> int or None None Minimum separation between detected peaks (auto if None)"},{"location":"da/homogeneity/#attributes","title":"Attributes","text":"<ul> <li>is_homogeneous: <code>bool or None</code>   Primary analysis result (None before fit, True/False after analysis)</li> <li>picks: <code>List[Dict]</code>   Detected maxima with detailed information (index, position, value, global flag)</li> <li>z0: <code>float or None</code>   Global optimum value from EGDF or detected from PDF</li> <li>global_extremum_idx: <code>int or None</code>   Array index of the global maximum</li> <li>fitted: <code>bool</code>   Indicates if analysis has been completed</li> </ul>"},{"location":"da/homogeneity/#methods","title":"Methods","text":""},{"location":"da/homogeneity/#fitplotfalse","title":"<code>fit(plot=False)</code>","text":"<p>Performs comprehensive homogeneity analysis on the EGDF object.</p> <ul> <li>plot: <code>bool</code> (optional)   If True, generates plots for visual inspection of the analysis results</li> </ul> <p>Returns: <code>bool</code> \u2014 True if data is homogeneous, False otherwise</p>"},{"location":"da/homogeneity/#results","title":"<code>results()</code>","text":"<p>Retrieves comprehensive homogeneity analysis results and metadata.</p> <p>Returns: <code>dict</code> \u2014 Contains keys such as <code>'is_homogeneous', 'picks', 'z0', 'global_extremum_idx', 'analysis_parameters', 'gdf_parameters', 'errors', 'warnings'</code></p>"},{"location":"da/homogeneity/#plotfigsize12-8-titlenone","title":"<code>plot(figsize=(12, 8), title=None)</code>","text":"<p>Visualizes the PDF, detected maxima, and homogeneity status.</p> <ul> <li>figsize: <code>tuple</code> (default: (12, 8))   Figure size in inches</li> <li>title: <code>str or None</code>   Custom plot title</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/homogeneity/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF, DataHomogeneity\n\n# Homogeneous data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\negdf = EGDF(data=data, catch=True)\negdf.fit()\n\n# Homogeneity analysis\nhomogeneity = DataHomogeneity(egdf, verbose=True)\nis_homogeneous = homogeneity.fit(plot=True)\nprint(f\"Data is homogeneous: {is_homogeneous}\")\n\n# Access results\nresults = homogeneity.results()\nprint(f\"Number of maxima detected: {len(results['picks'])}\")\n</code></pre>"},{"location":"da/homogeneity/#notes","title":"Notes","text":"<ul> <li>Only supports EGDF objects (not QGDF, ELDF, or QLDF)</li> <li>Homogeneity is defined by a single global maximum and no negative PDF values</li> <li>Outliers and clusters are detected as additional maxima</li> <li>Numerical analysis is preferred over visual inspection for reliability</li> <li>Use <code>flush=True</code> for large datasets to save memory</li> <li>All errors and warnings are tracked in the results dictionary</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/interval_analysis/","title":"IntervalAnalysis: Marginal Interval Analysis (Machine Gnostics)","text":"<p>The <code>IntervalAnalysis</code> class provides robust, adaptive, and diagnostic interval estimation for Gnostic Distribution Functions (GDFs) such as ELDF, EGDF, QLDF, and QGDF. It estimates meaningful data intervals (tolerance, typical intervals) based on the behavior of the GDF's central parameter (Z0) as the data is extended, enforcing ordering constraints and providing detailed diagnostics.</p>"},{"location":"da/interval_analysis/#overview","title":"Overview","text":"<p>IntervalAnalysis orchestrates the complete process of fitting GDFs, checking homogeneity, and computing robust data intervals using the DataIntervals engine. It is designed for reliability, diagnostics, and adaptive interval estimation in scientific and engineering data analysis.</p> <p>Gnostic vs. Statistical Interval Analysis:Gnostic interval analysis does not rely on probabilistic or statistical assumptions. Instead, it uses algebraic and geometric properties of the data and distribution functions, providing deterministic, reproducible, and interpretable intervals even for small, noisy, or non-Gaussian datasets. This is fundamentally different from classical statistical interval estimation, which depends on distributional assumptions and sampling theory.</p> <ul> <li>Assumption-Free: No parametric or probabilistic assumptions.</li> <li>Robust: Handles outliers, heterogeneity, and bounded/unbounded domains.</li> <li>Adaptive: Intervals adapt to data structure and central parameter behavior.</li> <li>Diagnostics: Tracks warnings, errors, and intermediate results.</li> <li>Visualization: Built-in plotting for distributions and intervals.</li> <li>Memory-Efficient: Optional flushing of intermediate arrays.</li> </ul>"},{"location":"da/interval_analysis/#key-features","title":"Key Features","text":"<ul> <li>End-to-end interval estimation for GDFs</li> <li>Automatic homogeneity testing and diagnostics</li> <li>Adaptive tolerance and typical interval computation</li> <li>Handles weighted, bounded, and unbounded data</li> <li>Detailed error and warning logging</li> <li>Visualization of fitted distributions and intervals</li> <li>Deterministic and reproducible results</li> </ul>"},{"location":"da/interval_analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter for distribution <code>z0_optimize</code> bool True Optimize central parameter Z0 during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 100 Number of points for distribution evaluation <code>homogeneous</code> bool True Assume data homogeneity (enables homogeneity testing) <code>catch</code> bool True Store warnings/errors and intermediate results <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'L-BFGS-B' Optimization method (scipy.optimize) <code>verbose</code> bool False Print detailed progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth GDF generation <code>flush</code> bool True Flush intermediate arrays after fitting <code>dense_zone_fraction</code> float 0.4 Fraction of domain near Z0 for dense interval search <code>dense_points_fraction</code> float 0.7 Fraction of search points in dense zone <code>convergence_window</code> int 15 Window size for convergence detection <code>convergence_threshold</code> float 1e-6 Threshold for Z0 convergence <code>min_search_points</code> int 30 Minimum search points before checking convergence <code>boundary_margin_factor</code> float 0.001 Margin factor to avoid searching at boundaries <code>extrema_search_tolerance</code> float 1e-6 Tolerance for detecting extrema in Z0 variation <code>gdf_recompute</code> bool False Recompute GDF for each candidate datum in interval search <code>gnostic_filter</code> bool False Apply gnostic clustering to filter outlier Z0 values <code>cluster_bounds</code> bool True Estimate cluster bounds using DataCluster <code>membership_bounds</code> bool True Estimate membership bounds using DataMembership"},{"location":"da/interval_analysis/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Stores all warnings, errors, and diagnostic information from the analysis.</li> </ul>"},{"location":"da/interval_analysis/#methods","title":"Methods","text":""},{"location":"da/interval_analysis/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Runs the complete interval analysis workflow on the input data.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>1D numpy array of input data for interval analysis</li> <li>plot: <code>bool</code> (optional)   If True, automatically generates diagnostic plots after fitting</li> </ul> <p>Returns: <code>dict</code> \u2014 Estimated interval bounds and diagnostics</p>"},{"location":"da/interval_analysis/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of estimated interval results and bounds. Also called 'Data Certification'</p> <p>Returns: <code>dict</code> \u2014 Contains keys such as <code>'LB', 'LSB', 'DLB', 'LCB', 'LSD', 'ZL', 'Z0L', 'Z0', 'Z0U', 'ZU', 'USD', 'UCB', 'DUB', 'USB', 'UB'</code></p> <pre><code>-**LB**: Lower Bound\nThe practical lower limit for the interval (may be set by user or inferred).\n\n-**LSB**: Lower Sample (Membership) Bound\nThe lowest value for which data is homogeneous.\n\n-**DLB**: Data Lower Bound\nThe absolute minimum value present in the data.\n\n-**LCB**: Lower Cluster Bound\nThe lower edge of the main data cluster.\n\n-**LSD**: Lower Standard Deviation Bound\nThe lowest value as per gnostic standard deviation.\n\n-**ZL**: Z0 Lower Interval\nThe lower bound of the typical interval.\n\n-**Z0L**: Z0 Lower Bound\nThe lower bound of the tolerance interval.\n\n-**Z0**: Central Value (Gnostic Mean)\nThe central parameter of the distribution (gnostic mean).\n\n-**Z0U**: Z0 Upper Bound\nThe upper bound of the tolerance interval.\n\n-**ZU**: Z0 Upper Interval\nThe upper bound of the typical interval.\n\n-**USD**: Upper Support/Domain Bound\nThe highest value in the support or domain of the fitted distribution.\n\n-**UCB**: Upper Cluster Bound\nThe upper edge of the main data cluster.\n\n-**DUB**: Data Upper Bound\nThe absolute maximum value present in the data.\n\n-**USB**: Upper Sample (Membership) Bound\nThe highest value for which data is homogeneous (membership analysis).\n\n-**UB**: Upper Bound\nThe practical upper limit for the interval (may be set by user or inferred).\n</code></pre>"},{"location":"da/interval_analysis/#plotgdftrue-intervalstrue","title":"<code>plot(GDF=True, intervals=True)</code>","text":"<p>Visualizes the fitted GDFs and the estimated intervals.</p> <ul> <li>GDF: <code>bool</code> (default: True)Plot the fitted ELDF (local distribution function)</li> <li>intervals: <code>bool</code> (default: True)   Plot the estimated intervals and Z0 variation</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/interval_analysis/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import IntervalAnalysis\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize IntervalAnalysis\nia = IntervalAnalysis(verbose=True)\n\n# Fit and get interval results\nia.fit(data, plot=True)\nprint(ia.results())\n\n# Visualize results\nia.plot(GDF=True, intervals=True)\n</code></pre>"},{"location":"da/interval_analysis/#notes","title":"Notes","text":"<ul> <li>Gnostic interval analysis is fundamentally different from statistical interval analysis: it does not rely on probability or sampling theory, but on algebraic and geometric properties of the data and distribution functions.</li> <li>Homogeneity of the data is checked automatically; warnings are issued if violated.</li> <li>For best results, use with ELDF/EGDF and set <code>wedf=False</code> for interval estimation.</li> <li>Suitable for scientific, engineering, and reliability applications.</li> <li>All warnings and errors are stored in the <code>params</code> attribute for later inspection.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/membership/","title":"DataMembership: Gnostic Membership Test (Machine Gnostics)","text":"<p>The <code>DataMembership</code> class provides a robust method to test whether a value can be considered a member of a homogeneous data sample, using the EGDF (Estimating Global Distribution Function) framework. It determines the bounds within which new data points can be added to a sample without disrupting its homogeneity.</p>"},{"location":"da/membership/#overview","title":"Overview","text":"<p>DataMembership answers the question: \"Will the homogeneous sample remain homogeneous after extension by a new value?\" The class uses EGDF and DataHomogeneity to check the effect of adding candidate values to the sample, and computes the Lower Sample Bound (LSB) and Upper Sample Bound (USB) for membership.</p> <ul> <li>EGDF-Based: Only works with EGDF objects.</li> <li>Homogeneity-Driven: Membership is defined by preservation of sample homogeneity.</li> <li>Adaptive Bound Search: Finds minimum and maximum values that keep the sample homogeneous.</li> <li>Diagnostic: Tracks errors, warnings, and search parameters.</li> <li>Visualization: Plots EGDF, PDF, and membership bounds.</li> </ul>"},{"location":"da/membership/#key-features","title":"Key Features","text":"<ul> <li>Tests membership of candidate values in homogeneous samples</li> <li>Computes LSB and USB for membership range</li> <li>Integrates with EGDF and DataHomogeneity</li> <li>Adaptive, iterative bound search</li> <li>Detailed logging and error tracking</li> <li>Visualization of membership bounds and EGDF</li> </ul>"},{"location":"da/membership/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>egdf</code> EGDF required Fitted EGDF object (must be fitted and contain data) <code>verbose</code> bool True Print detailed logs and diagnostics <code>catch</code> bool True Store errors, warnings, and results <code>tolerance</code> float 1e-3 Tolerance for numerical calculations <code>max_iterations</code> int 100 Maximum iterations for bound search <code>initial_step_factor</code> float 0.001 Initial step size factor for adaptive search"},{"location":"da/membership/#attributes","title":"Attributes","text":"<ul> <li>LSB: <code>float or None</code>   Lower Sample Bound (minimum value that keeps sample homogeneous)</li> <li>USB: <code>float or None</code>   Upper Sample Bound (maximum value that keeps sample homogeneous)</li> <li>is_homogeneous: <code>bool</code>   Indicates whether the original sample is homogeneous</li> <li>params: <code>dict</code>   Stores results, errors, warnings, and search parameters</li> <li>fitted: <code>bool</code>   Indicates whether membership analysis has been completed</li> </ul>"},{"location":"da/membership/#methods","title":"Methods","text":""},{"location":"da/membership/#fit","title":"<code>fit()</code>","text":"<p>Performs membership analysis to determine LSB and USB.</p> <p>Returns: <code>Tuple[float or None, float or None]</code> \u2014 The calculated LSB and USB values. Returns None for a bound if it cannot be determined.</p>"},{"location":"da/membership/#plotplot_smoothtrue-plotboth-boundstrue-figsize12-8","title":"<code>plot(plot_smooth=True, plot='both', bounds=True, figsize=(12, 8))</code>","text":"<p>Generates a plot of the EGDF and PDF with membership bounds and other relevant information.</p> <ul> <li>plot_smooth: <code>bool</code>   Plot smoothed EGDF and PDF</li> <li>plot: <code>str</code>   'gdf', 'pdf', or 'both'</li> <li>bounds: <code>bool</code>   Include data bounds in the plot</li> <li>figsize: <code>tuple</code>   Figure size</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/membership/#results","title":"<code>results()</code>","text":"<p>Returns the analysis results stored in the <code>params</code> attribute.</p> <p>Returns: <code>dict</code> \u2014 Contains LSB, USB, is_homogeneous, search parameters, errors, and warnings.</p>"},{"location":"da/membership/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF, DataMembership\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\negdf_instance = EGDF(data)\negdf_instance.fit()\n\nmembership = DataMembership(egdf_instance, verbose=True)\nlsb, usb = membership.fit()\n\nprint(f\"Lower Bound: {lsb}, Upper Bound: {usb}\")\n\nmembership.plot()\n\nresults = membership.results()\nprint(results)\n</code></pre>"},{"location":"da/membership/#notes","title":"Notes","text":"<ul> <li>Only works with EGDF objects; sample must be homogeneous for membership analysis.</li> <li>LSB and USB define the range of values that can be added to the sample without losing homogeneity.</li> <li>Errors and warnings are tracked in the results dictionary.</li> <li>Visualization shows EGDF, PDF, and membership bounds for interpretability.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/qgdf/","title":"QGDF: Quantifying Global Distribution Function (Machine Gnostics)","text":"<p>The <code>QGDF</code> class provides robust, assumption-free global quantification of data distributions using the Machine Gnostics framework. QGDF is designed for inlier-resistant, sample-wide analysis, making it ideal for heterogeneous, clustered, or uncertain datasets where dense regions may dominate.</p>"},{"location":"da/qgdf/#overview","title":"Overview","text":"<p>QGDF is optimized for global quantification and density estimation, especially when data may contain dense clusters, inliers, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Inlier-Resistant: Robust to dense clusters and inliers.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Parameter Estimation: Scale and bounds inferred from data.</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for QGDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/qgdf/#key-features","title":"Key Features","text":"<ul> <li>Fits a global quantifying distribution function to your data</li> <li>Robust to inliers and dense clusters</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of QGDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> </ul>"},{"location":"da/qgdf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter (auto-estimated or fixed value) <code>z0_optimize</code> bool True Optimize location parameter during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 500 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'L-BFGS-B' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth QGDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/qgdf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/qgdf/#methods","title":"Methods","text":""},{"location":"da/qgdf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the QGDF to your data, estimating all relevant parameters and generating the global quantifying distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/qgdf/#plotplot_smoothtrue-plotboth-boundsfalse-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=False, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted QGDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>Plot smooth interpolated curve.</li> <li>plot: <code>str</code>'qgdf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>Show bound lines.</li> <li>extra_df: <code>bool</code>Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/qgdf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/qgdf/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import QGDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize QGDF\nqgdf = QGDF()\n\n# Fit the model\nqgdf.fit(data)\n\n# Plot the results\nqgdf.plot()\n\n# Access fitted parameters\nresults = qgdf.results()\nprint(\"Global scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/qgdf/#notes","title":"Notes","text":"<ul> <li>QGDF is robust to inliers and suitable for non-Gaussian, clustered, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., reliability, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/qldf/","title":"QLDF: Quantifying Local Distribution Function (Machine Gnostics)","text":"<p>The <code>QLDF</code> class provides robust, assumption-free local quantification of data distributions using the Machine Gnostics framework. QLDF is designed for inlier-resistant, detailed local analysis, making it ideal for heterogeneous, clustered, or uncertain datasets where dense regions may dominate.</p>"},{"location":"da/qldf/#overview","title":"Overview","text":"<p>QLDF is optimized for local quantification and density estimation, especially when data may contain dense clusters, inliers, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Inlier-Resistant: Robust to dense clusters and inliers.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Z0 Identification: Finds local minima in probability density.</li> <li>Advanced Interpolation: Precise estimation of critical points.</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for QLDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/qldf/#key-features","title":"Key Features","text":"<ul> <li>Fits a local quantifying distribution function to your data</li> <li>Robust to inliers and dense clusters</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of QLDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> <li>Variable scale parameter option for heteroscedasticity</li> </ul>"},{"location":"da/qldf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 1 Scale parameter (auto-estimated or fixed value) <code>varS</code> bool False Use variable scale parameter during optimization <code>z0_optimize</code> bool True Optimize location parameter Z0 during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 500 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'L-BFGS-B' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth QLDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/qldf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, varS, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/qldf/#methods","title":"Methods","text":""},{"location":"da/qldf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the QLDF to your data, estimating all relevant parameters and generating the local quantifying distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/qldf/#plotplot_smoothtrue-plotboth-boundstrue-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=True, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted QLDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>   Plot smooth interpolated curve.</li> <li>plot: <code>str</code>   'qldf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>   Show bound lines.</li> <li>extra_df: <code>bool</code>   Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/qldf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/qldf/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import QLDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize QLDF\nqldf = QLDF()\n\n# Fit the model\nqldf.fit(data)\n\n# Plot the results\nqldf.plot()\n\n# Access fitted parameters\nresults = qldf.results()\nprint(\"Local scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/qldf/#notes","title":"Notes","text":"<ul> <li>QLDF is robust to inliers and suitable for non-Gaussian, clustered, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., clustering, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of local distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/scedasticity/","title":"DataScedasticity: Gnostic Homoscedasticity and Heteroscedasticity Test (Machine Gnostics)","text":"<p>The <code>DataScedasticity</code> class provides a gnostic approach to testing for homoscedasticity and heteroscedasticity in data, using gnostic variance and gnostic linear regression. Unlike classical statistical tests, this method is based on the principles of the Machine Gnostics framework and is designed for robust, interpretable diagnostics.</p>"},{"location":"da/scedasticity/#overview","title":"Overview","text":"<p>DataScedasticity analyzes the spread of residuals from a gnostic linear regression model by splitting the data at the median of the independent variable and comparing the gnostic variances of squared residuals in each half. This approach is fundamentally different from classical tests (e.g., Breusch-Pagan, White's test), focusing on gnostic variance and regression rather than least squares and probabilistic assumptions.</p> <ul> <li>Gnostic Variance: Measures uncertainty and spread according to gnostic principles.</li> <li>Gnostic Regression: Uses a gnostic linear regression model, not standard least squares.</li> <li>Diagnostic Philosophy: Not a formal statistical test, but a robust diagnostic for gnostic data analysis.</li> <li>Split Residuals: Compares variance in residuals before and after the median of the independent variable.</li> </ul>"},{"location":"da/scedasticity/#key-features","title":"Key Features","text":"<ul> <li>Gnostic variance and regression for scedasticity analysis</li> <li>No reliance on classical statistical assumptions</li> <li>Robust to outliers and non-Gaussian data</li> <li>Variance ratio calculation for split residuals</li> <li>Clear homoscedastic/heteroscedastic decision</li> <li>Detailed logging and parameter tracking</li> </ul>"},{"location":"da/scedasticity/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>scale</code> str/int/float 'auto' Scale parameter for regression <code>max_iter</code> int 100 Maximum iterations for regression optimization <code>tol</code> float 0.001 Tolerance for regression convergence <code>mg_loss</code> str 'hi' Loss function for gnostic regression <code>early_stopping</code> bool True Enable early stopping in regression <code>verbose</code> bool False Print detailed logs and diagnostics <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>gnostic_characteristics</code> bool True Use gnostic characteristics in regression <code>history</code> bool True Track regression history"},{"location":"da/scedasticity/#attributes","title":"Attributes","text":"<ul> <li>x: <code>np.ndarray</code>   Independent variable data.</li> <li>y: <code>np.ndarray</code>   Dependent variable data.</li> <li>model: <code>LinearRegressor</code>   Gnostic linear regression model.</li> <li>residuals: <code>np.ndarray</code>   Residuals from the fitted model.</li> <li>params: <code>dict</code>   Stores calculated variances and variance ratio.</li> <li>variance_ratio: <code>float</code>   Ratio of gnostic variances between data splits.</li> <li>is_homoscedastic: <code>bool</code>   True if data is homoscedastic under gnostic test, else False.</li> </ul>"},{"location":"da/scedasticity/#methods","title":"Methods","text":""},{"location":"da/scedasticity/#fitx-y","title":"<code>fit(x, y)</code>","text":"<p>Fits the gnostic linear regression model to the data and assesses scedasticity.</p> <ul> <li>x: <code>np.ndarray</code>   Independent variable data.</li> <li>y: <code>np.ndarray</code>   Dependent variable data.</li> </ul> <p>Returns: <code>bool</code> \u2014 True if data is homoscedastic under the gnostic test, False if heteroscedastic.</p>"},{"location":"da/scedasticity/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.magcal import DataScedasticity\n\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ny = np.array([2.1, 4.2, 6.1, 8.3, 10.2, 12.1, 14.2, 16.1, 18.2, 20.1])\n\nsced = DataScedasticity()\nis_homo = sced.fit(x, y)\nprint(f\"Is data homoscedastic? {is_homo}\")\nprint(f\"Variance ratio: {sced.variance_ratio}\")\n</code></pre>"},{"location":"da/scedasticity/#notes","title":"Notes","text":"<ul> <li>This is not a standard statistical test; results may differ from classical methods.</li> <li>Gnostic variance and regression are designed for robust, interpretable diagnostics.</li> <li>For more details on gnostic variance and regression, refer to the Machine Gnostics documentation.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/z0_estimator/","title":"Z0Estimator","text":""},{"location":"da/z0_estimator/#overview","title":"Overview","text":"<p>The <code>Z0Estimator</code> is a universal estimator for the Z0 point in GDF (Gnostics Distribution Function) distributions. Z0 represents a key location in the distribution: - For EGDF/ELDF: Z0 is where the PDF reaches its global maximum. - For QLDF/QGDF: Z0 is the point where the distribution function equals 0.5 (the median or 50th percentile).</p> <p>This class automatically detects the distribution type and applies advanced estimation strategies to accurately determine Z0, supporting both simple and sophisticated methods.</p>"},{"location":"da/z0_estimator/#key-features","title":"Key Features","text":"<ul> <li>Automatic distribution type detection (EGDF, ELDF, QLDF, QGDF)</li> <li>Multiple estimation strategies: simple discrete search and advanced optimization (spline, polynomial, interpolation)</li> <li>Robust handling of flat regions and edge cases</li> <li>Comprehensive diagnostic information</li> <li>Built-in visualization capabilities</li> <li>Automatic Z0 assignment to the GDF object</li> <li>Estimation of Z0 gnostic error properties (Residual Entropy, RRE)</li> </ul>"},{"location":"da/z0_estimator/#parameters","title":"Parameters","text":"Parameter Type Description Default gdf_object object Fitted GDF object (EGDF, ELDF, QLDF, or QGDF). Must be fitted before passing. Required optimize bool Use advanced optimization methods (spline, polynomial, etc.) True verbose bool Print detailed progress information during estimation False"},{"location":"da/z0_estimator/#attributes","title":"Attributes","text":"Attribute Type Description gdf object The fitted GDF object gdf_type str Detected distribution type ('egdf', 'eldf', 'qldf', 'qgdf') optimize bool Whether advanced optimization is used verbose bool Whether verbose output is enabled find_median bool True for QLDF/QGDF (find 0.5 point), False for EGDF/ELDF (find PDF max) z0 float Estimated Z0 value (None until fit() is called) estimation_info dict Detailed information about the estimation process"},{"location":"da/z0_estimator/#methods","title":"Methods","text":"Method Description <code>fit()</code> Estimates the Z0 point for the given GDF object. Assigns Z0 to the GDF object. <code>get_estimation_info()</code> Returns a dictionary with details about the estimation process. <code>plot_z0_analysis()</code> Generates diagnostic plots showing PDF/CDF and the Z0 point. <code>__repr__()</code> Returns a string representation of the estimator and its status."},{"location":"da/z0_estimator/#example-usage","title":"Example Usage","text":""},{"location":"da/z0_estimator/#1-egdfeldf-pdf-maximum","title":"1. EGDF/ELDF (PDF Maximum)","text":"<pre><code>from machinegnostics.magcal import EGDF, Z0Estimator\n\n# Fit your distribution\negdf = EGDF(data=your_data)\negdf.fit()\n\n# Estimate Z0\nestimator = Z0Estimator(egdf, verbose=True)\nz0 = estimator.fit()\nprint(f\"Z0 at PDF maximum: {z0}\")\n</code></pre>"},{"location":"da/z0_estimator/#2-qldfqgdf-median05-point","title":"2. QLDF/QGDF (Median/0.5 Point)","text":"<pre><code>from machinegnostics.magcal import QLDF, Z0Estimator\n\n# Fit your Q-distribution\nqldf = QLDF(data=your_data)\nqldf.fit()\n\n# Estimate Z0 at median (0.5)\nestimator = Z0Estimator(qldf, optimize=True, verbose=True)\nz0 = estimator.fit()\nprint(f\"Z0 at median (0.5): {z0}\")\n</code></pre>"},{"location":"da/z0_estimator/#3-simple-vs-advanced-estimation","title":"3. Simple vs Advanced Estimation","text":"<pre><code># Fast discrete estimation\nestimator_simple = Z0Estimator(gdf_object, optimize=False)\nz0_simple = estimator_simple.fit()\n\n# Advanced optimization\nestimator_advanced = Z0Estimator(gdf_object, optimize=True, verbose=True)\nz0_advanced = estimator_advanced.fit()\n</code></pre>"},{"location":"da/z0_estimator/#4-getting-diagnostic-information","title":"4. Getting Diagnostic Information","text":"<pre><code>info = estimator.get_estimation_info()\nprint(f\"Method used: {info['z0_method']}\")\nprint(f\"Target type: {info['target_type']}\")\nprint(f\"Distribution type: {info['gdf_type']}\")\n</code></pre>"},{"location":"da/z0_estimator/#5-visualization","title":"5. Visualization","text":"<pre><code>estimator.plot_z0_analysis()\n# Shows PDF with Z0 point and distribution function/CDF\n</code></pre>"},{"location":"da/z0_estimator/#notes","title":"Notes","text":"<ul> <li>The GDF object must be fitted before passing to <code>Z0Estimator</code>.</li> <li>For Q-distributions, Z0 is the point where the distribution function equals 0.5 (median).</li> <li>For E-distributions, Z0 is the point where the PDF reaches its global maximum.</li> <li>Advanced methods (spline, polynomial, interpolation) are tried in order of reliability.</li> <li>The estimated Z0 is automatically assigned back to the GDF object.</li> <li>Handles flat regions by selecting the middle point.</li> <li>Works with any GDF subclass following the standard interface.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"magnet/magnet/","title":"Magnet: Machine Gnostic Neural Network","text":"<p>Magnet is a next-generation neural network architecture inspired by Mathematical Gnostics (MG). Unlike traditional neural networks that rely on probabilistic backpropagation, Magnet is built on a deterministic, finite, and algebraic foundation\u2014offering new possibilities for robust, interpretable learning.</p>"},{"location":"magnet/magnet/#what-makes-magnet-unique","title":"What Makes Magnet Unique?","text":"<ul> <li>Deterministic Learning: All computations are finite, reproducible, and free from randomness.</li> <li>Event-Level Modeling: Uncertainty and error are handled at the level of individual data events, not just populations.</li> <li>Algebraic Inference: Magnet leverages gnostic algebra and error geometry for transparent, explainable results.</li> <li>Resilient Architecture: Designed to withstand outliers, corrupted data, and distributional shifts.</li> </ul>"},{"location":"magnet/magnet/#roadmap-collaboration","title":"Roadmap &amp; Collaboration","text":"<p>Magnet is currently under active development. Coming soon: - Detailed documentation and architecture diagrams - Implementation guides and code examples - Benchmarks and comparison studies</p> <p>NOTE</p> <p>We welcome collaboration and new ideas! If you\u2019re interested in contributing, sharing feedback, or exploring partnerships, please reach out\u2014your insights can help shape the future of Machine Gnostic neural networks.</p> <p>Stay tuned for updates as we bring the next generation of neural networks to Machine Gnostics!</p> <p>Suggestions for future additions:</p> <ul> <li>Add a high-level diagram or conceptual illustration of Magnet\u2019s architecture.</li> <li>Include a \u201cVision\u201d or \u201cGoals\u201d section describing what Magnet aims to solve compared to existing neural networks.</li> <li>Provide a link or contact for collaboration (email, GitHub, etc.).</li> <li>List planned features or modules (e.g., layers, activation functions, training methods).</li> <li>Share any preliminary results or benchmarks if available.</li> </ul>"},{"location":"metrics/accuracy/","title":"accuracy_score: Classification Accuracy Metric","text":"<p>The <code>accuracy_score</code> function computes the accuracy of classification models by comparing predicted labels to true labels. It is a fundamental metric for evaluating the performance of classifiers in binary and multiclass settings.</p>"},{"location":"metrics/accuracy/#overview","title":"Overview","text":"<p>Accuracy is defined as the proportion of correct predictions among the total number of cases examined. It is a simple yet powerful metric for assessing how well a model is performing, especially when the classes are balanced.</p>"},{"location":"metrics/accuracy/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like or pandas Series Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series Estimated target values as returned by a classifier. Shape: (n_samples,) <code>verbose</code> bool Print detailed progress, warnings, and results <ul> <li>Both <code>y_true</code> and <code>y_pred</code> can be numpy arrays, lists, or pandas Series.</li> <li>If a pandas DataFrame is passed, a <code>ValueError</code> is raised (select a column instead).</li> </ul>"},{"location":"metrics/accuracy/#returns","title":"Returns","text":"<ul> <li>accuracy: <code>float</code>   The accuracy score as a float in the range [0, 1].</li> </ul>"},{"location":"metrics/accuracy/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> </ul>"},{"location":"metrics/accuracy/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import accuracy_score\n\n# Example 1: Using lists\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(accuracy_score(y_true, y_pred))  # Output: 0.8\n\n# Example 2: Using pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(accuracy_score(df['true'], df['pred']))\n</code></pre>"},{"location":"metrics/accuracy/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>The accuracy metric is most informative when the dataset is balanced. For imbalanced datasets, consider additional metrics such as precision, recall, or F1 score.</li> </ul>"},{"location":"metrics/classification_report/","title":"classification_report: Classification Metrics Summary","text":"<p>The <code>classification_report</code> function generates a comprehensive summary of key classification metrics\u2014precision, recall, F1 score, and support\u2014for each class in your dataset. It supports both string and dictionary output formats, making it suitable for both human-readable reports and programmatic analysis.</p>"},{"location":"metrics/classification_report/#overview","title":"Overview","text":"<p>This function provides a detailed breakdown of classifier performance for each class, including:</p> <ul> <li>Precision: Proportion of positive identifications that were actually correct.</li> <li>Recall: Proportion of actual positives that were correctly identified.</li> <li>F1 Score: Harmonic mean of precision and recall.</li> <li>Support: Number of true instances for each class.</li> </ul> <p>It also computes weighted averages across all classes.</p>"},{"location":"metrics/classification_report/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>labels</code> array-like or None None List of labels to include in the report. If None, uses sorted unique labels from y_true and y_pred. <code>target_names</code> list of str or None None Optional display names matching the labels (same order). <code>digits</code> int 2 Number of digits for formatting output. <code>output_dict</code> bool False If True, return output as a dict. If False, return as a formatted string. <code>zero_division</code> {0, 1, 'warn'} 0 Value to return when there is a zero division (no predicted samples for a class). <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/classification_report/#returns","title":"Returns","text":"<ul> <li>report: <code>str</code> or <code>dict</code>   Text summary or dictionary of the precision, recall, F1 score, and support for each class.</li> </ul>"},{"location":"metrics/classification_report/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import classification_report\n\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\n\n# String report\nprint(classification_report(y_true, y_pred))\n\n# Dictionary report\nreport_dict = classification_report(y_true, y_pred, output_dict=True)\nprint(report_dict)\n</code></pre>"},{"location":"metrics/classification_report/#output-example","title":"Output Example","text":"<p>String Output:</p> <pre><code>Class             Precision    Recall   F1-score    Support\n==========================================================\n0                    1.00      0.50      0.67          2\n1                    0.00      0.00      0.00          1\n2                    1.00      1.00      1.00          2\n==========================================================\nAvg/Total            0.80      0.60      0.67          5\n</code></pre> <p>Dictionary Output:</p> <pre><code>{\n  '0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.67, 'support': 2},\n  '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n  '2': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2},\n  'avg/total': {'precision': 0.8, 'recall': 0.6, 'f1-score': 0.67, 'support': 5}\n}\n</code></pre>"},{"location":"metrics/classification_report/#notes","title":"Notes","text":"<ul> <li>The function uses <code>precision_score</code>, <code>recall_score</code>, and <code>f1_score</code> from the Machine Gnostics metrics module for consistency.</li> <li>If <code>target_names</code> is provided, its length must match the number of labels.</li> <li>For imbalanced datasets, the weighted average provides a more informative summary than the unweighted mean.</li> <li>The <code>zero_division</code> parameter controls the behavior when a class has no predicted samples.</li> </ul>"},{"location":"metrics/confusion_matrix/","title":"confusion_matrix: Confusion Matrix Metric","text":"<p>The <code>confusion_matrix</code> function computes the confusion matrix for classification tasks. This metric provides a summary of prediction results, showing how many samples were correctly or incorrectly classified for each class.</p>"},{"location":"metrics/confusion_matrix/#overview","title":"Overview","text":"<p>A confusion matrix is a table that is often used to describe the performance of a classification model. - Each row represents the actual class. - Each column represents the predicted class. - Entry (i, j) is the number of samples with true label i and predicted label j.</p> <p>This metric helps you understand the types of errors your classifier is making and is essential for evaluating classification accuracy, precision, recall, and other related metrics.</p>"},{"location":"metrics/confusion_matrix/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>y_true</code> array-like or pandas Series Ground truth (correct) target values. Shape: (n_samples,) Required <code>y_pred</code> array-like or pandas Series Estimated targets as returned by a classifier. Shape: (n_samples,) Required <code>labels</code> array-like List of labels to index the matrix. If None, uses all labels in sorted order. <code>None</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/confusion_matrix/#returns","title":"Returns","text":"<ul> <li>ndarray   Confusion matrix of shape (n_classes, n_classes). Entry (i, j) is the number of samples with true label i and predicted label j.</li> </ul>"},{"location":"metrics/confusion_matrix/#raises","title":"Raises","text":"<ul> <li>ValueError   If input arrays are not the same shape, are empty, are not 1D, or contain NaN/Inf.</li> </ul>"},{"location":"metrics/confusion_matrix/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import confusion_matrix\n\n# Example 1: Basic usage\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\n# Output:\n# array([[2, 0, 0],\n#        [0, 0, 1],\n#        [1, 0, 2]])\n\n# Example 2: With custom labels\ncm_custom = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\nprint(cm_custom)\n</code></pre>"},{"location":"metrics/confusion_matrix/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, numpy arrays, or pandas Series.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must be 1D, have the same shape, and must not be empty or contain NaN/Inf.</li> <li>If <code>labels</code> is not provided, all unique labels in <code>y_true</code> and <code>y_pred</code> are used in sorted order.</li> <li>The confusion matrix is essential for computing other metrics such as accuracy, precision, recall, and F1-score.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/divI/","title":"divI: Divergence Information (DivI) Metric","text":"<p>The <code>divI</code> function computes the Divergence Information (DivI) metric, a robust measure for evaluating the divergence between observed data and model predictions. This metric is based on gnostic characteristics and is particularly useful for assessing the quality of model fits, especially in the presence of noise or outliers.</p>"},{"location":"metrics/divI/#overview","title":"Overview","text":"<p>Divergence Information (DivI) quantifies how much the information content of the predicted values diverges from that of the true values. Unlike classical divergence measures, DivI leverages gnostic algebra, making it robust to irregularities and non-Gaussian data.</p> <p>Mathematically, DivI is defined as:</p> \\[ \\text{DivI} = \\frac{1}{N} \\sum_{i=1}^N \\frac{I(y_i)}{I(\\hat{y}_i)} \\] <p>where:</p> <ul> <li>\\(I(y_i)\\) is the E-information of the observed value \\(y_i\\),</li> <li>\\(I(\\hat{y}_i)\\) is the E-information of the fitted value \\(\\hat{y}_i\\),</li> <li>\\(N\\) is the number of data points.</li> </ul> <p>DivI compares the information content of the dependent variable and its fit. The better the fit, the closer DivI is to 1. If the fit is highly uncertain or poor, DivI decreases.</p>"},{"location":"metrics/divI/#interpretation","title":"Interpretation","text":"<ul> <li>Higher DivI: Indicates that the fitted values retain more of the information content of the observed data, suggesting a better model fit.</li> <li>Lower DivI: Indicates greater divergence between the distributions of the observed and fitted values, suggesting a poorer fit or higher uncertainty in the model.</li> </ul> <p>DivI is particularly useful in robust model evaluation, as it is less sensitive to outliers and non-normal data distributions.</p>"},{"location":"metrics/divI/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/divI/#returns","title":"Returns","text":"<ul> <li>float   The computed Divergence Information (DivI) value.</li> </ul>"},{"location":"metrics/divI/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/divI/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import divI\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = divI(y, y_fit)\nprint(result)\n</code></pre>"},{"location":"metrics/divI/#notes","title":"Notes","text":"<ul> <li>DivI is calculated using gnostic characteristics, providing a robust way to measure divergence between distributions.</li> <li>The metric is especially useful for model evaluation in real-world scenarios where data may be noisy or contain outliers.</li> <li>In the context of model evaluation, DivI is often used alongside other criteria such as Robust R-squared (RobR2) and the Geometric Mean of Multiplicative Fitting Errors (GMMFE) to provide a comprehensive assessment of model performance.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/evalmet/","title":"evalMet: Composite Evaluation Metric","text":"<p>The <code>evalMet</code> function computes the Evaluation Metric (EvalMet), a composite score that combines three robust criteria\u2014Robust R-squared (RobR2), Geometric Mean of Model Fit Error (GMMFE), and Divergence Information (DivI)\u2014to provide a comprehensive assessment of model performance.</p>"},{"location":"metrics/evalmet/#overview","title":"Overview","text":"<p>EvalMet is designed to quantify the overall quality of a model fit by integrating three complementary metrics:</p> <ul> <li>RobR2: Measures the proportion of variance explained by the model, robust to outliers.</li> <li>GMMFE: Captures the average multiplicative fitting error on a logarithmic scale.</li> <li>DivI: Quantifies the divergence in information content between the observed data and the model fit.</li> </ul> <p>The combined metric is calculated as:</p> \\[ \\text{EvalMet} = \\frac{\\text{RobR2}}{\\text{GMMFE} \\cdot \\text{DivI}} \\] <p>A higher EvalMet value indicates a better model fit, balancing explained variance, error magnitude, and information divergence.</p>"},{"location":"metrics/evalmet/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y</code> np.ndarray \u2014 Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray \u2014 Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>w</code> np.ndarray None Optional weights for data points. 1D array, same shape as <code>y</code>. If not provided, equal weights are used. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/evalmet/#returns","title":"Returns","text":"<ul> <li>float   The computed Evaluation Metric (EvalMet) value.</li> </ul>"},{"location":"metrics/evalmet/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>w</code> is provided and does not have the same shape as <code>y</code>.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/evalmet/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import evalMet\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = evalMet(y, y_fit)\nprint(result)\n</code></pre>"},{"location":"metrics/evalmet/#notes","title":"Notes","text":"<ul> <li>EvalMet is most informative when used to compare multiple models or methods on the same dataset.</li> <li>The metric is robust to outliers and non-Gaussian data due to its use of gnostic algebra.</li> <li>EvalMet is especially useful in benchmarking and model selection scenarios, as it integrates multiple aspects of fit quality into a single score.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/f1_score/","title":"f1_score: Classification F1 Score Metric","text":"<p>The <code>f1_score</code> function computes the F1 score for classification models, supporting both binary and multiclass settings. The F1 score is the harmonic mean of precision and recall, providing a balanced measure that is especially useful when classes are imbalanced.</p>"},{"location":"metrics/f1_score/#overview","title":"Overview","text":"<p>The F1 score combines precision and recall into a single metric by taking their harmonic mean.</p> <p>This metric is particularly important when you want to balance the trade-off between precision and recall, such as in information retrieval, medical diagnosis, and fraud detection.</p>"},{"location":"metrics/f1_score/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred. <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/f1_score/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the F1 score for each class as an array.</li> </ul>"},{"location":"metrics/f1_score/#returns","title":"Returns","text":"<ul> <li>f1: <code>float</code> or <code>array of floats</code>   F1 score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of F1 values for each class.</li> </ul>"},{"location":"metrics/f1_score/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"metrics/f1_score/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import f1_score\n\n# Example 1: Macro-averaged F1 for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(f1_score(y_true, y_pred, average='macro'))\n\n# Example 2: Binary F1 with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(f1_score(df['true'], df['pred'], average='binary'))\n</code></pre>"},{"location":"metrics/f1_score/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"metrics/g_auto_corelation/","title":"auto_correlation: Gnostic Auto-Correlation Metric","text":"<p>The <code>auto_correlation</code> function computes the Gnostic auto-correlation coefficient for a data sample. This metric measures the similarity between a data sample and a lagged version of itself, using robust gnostic theory principles for reliable diagnostics\u2014even in the presence of noise or outliers.</p>"},{"location":"metrics/g_auto_corelation/#overview","title":"Overview","text":"<p>Auto-correlation quantifies how much a data sample resembles itself when shifted by a specified lag. Unlike classical auto-correlation, the Gnostic version uses irrelevance measures from gnostic theory, providing robust, assumption-free estimates that reflect the true structure of your data.</p>"},{"location":"metrics/g_auto_corelation/#parameters","title":"Parameters","text":"Parameter Type Description <code>data</code> np.ndarray Data sample (1D numpy array, no NaN/Inf). <code>lag</code> int Lag value (non-negative, less than length of data). Default:<code>0</code>. <code>case</code> str Geometry type:<code>'i'</code> for estimation (EGDF), <code>'j'</code> for quantifying (QGDF). Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default:<code>False</code>."},{"location":"metrics/g_auto_corelation/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic auto-correlation coefficient for the given lag.</li> </ul>"},{"location":"metrics/g_auto_corelation/#raises","title":"Raises","text":"<ul> <li>ValueError   If input is not a 1D numpy array, is empty, contains NaN/Inf, or if lag/case is invalid.</li> </ul>"},{"location":"metrics/g_auto_corelation/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import auto_correlation\n\n# Example 1: Compute auto-correlation for a simple dataset\ndata = np.array([1, 2, 3, 4, 5])\nlag = 1\nauto_corr = auto_correlation(data, lag=lag, case='i', verbose=False)\nprint(f\"Auto-Correlation (lag={lag}, case='i'): {auto_corr}\")\n\n# Example 2: Using quantifying geometry\nauto_corr_j = auto_correlation(data, lag=2, case='j', verbose=True)\nprint(f\"Auto-Correlation (lag=2, case='j'): {auto_corr_j}\")\n</code></pre>"},{"location":"metrics/g_auto_corelation/#notes","title":"Notes","text":"<ul> <li>The metric is robust to data uncertainty, noise, and outliers.</li> <li>Input data must be preprocessed and cleaned for optimal results.</li> <li>If data homogeneity is not met, the function automatically adjusts scale parameters for better reliability.</li> <li>The Gnostic auto-correlation uses irrelevance measures rather than classical means, providing deeper insight into temporal relationships in your data.</li> <li>Supports both estimation and quantification geometries for flexible analysis.</li> </ul>"},{"location":"metrics/g_auto_corelation/#gnostic-vs-classical-auto-correlation","title":"Gnostic vs. Classical Auto-Correlation","text":"<p>Note: Unlike classical auto-correlation metrics that rely on statistical means, the Gnostic auto-correlation uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true temporal relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/g_auto_covariance/","title":"auto_covariance: Gnostic Auto-Covariance Metric","text":"<p>The <code>auto_covariance</code> function computes the Gnostic auto-covariance coefficient for a data sample. This metric measures the relationship between a data sample and a lagged version of itself, using robust gnostic theory principles for reliable diagnostics\u2014even in the presence of noise or outliers.</p>"},{"location":"metrics/g_auto_covariance/#overview","title":"Overview","text":"<p>Auto-covariance quantifies how much a data sample co-varies with itself when shifted by a specified lag. Unlike classical auto-covariance, the Gnostic version uses irrelevance measures from gnostic theory, providing robust, assumption-free estimates that reflect the true structure of your data.</p>"},{"location":"metrics/g_auto_covariance/#parameters","title":"Parameters","text":"Parameter Type Description <code>data</code> np.ndarray Data sample (1D numpy array, no NaN/Inf). Represents a time series or sequential data. <code>lag</code> int Lag value (non-negative, less than length of data). Default:<code>0</code>. <code>case</code> str Geometry type:<code>'i'</code> for estimation (EGDF), <code>'j'</code> for quantifying (QGDF). Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default:<code>False</code>."},{"location":"metrics/g_auto_covariance/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic auto-covariance coefficient for the given lag. If the computed value is less than 1e-6, it is set to 0.0.</li> </ul>"},{"location":"metrics/g_auto_covariance/#raises","title":"Raises","text":"<ul> <li>ValueError   If input is not a 1D numpy array, is empty, contains NaN/Inf, or if lag/case is invalid.</li> </ul>"},{"location":"metrics/g_auto_covariance/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import auto_covariance\n\n# Example 1: Compute auto-covariance for a simple dataset\ndata = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\nlag = 1\nauto_covar = auto_covariance(data, lag=lag, case='i')\nprint(f\"Auto-covariance with lag={lag}: {auto_covar}\")\n\n# Example 2: Compute auto-covariance with QGDF\nauto_covar_j = auto_covariance(data, lag=2, case='j')\nprint(f\"Auto-covariance with lag=2: {auto_covar_j}\")\n\n# Example 3: Handle invalid input\ndata_invalid = np.array([1.0, np.nan, 3.0, 4.0, 5.0])\ntry:\n    auto_covar = auto_covariance(data_invalid, lag=1, case='i')\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"metrics/g_auto_covariance/#notes","title":"Notes","text":"<ul> <li>The function uses Gnostic theory to compute irrelevance values for the data and its lagged version.</li> <li>Irrelevance values are clipped to avoid overflow, with a maximum value of 1e12.</li> <li>Homogeneity checks are performed on the data and its lagged version. If the data is not homogeneous, warnings are raised and scale parameters are adjusted.</li> <li>The metric is robust to data uncertainty, noise, and outliers.</li> <li>Input data must be preprocessed and cleaned for optimal results.</li> </ul>"},{"location":"metrics/g_auto_covariance/#gnostic-vs-classical-auto-covariance","title":"Gnostic vs. Classical Auto-Covariance","text":"<p>Note: Unlike classical auto-covariance metrics that rely on statistical means, the Gnostic auto-covariance uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true temporal relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_correlation/","title":"correlation: Gnostic Correlation Metric","text":"<p>The <code>correlation</code> function computes the Gnostic correlation coefficient between two data samples. This metric provides a robust, assumption-free measure of association, leveraging Mathematical Gnostics to deliver reliable results even in the presence of noise, outliers, or non-Gaussian data.</p>"},{"location":"metrics/g_correlation/#overview","title":"Overview","text":"<p>The Gnostic correlation metric measures the association between two arrays using either estimation geometry (EGDF) or quantifying geometry (QGDF):</p> <ul> <li>Case <code>'i'</code>: Uses estimation geometry (EGDF) for correlation.</li> <li>Case <code>'j'</code>: Uses quantifying geometry (QGDF) for correlation.</li> </ul> <p>Unlike classical correlation coefficients, the Gnostic approach is resilient to data uncertainty and is meaningful for both small and large datasets.</p>"},{"location":"metrics/g_correlation/#parameters","title":"Parameters","text":"Parameter Type Description <code>X</code> array-like Feature data sample (1D array or single column from 2D array, no NaN/Inf). <code>y</code> array-like Target data sample (1D array, no NaN/Inf). <code>case</code> str <code>'i'</code> for estimation geometry, <code>'j'</code> for quantifying geometry. Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default: <code>False</code>."},{"location":"metrics/g_correlation/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic correlation coefficient between the two data samples.</li> </ul>"},{"location":"metrics/g_correlation/#raises","title":"Raises","text":"<ul> <li>TypeError   If <code>X</code> or <code>y</code> are not array-like.</li> <li>ValueError   If input arrays are empty, contain NaN/Inf, are not 1D, have mismatched shapes, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"metrics/g_correlation/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import correlation\nimport numpy as np\n\n# Example 1: Simple 1D arrays\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([5, 4, 3, 2, 1])\ncorr = correlation(X, y, case='i')\nprint(corr)\n\n# Example 2: Multi-column X\nX = np.array([[1, 10], [2, 20], [3, 30], [4, 40], [5, 50]])\ny = np.array([5, 4, 3, 2, 1])\nfor i in range(X.shape[1]):\n    corr = correlation(X[:, i], y)\n    print(f\"Correlation for column {i}: {corr}\")\n</code></pre>"},{"location":"metrics/g_correlation/#notes","title":"Notes","text":"<ul> <li>Both <code>X</code> and <code>y</code> must be 1D arrays of the same length, with no missing or infinite values.</li> <li>For multi-column <code>X</code>, pass each column separately (e.g., <code>X[:, i]</code>).</li> <li>The metric is robust to outliers and provides meaningful estimates even for small or noisy datasets.</li> <li>If data homogeneity is not met, the function will adjust parameters and issue a warning for best results.</li> <li>For optimal results, ensure your data is preprocessed and cleaned.</li> </ul>"},{"location":"metrics/g_correlation/#gnostic-vs-classical-correlation","title":"Gnostic vs. Classical Correlation","text":"<p>Note: Unlike classical correlation metrics that rely on statistical means and linear relationships, the Gnostic correlation uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true data relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_cross_variance/","title":"Gnostic Cross-Covariance Metric","text":"<p>The <code>cross_covariance</code> function computes the Gnostic cross-covariance between two data samples. This metric uses gnostic theory to provide robust, assumption-free estimates of relationships between datasets, even in the presence of noise or outliers. Argument names have changed: use <code>X</code> and <code>y</code> for input arrays.</p>"},{"location":"metrics/g_cross_variance/#overview","title":"Overview","text":"<p>Gnostic cross-covariance generalizes classical covariance by leveraging irrelevance measures from gnostic theory:</p> <ul> <li>Estimating irrelevances are aggregated as trigonometric sines (case <code>'i'</code>).</li> <li>Quantifying irrelevances are aggregated as hyperbolic sines (case <code>'j'</code>).</li> </ul> <p>Both approaches converge to linear error in cases of weak uncertainty, but provide robust diagnostics in challenging data scenarios. The metric is computed as the mean product of irrelevances between two data samples.</p>"},{"location":"metrics/g_cross_variance/#parameters","title":"Parameters","text":"Parameter Type Description <code>X</code> np.ndarray Feature data sample (1D numpy array, no NaN/Inf). For multi-column X, pass each column separately. <code>y</code> np.ndarray Target data sample (1D numpy array, no NaN/Inf). <code>case</code> str Geometry type: <code>'i'</code> for estimation (EGDF), <code>'j'</code> for quantifying (QGDF). Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default: <code>False</code>."},{"location":"metrics/g_cross_variance/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic cross-covariance between the two data samples.</li> </ul>"},{"location":"metrics/g_cross_variance/#raises","title":"Raises","text":"<ul> <li>ValueError   If input arrays are not the same length, are empty, contain NaN/Inf, are not 1D, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul> <p>covar = cross_covariance(data_1, data_2, case='i', verbose=False)</p>"},{"location":"metrics/g_cross_variance/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import cross_covariance\n\n# Example 1: Compute cross-covariance for two simple datasets\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([5, 4, 3, 2, 1])\ncovar = cross_covariance(X, y, case='i', verbose=False)\nprint(f\"Cross-Covariance (case='i'): {covar}\")\n\n# Example 2: For multi-column X\nX = np.array([[1, 10], [2, 20], [3, 30], [4, 40], [5, 50]])\ny = np.array([5, 4, 3, 2, 1])\nfor i in range(X.shape[1]):\n  covar = cross_covariance(X[:, i], y)\n  print(f\"Cross-Covariance for column {i}: {covar}\")\n\n# Example 3: Handle invalid input\nX_invalid = np.array([1, np.nan, 3, 4, 5])\ntry:\n  covar = cross_covariance(X_invalid, y, case='i')\nexcept ValueError as e:\n  print(f\"Error: {e}\")\n</code></pre>"},{"location":"metrics/g_cross_variance/#notes","title":"Notes","text":"<ul> <li><code>X</code> must be a 1D numpy array (single column). For multi-column X, pass each column separately (e.g., <code>X[:, i]</code>).</li> <li><code>y</code> must be a 1D numpy array.</li> <li>Both arrays must be of the same length, with no NaN or Inf values.</li> <li>The metric is robust to data uncertainty and provides meaningful estimates even in the presence of noise or outliers.</li> <li>Ensure that the input data is preprocessed and cleaned for optimal results.</li> <li>If data homogeneity is not met, the function automatically adjusts scale parameters for better reliability.</li> <li>The Gnostic cross-covariance uses irrelevance measures rather than classical means, providing deeper insight into relationships between datasets.</li> <li>Supports both estimation and quantification geometries for flexible analysis.</li> </ul>"},{"location":"metrics/g_cross_variance/#gnostic-vs-classical-covariance","title":"Gnostic vs. Classical Covariance","text":"<p>Note: Unlike classical covariance metrics that rely on statistical means, the Gnostic cross-covariance uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar    Date: 2025-09-24</p>"},{"location":"metrics/g_mean/","title":"mean: Gnostic Mean Metric","text":"<p>The <code>mean</code> function computes the Gnostic mean (Local Estimate of Location) of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of central tendency, leveraging irrelevance and fidelity measures for deeper insight into data structure and uncertainty.</p>"},{"location":"metrics/g_mean/#overview","title":"Overview","text":"<p>Gnostic mean generalizes classical mean by using irrelevance and fidelity measures:</p> <ul> <li>Case <code>'i'</code>: Estimates mean using ELDF (Empirical Likelihood Distribution Function).</li> <li>Case <code>'j'</code>: Quantifies mean using QLDF (Quantile Likelihood Distribution Function).</li> </ul> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios.</p>"},{"location":"metrics/g_mean/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>S</code> float/str Scaling parameter for ELDF/QLDF (<code>float</code> or <code>'auto'</code>). <code>1</code> <code>case</code> str <code>'i'</code> for estimating mean (ELDF), <code>'j'</code> for quantifying mean (QLDF). <code>'i'</code> <code>z0_optimize</code> bool Whether to optimize z0 in ELDF/QLDF. <code>True</code> <code>data_form</code> str Data form for ELDF/QLDF:<code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for ELDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_mean/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic mean of the data.</li> </ul>"},{"location":"metrics/g_mean/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf input is not a numpy array, or if <code>S</code> is not a float or <code>'auto'</code>.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code>/<code>data_form</code> is invalid.</li> </ul>"},{"location":"metrics/g_mean/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic mean (default case)\ndata = np.array([1, 2, 3, 4, 5])\nmean_value = mg.mean(data)\nprint(mean_value)\n\n# Example 2: Quantifying mean with QLDF\nmean_j = mg.mean(data, case='j')\nprint(mean_j)\n</code></pre>"},{"location":"metrics/g_mean/#notes","title":"Notes","text":"<ul> <li>The function uses ELDF or QLDF to compute irrelevance and fidelity values, which are then used to estimate the mean.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical mean.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> </ul>"},{"location":"metrics/g_mean/#gnostic-vs-classical-mean","title":"Gnostic vs. Classical Mean","text":"<p>Note: Unlike classical mean metrics that use statistical averages, the Gnostic mean is computed using irrelevance and fidelity measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Authors: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_median/","title":"median: Gnostic Median Metric","text":"<p>The <code>median</code> function computes the Gnostic median (Global Estimate of Location) of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of central tendency, leveraging irrelevance and fidelity measures for deeper insight into data structure and uncertainty.</p>"},{"location":"metrics/g_median/#overview","title":"Overview","text":"<p>Gnostic median generalizes classical median by using irrelevance and fidelity measures:</p> <ul> <li>Case <code>'i'</code>: Estimates median using EGDF (Empirical Gnostics Distribution Function).</li> <li>Case <code>'j'</code>: Quantifies median using QGDF (Quantile Gnostics Distribution Function).</li> </ul> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios.</p>"},{"location":"metrics/g_median/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>case</code> str <code>'i'</code> for estimating median (EGDF), <code>'j'</code> for quantifying median (QGDF). <code>'i'</code> <code>S</code> float Scaling parameter for EGDF/QGDF. <code>1</code> <code>z0_optimize</code> bool Whether to optimize z0 in EGDF/QGDF. <code>True</code> <code>data_form</code> str Data form for EGDF/QGDF:<code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for EGDF/QGDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_median/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic median of the data.</li> </ul>"},{"location":"metrics/g_median/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf input is not a numpy array, or if <code>S</code> is not a float or <code>'auto'</code>.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code>/<code>data_form</code> is invalid.</li> </ul>"},{"location":"metrics/g_median/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic median (default case)\ndata = np.array([1, 2, 3, 4, 5])\nmedian_value = mg.median(data)\nprint(median_value)\n\n# Example 2: Quantifying median with QGDF\nmedian_j = mg.median(data, case='j')\nprint(median_j)\n</code></pre>"},{"location":"metrics/g_median/#notes","title":"Notes","text":"<ul> <li>The function uses EGDF or QGDF to compute irrelevance and fidelity values, which are then used to estimate the median.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical median.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> </ul>"},{"location":"metrics/g_median/#gnostic-vs-classical-median","title":"Gnostic vs. Classical Median","text":"<p>Note: Unlike classical median metrics that use statistical order statistics, the Gnostic median is computed using irrelevance and fidelity measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/g_relevance/","title":"hc: Gnostic Characteristics (Hc) Metric","text":"<p>The <code>hc</code> function computes the Gnostic Characteristics (Hc) metric for a set of true and predicted values. This metric evaluates the relevance or irrelevance of predictions using gnostic theory, providing robust, assumption-free diagnostics for model performance.</p>"},{"location":"metrics/g_relevance/#overview","title":"Overview","text":"<p>The Hc metric measures the gnostic relevance or irrelevance between true and predicted values:</p> <ul> <li>Case <code>'i'</code>: Estimates gnostic relevance. Values close to one indicate less relevance. Range: [0, 1].</li> <li>Case <code>'j'</code>: Estimates gnostic irrelevance. Values close to 1 indicate less irrelevance. Range: [0, \u221e).</li> </ul> <p>Unlike classical metrics, Hc uses gnostic algebra to provide deeper insight into the relationship between predictions and actual outcomes, especially in the presence of outliers or non-normal data.</p>"},{"location":"metrics/g_relevance/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (list, tuple, or numpy array). <code>y_pred</code> array-like Predicted values (list, tuple, or numpy array). <code>case</code> str <code>'i'</code> for relevance, <code>'j'</code> for irrelevance. Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default:<code>False</code>."},{"location":"metrics/g_relevance/#returns","title":"Returns","text":"<ul> <li>float   The calculated Hc value (normalized sum of squared gnostic characteristics).</li> </ul>"},{"location":"metrics/g_relevance/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like.</li> <li>ValueError   If inputs are empty, contain NaN/Inf, are not 1D, have mismatched shapes, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"metrics/g_relevance/#example-usage","title":"Example Usage","text":"<pre><code>from mango.metrics import hc\n\n# Example 1: Using lists\ny_true = [1, 2, 3]\ny_pred = [1, 2, 3]\nhc_value = hc(y_true, y_pred, case='i')\nprint(hc_value)\n\n# Example 2: Using numpy arrays and irrelevance case\nimport numpy as np\ny_true = np.array([2, 4, 6])\ny_pred = np.array([1, 2, 3])\nhc_value = hc(y_true, y_pred, case='j', verbose=True)\nprint(hc_value)\n</code></pre>"},{"location":"metrics/g_relevance/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must be 1D, have the same shape, and must not be empty or contain NaN/Inf.</li> <li>For standard comparison, irrelevances are calculated with S=1.</li> <li>The Hc metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical metrics.</li> </ul>"},{"location":"metrics/g_relevance/#gnostic-vs-classical-metrics","title":"Gnostic vs. Classical Metrics","text":"<p>Note: Unlike traditional metrics that use statistical means, the Hc metric is computed using gnostic algebra and characteristics. This approach is assumption-free and designed to reveal the true diagnostic properties of your data and model predictions.</p> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_std/","title":"std: Gnostic Standard Deviation Metric","text":"<p>The <code>std</code> function computes the Gnostic standard deviation of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of data dispersion, leveraging irrelevance and fidelity measures for deeper insight into uncertainty and structure.</p>"},{"location":"metrics/g_std/#overview","title":"Overview","text":"<p>Gnostic standard deviation generalizes classical standard deviation by using irrelevance and fidelity measures: - Case <code>'i'</code>: Estimates standard deviation using the estimating geometry. - Case <code>'j'</code>: Quantifies standard deviation using the quantifying geometry.</p> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios. The function returns lower and upper bounds for the standard deviation.</p>"},{"location":"metrics/g_std/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>case</code> str <code>'i'</code> for estimating standard deviation, <code>'j'</code> for quantifying standard deviation. <code>'i'</code> <code>S</code> float/str Scaling parameter for ELDF. Can be <code>'auto'</code> to optimize using EGDF. Suggested range: [0.01, 2]. <code>'auto'</code> <code>z0_optimize</code> bool Whether to optimize z0 in ELDF. <code>True</code> <code>data_form</code> str Data form for ELDF: <code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for ELDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_std/#returns","title":"Returns","text":"<ul> <li>tuple   Lower and upper bounds of the Gnostic standard deviation.</li> </ul>"},{"location":"metrics/g_std/#raises","title":"Raises","text":"<ul> <li>TypeError   If input is not a numpy array, or if <code>S</code> is not a float or <code>'auto'</code>.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code>/<code>data_form</code> is invalid.</li> </ul>"},{"location":"metrics/g_std/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic standard deviation (default case)\ndata = np.array([1, 2, 3, 4, 5])\nstd_lb, std_ub = mg.std(data)\nprint(std_lb, std_ub)\n\n# Example 2: Quantifying standard deviation\nstd_lb_j, std_ub_j = mg.std(data, case='j')\nprint(std_lb_j, std_ub_j)\n</code></pre>"},{"location":"metrics/g_std/#notes","title":"Notes","text":"<ul> <li>The function uses mean and variance from gnostic theory to compute lower and upper bounds for standard deviation.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical standard deviation.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> <li>If <code>S='auto'</code>, the function optimizes the scaling parameter using EGDF.</li> </ul>"},{"location":"metrics/g_std/#gnostic-vs-classical-standard-deviation","title":"Gnostic vs. Classical Standard Deviation","text":"<p>Note: Unlike classical standard deviation metrics that use statistical means and variances, the Gnostic standard deviation is computed using irrelevance and fidelity measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Authors: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_variance/","title":"variance: Gnostic Variance Metric","text":"<p>The <code>variance</code> function computes the Gnostic variance of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of data variability, leveraging irrelevance measures for deeper insight into uncertainty and structure.</p>"},{"location":"metrics/g_variance/#overview","title":"Overview","text":"<p>Gnostic variance generalizes classical variance by using irrelevance measures:</p> <ul> <li>Case <code>'i'</code>: Estimates variance using ELDF (Empirical Likelihood Distribution Function).</li> <li>Case <code>'j'</code>: Quantifies variance using QLDF (Quantile Likelihood Distribution Function).</li> </ul> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios.</p>"},{"location":"metrics/g_variance/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>case</code> str <code>'i'</code> for estimating variance (ELDF), <code>'j'</code> for quantifying variance (QLDF). <code>'i'</code> <code>S</code> float Scaling parameter for ELDF/QLDF. <code>1</code> <code>z0_optimize</code> bool Whether to optimize z0 in ELDF/QLDF. <code>True</code> <code>data_form</code> str Data form for ELDF/QLDF:<code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for ELDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_variance/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic variance of the data.</li> </ul>"},{"location":"metrics/g_variance/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf input is not a numpy array.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"metrics/g_variance/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic variance (default case)\ndata = np.array([1, 2, 3, 4, 5])\nvar = mg.variance(data)\nprint(var)\n\n# Example 2: Quantifying variance with QLDF\nvar_j = mg.variance(data, case='j')\nprint(var_j)\n</code></pre>"},{"location":"metrics/g_variance/#notes","title":"Notes","text":"<ul> <li>The function uses ELDF or QLDF to compute irrelevance values, which are then squared and averaged.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical variance.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> </ul>"},{"location":"metrics/g_variance/#gnostic-vs-classical-variance","title":"Gnostic vs. Classical Variance","text":"<p>Note: Unlike classical variance metrics that use statistical means, the Gnostic variance is computed using irrelevance measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Authors: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/gmmfe/","title":"gmmfe: Geometric Mean of Model Fit Error (GMMFE) Metric","text":"<p>The <code>gmmfe</code> function computes the Geometric Mean of Model Fit Error (GMMFE), a robust metric for evaluating the average relative error between observed data and model predictions on a logarithmic scale. GMMFE is especially useful for datasets with a wide range of values or when the data is multiplicative in nature.</p>"},{"location":"metrics/gmmfe/#overview","title":"Overview","text":"<p>GMMFE quantifies the average multiplicative error between the true and predicted values, making it less sensitive to outliers and scale differences than classical metrics. It is one of the three core criteria (alongside RobR2 and DivI) for evaluating model performance in the Machine Gnostics framework.</p> <p>Mathematically, GMMFE is defined as:</p> \\[ \\text{GMMFE} = \\exp\\left( \\frac{1}{N} \\sum_{i=1}^N \\left| \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) \\right| \\right) \\] <p>where:</p> <ul> <li>\\(y_i\\) is the observed value,</li> <li>\\(\\hat{y}_i\\) is the fitted (predicted) value,</li> <li>\\(N\\) is the number of data points.</li> </ul> <p>A lower GMMFE indicates a better fit, as it means the geometric mean of the relative errors is smaller.</p>"},{"location":"metrics/gmmfe/#interpretation","title":"Interpretation","text":"<ul> <li>Lower GMMFE: Indicates smaller average multiplicative errors and a better model fit.</li> <li>Higher GMMFE: Indicates larger average multiplicative errors and a poorer fit.</li> </ul> <p>GMMFE is particularly valuable when comparing models across datasets with different scales or when the error distribution is multiplicative.</p>"},{"location":"metrics/gmmfe/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/gmmfe/#returns","title":"Returns","text":"<ul> <li>float   The computed Geometric Mean of Model Fit Error (GMMFE) value.</li> </ul>"},{"location":"metrics/gmmfe/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/gmmfe/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import gmmfe\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = gmmfe(y, y_fit)\nprint(result)\n</code></pre>"},{"location":"metrics/gmmfe/#notes","title":"Notes","text":"<ul> <li>GMMFE is calculated using the weighted geometric mean of the relative errors.</li> <li>It is robust to outliers and scale differences, making it suitable for a wide range of regression problems.</li> <li>In the Machine Gnostics framework, GMMFE is used alongside RobR2 and DivI to provide a comprehensive evaluation of model performance.</li> <li>The overall evaluation metric can be computed as:</li> </ul> <p>$$   \\text{EvalMet} = \\frac{\\text{RobR2}}{\\text{GMMFE} \\cdot \\text{DivI}}   $$</p> <p>where a higher EvalMet indicates better model performance.</p> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"metrics/mae/","title":"mean_absolute_error: Mean Absolute Error (MAE) Metric","text":"<p>The <code>mean_absolute_error</code> function computes the mean absolute error (MAE) between true and predicted values. MAE is a fundamental regression metric that measures the average magnitude of errors in a set of predictions, without considering their direction.</p> <p>Unlike traditional error metrics that use the statistical mean, Machine Gnostics metrics are computed using the gnostic mean. The gnostic mean is a robust, assumption-free measure designed to provide deeper insight and reliability, especially in the presence of outliers or non-normal data. This approach ensures that error metrics reflect the true structure and diagnostic properties of your data, in line with the principles of Mathematical Gnostics.</p>"},{"location":"metrics/mae/#overview","title":"Overview","text":"<p>Mean Absolute Error is defined as the average of the absolute differences between actual and predicted values.</p> <p>MAE is widely used in regression analysis to quantify how close predictions are to the actual outcomes. Lower MAE values indicate better model performance.</p>"},{"location":"metrics/mae/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/mae/#returns","title":"Returns","text":"<ul> <li>float   The average absolute difference between actual and predicted values.</li> </ul>"},{"location":"metrics/mae/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"metrics/mae/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import mean_absolute_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_absolute_error(y_true, y_pred))\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(mean_absolute_error(y_true, y_pred))\n</code></pre>"},{"location":"metrics/mae/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>MAE is robust to outliers but does not penalize large errors as strongly as mean squared error (MSE).</li> </ul> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"metrics/mse/","title":"mean_squared_error: Mean Squared Error (MSE) Metric","text":"<p>The <code>mean_squared_error</code> function computes the mean squared error (MSE) between true and predicted values. MSE is a fundamental regression metric that measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value.</p> <p>Unlike traditional error metrics that use the statistical mean, Machine Gnostics metrics are computed using the gnostic mean. The gnostic mean is a robust, assumption-free measure designed to provide deeper insight and reliability, especially in the presence of outliers or non-normal data. This approach ensures that error metrics reflect the true structure and diagnostic properties of your data, in line with the principles of Mathematical Gnostics.</p>"},{"location":"metrics/mse/#overview","title":"Overview","text":"<p>Mean Squared Error is defined as the average of the squared differences between actual and predicted values.</p> <p>MSE is widely used in regression analysis to quantify the accuracy of predictions. Lower MSE values indicate better model performance, while higher values indicate larger errors.</p>"},{"location":"metrics/mse/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/mse/#returns","title":"Returns","text":"<ul> <li>float   The average of squared differences between actual and predicted values.</li> </ul>"},{"location":"metrics/mse/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"metrics/mse/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import mean_squared_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_squared_error(y_true, y_pred))\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(mean_squared_error(y_true, y_pred))\n</code></pre>"},{"location":"metrics/mse/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>MSE penalizes larger errors more than MAE (mean absolute error), making it sensitive to outliers.</li> </ul> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"metrics/precision/","title":"precision_score: Classification Precision Metric","text":"<p>The <code>precision_score</code> function computes the precision of classification models, supporting both binary and multiclass settings. Precision measures the proportion of positive identifications that were actually correct, making it a key metric for evaluating classifiers, especially when the cost of false positives is high.</p>"},{"location":"metrics/precision/#overview","title":"Overview","text":"<p>Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP).</p> <p>This metric is especially important in scenarios where false positives are more costly than false negatives (e.g., spam detection, medical diagnosis).</p>"},{"location":"metrics/precision/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred. <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/precision/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the precision for each class as an array.</li> </ul>"},{"location":"metrics/precision/#returns","title":"Returns","text":"<ul> <li>precision: <code>float</code> or <code>array of floats</code>   Precision score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of precision values for each class.</li> </ul>"},{"location":"metrics/precision/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"metrics/precision/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import precision_score\n\n# Example 1: Macro-averaged precision for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(precision_score(y_true, y_pred, average='macro'))\n\n# Example 2: Binary precision with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(precision_score(df['true'], df['pred'], average='binary')) \n</code></pre>"},{"location":"metrics/precision/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"metrics/r2/","title":"r2_score: Coefficient of Determination (R\u00b2) Metric","text":"<p>The <code>r2_score</code> function computes the coefficient of determination (R\u00b2) for regression tasks. This metric measures the proportion of variance in the target variable that is explained by the model.</p>"},{"location":"metrics/r2/#overview","title":"Overview","text":"<p>R\u00b2 is a standard metric for evaluating regression models: - R\u00b2 = 1 indicates perfect prediction. - R\u00b2 = 0 indicates the model does no better than the mean. - R\u00b2 &lt; 0 indicates the model performs worse than the mean.</p>"},{"location":"metrics/r2/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>y_true</code> array-like True values (targets). Required <code>y_pred</code> array-like Predicted values. Required <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/r2/#returns","title":"Returns","text":"<ul> <li>float   Proportion of variance explained (1 is perfect prediction).</li> </ul>"},{"location":"metrics/r2/#raises","title":"Raises","text":"<ul> <li>TypeError   If inputs are not array-like.</li> <li>ValueError   If shapes do not match or inputs are empty.</li> </ul>"},{"location":"metrics/r2/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import r2_score\n\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nscore = r2_score(y_true, y_pred)\nprint(score)\n</code></pre>"},{"location":"metrics/r2/#adjusted_r2_score-adjusted-r2-metric","title":"adjusted_r2_score: Adjusted R\u00b2 Metric","text":"<p>The <code>adjusted_r2_score</code> function computes the adjusted R\u00b2 score, which accounts for the number of predictors in the model. This metric penalizes the addition of unnecessary features.</p>"},{"location":"metrics/r2/#overview_1","title":"Overview","text":"<p>Adjusted R\u00b2 is useful for comparing models with different numbers of predictors: - Adjusted R\u00b2 increases only if the new predictor improves the model more than would be expected by chance.</p>"},{"location":"metrics/r2/#parameters_1","title":"Parameters","text":"Parameter Type Description Default <code>y_true</code> array-like True values (targets). Required <code>y_pred</code> array-like Predicted values. Required <code>n_features</code> int Number of features (independent variables) in the model. Required <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/r2/#returns_1","title":"Returns","text":"<ul> <li>float   Adjusted R\u00b2 accounting for number of predictors.</li> </ul>"},{"location":"metrics/r2/#raises_1","title":"Raises","text":"<ul> <li>TypeError   If inputs are not array-like or <code>n_features</code> is not a non-negative integer.</li> <li>ValueError   If shapes do not match, inputs are empty, or <code>n_features</code> is invalid.</li> </ul>"},{"location":"metrics/r2/#example-usage_1","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import adjusted_r2_score\n\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nn_features = 2\nscore_adj = adjusted_r2_score(y_true, y_pred, n_features)\nprint(score_adj)\n</code></pre>"},{"location":"metrics/r2/#notes","title":"Notes","text":"<ul> <li>Both functions support input as lists, numpy arrays, or pandas Series.</li> <li>Inputs must be 1D, have the same shape, and must not be empty or contain NaN/Inf.</li> <li>Adjusted R\u00b2 is undefined if the number of samples is less than or equal to the number of features plus one.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/r2_score/","title":"robr2: Robust R-squared (RobR2) Metric","text":"<p>The <code>robr2</code> function computes the Robust R-squared (RobR2) value for evaluating the goodness of fit between observed data and model predictions. Unlike the classical R-squared metric, RobR2 is robust to outliers and incorporates sample weights, making it ideal for noisy or irregular datasets.</p>"},{"location":"metrics/r2_score/#overview","title":"Overview","text":"<p>Robust R-squared (RobR2) measures the proportion of variance in the observed data explained by the fitted data, while reducing sensitivity to outliers. This is achieved by using a weighted formulation, which allows for more reliable model evaluation in real-world scenarios where data may not be perfectly clean.</p>"},{"location":"metrics/r2_score/#formula","title":"Formula","text":"\\[ \\text{RobR2} = 1 - \\frac{\\sum_i w_i (e_i - \\bar{e})^2}{\\sum_i w_i (y_i - \\bar{y})^2} \\] <p>Where:</p> <ul> <li>\\(e_i = y_i - \\hat{y}_i\\) (residuals)</li> <li>\\(\\bar{e}\\) = weighted mean of residuals</li> <li>\\(\\bar{y}\\) = weighted mean of observed data</li> <li>\\(w_i\\) = weight for each data point</li> </ul> <p>If weights are not provided, equal weights are assumed.</p>"},{"location":"metrics/r2_score/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>w</code> np.ndarray or None Optional weights for data points. 1D array, same shape as <code>y</code>. If None, equal weights are used. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/r2_score/#returns","title":"Returns","text":"<ul> <li>float   The computed Robust R-squared (RobR2) value. Ranges from 0 (no explanatory power) to 1 (perfect fit).</li> </ul>"},{"location":"metrics/r2_score/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>w</code> is provided and does not have the same shape as <code>y</code>.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/r2_score/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import robr2\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nw = np.array([1.0, 1.0, 1.0, 1.0])\n\nresult = robr2(y, y_fit, w)\nprint(result)\n</code></pre>"},{"location":"metrics/r2_score/#comparison-with-classical-r-squared","title":"Comparison with Classical R-squared","text":"<ul> <li>Classical R-squared: Assumes equal weights and is sensitive to outliers.</li> <li>RobR2: Incorporates weights and is robust to outliers, making it more reliable for datasets with irregularities or noise.</li> </ul>"},{"location":"metrics/r2_score/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis, Chapter 19</li> </ul>"},{"location":"metrics/r2_score/#notes","title":"Notes","text":"<ul> <li>If weights are not provided, the metric defaults to equal weighting for all data points.</li> <li>RobR2 is particularly useful for robust regression and model evaluation in the presence of outliers.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/recall/","title":"recall_score: Classification Recall Metric","text":"<p>The <code>recall_score</code> function computes the recall of classification models, supporting both binary and multiclass settings. Recall measures the proportion of actual positives that were correctly identified, making it a key metric for evaluating classifiers, especially when the cost of false negatives is high.</p>"},{"location":"metrics/recall/#overview","title":"Overview","text":"<p>Recall is defined as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).</p> <p>This metric is especially important in scenarios where false negatives are more costly than false positives (e.g., disease screening, fraud detection).</p>"},{"location":"metrics/recall/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred. <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/recall/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the recall for each class as an array.</li> </ul>"},{"location":"metrics/recall/#returns","title":"Returns","text":"<ul> <li>recall: <code>float</code> or <code>array of floats</code>   Recall score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of recall values for each class.</li> </ul>"},{"location":"metrics/recall/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"metrics/recall/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import recall_score\n\n# Example 1: Macro-averaged recall for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(recall_score(y_true, y_pred, average='macro'))\n\n# Example 2: Binary recall with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(recall_score(df['true'], df['pred'], average='binary'))\n</code></pre>"},{"location":"metrics/recall/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"metrics/rmse/","title":"root_mean_squared_error: Root Mean Squared Error (RMSE) Metric","text":"<p>The <code>root_mean_squared_error</code> function computes the Root Mean Squared Error (RMSE) between true and predicted values. RMSE is a widely used regression metric that measures the square root of the average of the squared differences between predicted and actual values.</p> <p>Unlike traditional error metrics that use the statistical mean, Machine Gnostics metrics are computed using the gnostic mean. The gnostic mean is a robust, assumption-free measure designed to provide deeper insight and reliability, especially in the presence of outliers or non-normal data. This approach ensures that error metrics reflect the true structure and diagnostic properties of your data, in line with the principles of Mathematical Gnostics.</p>"},{"location":"metrics/rmse/#overview","title":"Overview","text":"<p>Root Mean Squared Error is defined as the square root of the mean squared error.</p> <p>RMSE provides an interpretable measure of prediction error in the same units as the target variable. Lower RMSE values indicate better model performance.</p>"},{"location":"metrics/rmse/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/rmse/#returns","title":"Returns","text":"<ul> <li>float   The square root of the average of squared errors between actual and predicted values.</li> </ul>"},{"location":"metrics/rmse/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"metrics/rmse/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import root_mean_squared_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(root_mean_squared_error(y_true, y_pred))\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(root_mean_squared_error(y_true, y_pred))\n</code></pre>"},{"location":"metrics/rmse/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>RMSE is sensitive to outliers due to the squaring of errors.</li> <li>RMSE is in the same units as the target variable, making it easy to interpret.</li> </ul> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"mg/architecture/","title":"Machine Gnostics Architecture","text":"<p>This diagram presents the conceptual architecture of the Machine Gnostics paradigm. Unlike traditional machine learning rooted in statistical theory, this new approach is built on the foundation of Mathematical Gnostics (MG)\u2014a finite, deterministic, and physically inspired framework.</p> <p>High-level Architecture Diagram:</p> <pre><code>flowchart TD\n    DATA[\"INPUT\"]\n    USER[\"OUTPUT\"]\n    subgraph MG_SYS[\"Machine Gnostics Architecture\"]\n        IFACE1[\"Machine Gnostics Interface\"]\n        MGTheory[\"Mathematical Gnostics\"]\n        MAGCAL[\"MAGCAL\"]\n        Models[\"Models\"]\n        Metrics[\"Metrics\"]\n        Magnet[\"Magnet\"]\n        MLFlow[\"mlflow Integration\"]\n        IFACE2[\"Machine Gnostics Interface\"]\n    end\n    DATA --&gt; IFACE1\n    IFACE1 --&gt; MGTheory\n    MGTheory --&gt; MAGCAL\n    MAGCAL --&gt; Models\n    MAGCAL --&gt; Metrics\n    MAGCAL --&gt; Magnet\n    Models &lt;--&gt; Metrics\n    Metrics &lt;--&gt; Magnet\n    Models --&gt; MLFlow\n    Metrics --&gt; MLFlow\n    Magnet --&gt; MLFlow\n    MLFlow --&gt; IFACE2\n    IFACE2 --&gt; USER</code></pre> <p>Glossary:</p> <ul> <li>MAGCAL: Mathematical Gnostics Calculations and Data Analysis Models</li> <li>Models: Machine Learning Models</li> <li>Magnet: Machine Gnostics Neural Networks</li> <li>Metrics: Machine Gnostics and Statistical Metrics</li> </ul>"},{"location":"mg/architecture/#1-data","title":"1. DATA","text":"<p>The foundation of Machine Gnostics is DATA, interpreted differently from statistical frameworks:</p> <ul> <li>Each data point is a real event with individual importance and uncertainty.</li> <li>No reliance on large sample assumptions or population-level abstractions.</li> <li>Adheres to the principle: \u201cLet the data speak for themselves.\u201d</li> </ul>"},{"location":"mg/architecture/#2-mathematical-gnostics","title":"2. Mathematical Gnostics","text":"<p>This is the theoretical base of the system. It replaces the assumptions of probability with deterministic modeling:</p> <ul> <li>Uses Riemannian geometry, Einsteinian relativity, vector bi-algebra, and thermodynamics.</li> <li>Models uncertainty at the level of individual events, not populations.</li> <li>Establishes a finite theory for finite data, with robust treatment of variability.</li> </ul>"},{"location":"mg/architecture/#3-magcal-mathematical-gnostics-calculations","title":"3. MAGCAL (Mathematical Gnostics Calculations)","text":"<p>MAGCAL is the computational engine that enables gnostic inference:</p> <ul> <li>Performs deterministic, non-statistical calculations.</li> <li>Enables robust modeling using gnostic algebra and error geometry.</li> <li>Resilient to outliers, corrupted data, and distributional shifts.</li> </ul>"},{"location":"mg/architecture/#4-models-metrics-magnet","title":"4. Models | Metrics | Magnet","text":"<p>This layer maps to familiar components of ML pipelines but with MG-specific logic:</p> <ul> <li>Models: Developed on the principles of Mathematical Gnostics.</li> <li>Metrics: Evaluate using gnostic loss functions and event-level error propagation.</li> <li>Magnet: A novel neural architecture based on Mathematical Gnostics</li> </ul>"},{"location":"mg/architecture/#5-mlflow-integration","title":"5. mlflow Integration","text":"<p>Despite its theoretical novelty, Machine Gnostics fits smoothly into modern ML workflows:</p> <ul> <li>mlflow provides tracking, model registry, and reproducibility.</li> <li>Ensures that experiments and deployments align with standard ML practices.</li> </ul>"},{"location":"mg/architecture/#6-machine-gnostics-integration-layer-for-machine-learning","title":"6. Machine Gnostics (Integration Layer for Machine Learning)","text":"<p>This layer unifies all components into a working system:</p> <ul> <li>MAGCAL is a Mathematical Gnostics based engine.</li> <li>Functions as a complete ML framework based on a deterministic, finite, and algebraic paradigm.</li> <li>Enables seamless data-to-model pipelines rooted in the principles of Mathematical Gnostics.</li> </ul>"},{"location":"mg/architecture/#summary","title":"Summary","text":"<p>Quick Understanding</p> Traditional ML (Statistics) Machine Gnostics Based on probability theory Based on deterministic finite theory Relies on large datasets Works directly with small datasets Uses averages and distributions Uses individual error and event modeling Rooted in Euclidean geometry Rooted in Riemannian geometry &amp; physics Vulnerable to outliers Robust to real-world irregularities"},{"location":"mg/architecture/#references","title":"References","text":"<p>Machine Gnostics is not just an alternative\u2014it is a new foundation for AI, capable of rational, robust, and interpretable data modeling.</p>"},{"location":"mg/concepts/","title":"Foundations of Mathematical Gnostics","text":"<p>Mathematical Gnostics (MG) offers a fundamentally different approach to data analysis and uncertainty compared to traditional mathematical statistics. Understanding these differences is crucial for users of the Machine Gnostics library, as it shapes the philosophy, algorithms, and practical outcomes of gnostic-based data analysis.</p>"},{"location":"mg/concepts/#key-differences-between-statistics-and-gnostics","title":"Key Differences Between Statistics and Gnostics","text":""},{"location":"mg/concepts/#1-focus-on-the-individual-event","title":"1. Focus on the Individual Event","text":"<ul> <li>Statistics: Traditional statistics investigates the regularities and properties of large collections of uncertain events, relying on the Law of Large Numbers and the Central Limit Theorem. The theory is built for infinite or very large sample sizes, and results for finite datasets are often extrapolated from these infinite models.</li> <li>Gnostics:   MG concentrates on the uncertainty of a single event. It builds mathematical and physical models that directly address finite (even small) collections of uncertain events. This approach is more natural for real-world scenarios, where data is always finite.</li> </ul>"},{"location":"mg/concepts/#2-treatment-of-data-and-uncertainty","title":"2. Treatment of Data and Uncertainty","text":"<ul> <li>Statistics: Assumes the existence of a mean and standard deviation for an underlying probability distribution. Data is often treated as samples from an idealized random process, and analysis is based on population-level properties.</li> <li>Gnostics:   Treats each data point as an image of a real, existing event governed by the laws of nature. MG respects the actual values of the data, following the principle: \u201cLet data speak for themselves.\u201d The weight or importance of each data item is determined by its own individual error, not by its class or family.</li> </ul>"},{"location":"mg/concepts/#3-mathematical-and-physical-foundations","title":"3. Mathematical and Physical Foundations","text":"<ul> <li>Statistics: Relies primarily on Euclidean geometry and Newtonian mechanics, with mathematical theory of measure as its foundation.</li> <li>Gnostics:   Utilizes Riemannian geometry and Einstein\u2019s relativistic mechanics, along with vector bi-algebra and thermodynamics. MG also introduces quantification theory as a foundational measurement theory.</li> </ul>"},{"location":"mg/concepts/#4-aggregation-and-analysis","title":"4. Aggregation and Analysis","text":"<ul> <li>Statistics: Aggregates observed data additively, focusing on population-level summaries.</li> <li>Gnostics:   Suggests that additive aggregation should be applied to the parameters of the Ideal Gnostic Cycle, not directly to the observed data.</li> </ul>"},{"location":"mg/concepts/#scientific-bases-a-comparative-diagram","title":"Scientific Bases: A Comparative Diagram","text":"<p>Figure: The scientific foundations of statistics (left) and gnostics (right) span mathematics, physics, geometry, and measurement theory, but differ fundamentally in their approach and underlying principles. [Pavel Kovanic, Mathematical Gnostics (2023)]</p>"},{"location":"mg/concepts/#approaches-to-data-uncertainty","title":"Approaches to Data Uncertainty","text":"<p>Figure: Statistics builds its theory for infinite sample sizes and extrapolates results for finite datasets. Gnostics, in contrast, constructs its theory directly for finite (even single) events, providing a more natural fit for real-world data. [Pavel Kovanic, Mathematical Gnostics (2023)]</p>"},{"location":"mg/concepts/#paradigm-shift-from-statistics-to-gnostics","title":"Paradigm Shift: From Statistics to Gnostics","text":"<p>Mathematical gnostics represents a paradigm shift in how we approach data variability and analysis:</p> <ul> <li>Statistics is rooted in the behavior of large numbers and infinite limits, often requiring extrapolation to address finite datasets.</li> <li>Gnostics is designed for the finite world, modeling uncertainty at the level of individual events and small datasets.</li> </ul> <p>This shift requires a new way of thinking, much like moving from Newtonian to Einsteinian physics. While statistics is easily demonstrated with simple experiments (like coin tosses), the power of gnostics is revealed through its algorithms and their performance on real-world, finite data.</p>"},{"location":"mg/concepts/#principles-of-the-gnostic-paradigm","title":"Principles of the Gnostic Paradigm","text":"<ol> <li>Concentration on Individual Events: MG focuses on the regularities and uncertainty of individual events, not just large populations.</li> <li>Respect for Data Values: Data is taken as it is, with each value carrying its own information and uncertainty.</li> <li>Use of Advanced Geometry and Mechanics: MG employs Riemannian geometry and relativistic mechanics, providing a richer mathematical framework for modeling uncertainty.</li> <li>Individual Error Weighting:    The importance of each data point is determined by its own error, not by group-level properties.</li> </ol>"},{"location":"mg/concepts/#why-adopt-the-gnostic-approach","title":"Why Adopt the Gnostic Approach?","text":"<ul> <li>Natural Fit for Finite Data: Real-world data is always finite. MG provides tools and theory that are directly applicable without relying on extrapolation from infinite models.</li> <li>Robustness: By focusing on individual data points and their uncertainties, MG offers greater resilience to outliers and corrupted data.</li> <li>Paradigm-Changing Power:   MG overcomes many limitations of traditional statistics, especially in cases where statistical assumptions break down.</li> </ul>"},{"location":"mg/concepts/#further-reading","title":"Further Reading","text":"<p>For a deeper dive into the foundations and applications of mathematical gnostics, see:</p> <ul> <li>Pavel Kovanic, Mathematical Gnostics (2023)</li> <li>Pavel Kovanic &amp; M.B. Humber, The Economics of Information: Mathematical Gnostics for Data Analysis (2015)</li> </ul> <p>Mathematical Gnostics is a new paradigm for data analysis\u2014one that respects the individuality of data, leverages advanced mathematics, and is designed for the finite, real world.</p>"},{"location":"mg/gdf/","title":"Gnostic Distribution Functions (GDF)","text":"<p>Gnostic Distribution Functions (GDF) are a new class of probability and density estimators designed for robust, flexible, and assumption-free data analysis. Unlike traditional statistical distributions, GDFs do not require any prior assumptions about the underlying data distribution. Instead, they allow the data to \"speak for themselves,\" making them especially powerful for small, noisy, or uncertain datasets.</p>"},{"location":"mg/gdf/#why-use-gdf","title":"Why Use GDF?","text":"<ul> <li>No A Priori Assumptions: GDFs do not rely on predefined parametric forms or statistical models.</li> <li>Robustness: They are inherently robust to outliers and inner noise, making them suitable for real-world, contaminated, or uncertain data.</li> <li>Flexibility: GDFs adapt to both homogeneous and heterogeneous data samples, providing detailed insights into data structure.</li> <li>Wide Applicability: Useful for probability estimation, density estimation, cluster analysis, and homogeneity testing.</li> </ul>"},{"location":"mg/gdf/#four-types-of-gnostic-distribution-functions","title":"Four Types of Gnostic Distribution Functions","text":"<p>GDFs are organized along two axes:</p> <ul> <li>Local vs. Global: Local functions use weighted irrelevance, while global functions use normalized weights.</li> <li>Estimating vs. Quantifying: Estimating functions use estimating irrelevance, while quantifying functions use quantifying irrelevance.</li> </ul> <p>This results in four types:</p> <ul> <li>ELDF: Estimating Local Distribution Function (outlier-resistant, highly flexible)</li> <li>EGDF: Estimating Global Distribution Function (outlier-resistant, unique for each sample)</li> <li>QLDF: Quantifying Local Distribution Function (inlier-resistant, highly flexible)</li> <li>QGDF: Quantifying Global Distribution Function (inlier-resistant, unique for each sample)</li> </ul>"},{"location":"mg/gdf/#key-concepts","title":"Key Concepts","text":"<ul> <li>Data-Driven: GDFs are parameterized directly by your data, not by external assumptions.</li> <li>Robustness: ELDF and EGDF are robust to outliers, while QLDF and QGDF are robust to inliers (dense clusters).</li> <li>Flexibility: Local functions (ELDF, QLDF) are highly flexible and ideal for cluster analysis and exploring marginal data structures. Global functions (EGDF, QGDF) are best for homogeneity testing and robust probability estimation.</li> <li>Scale Parameter: The flexibility of local functions is controlled by a scale parameter, allowing you to \"zoom in\" on data structure.</li> </ul> <p>Practical Guidance</p> <ul> <li>Use local functions for exploratory, granular analysis and cluster detection.</li> <li>Use global functions for summary, sample-wide analysis and homogeneity testing.</li> <li>Choose estimating functions when robustness to outliers is needed.</li> <li>Choose quantifying functions when robustness to inliers is important.</li> </ul>"},{"location":"mg/gdf/#summary","title":"Summary","text":"<p>GDFs provide robust, flexible tools for probability and density estimation, especially in challenging data scenarios. The four types allow you to tailor your analysis to the nature of your data and the goals of your study. By removing the constraints of traditional statistical models, GDFs open new possibilities for data-driven insights.</p> <p>For implementation details and examples, see the Tutorials.</p>"},{"location":"mg/mg_arguments/","title":"Glossary","text":"<p>This document provides definitions and explanations for the main arguments and variables used in Machine Gnostics data analytics, machine learning and deep learning models. Understanding these concepts will help users grasp the unique characteristics of the Machine Gnostics library, which is based on the non-statistical paradigm of Mathematical Gnostics.</p>"},{"location":"mg/mg_arguments/#core-concepts","title":"Core Concepts","text":"<ul> <li> <p>Machine Gnostics   A machine learning and deep learning library founded on Mathematical Gnostics, a non-statistical paradigm for data analysis.</p> </li> <li> <p>Mathematical Gnostics   An alternative to traditional statistical methods, focusing on the quantification and estimation of uncertainty in data.</p> </li> </ul>"},{"location":"mg/mg_arguments/#key-arguments-and-gnostic-characteristics","title":"Key Arguments and Gnostic Characteristics","text":""},{"location":"mg/mg_arguments/#1-gnostic-geometries","title":"1. Gnostic Geometries","text":"<p>These are the fundamental variables used to describe data in the gnostic framework. They are divided into two main spaces:</p> <ul> <li>Quantifying Space (Q-space, j):   Describes the variability and irrelevance in the data.</li> <li>Estimating Space (E-space, i):   Describes the estimation of variability and relevance.</li> </ul>"},{"location":"mg/mg_arguments/#quantifying-geometry","title":"Quantifying Geometry","text":"<ul> <li> <p>fj: Quantifying data variability   Measures the variability present in the data.</p> </li> <li> <p>hj: Quantifying irrelevance   Measures the irrelevance or error due to variability.</p> </li> </ul>"},{"location":"mg/mg_arguments/#estimating-geometry","title":"Estimating Geometry","text":"<ul> <li> <p>fi: Estimating data variability   Provides an estimation of the data's variability.</p> </li> <li> <p>hi: Estimating relevance   Provides an estimation of the data's relevance.</p> </li> </ul> <p>All four variables (\\( f_j, h_j, f_i, h_i \\)) are called gnostic characteristics.</p>"},{"location":"mg/mg_arguments/#2-probability-arguments","title":"2. Probability Arguments","text":"<ul> <li> <p>pi: Estimating probability   Probability estimate in the context of the gnostic model.</p> </li> <li> <p>pj: Quantifying probability   Quantifies the probability based on the quantifying characteristics.</p> </li> </ul>"},{"location":"mg/mg_arguments/#3-information","title":"3. Information","text":"<ul> <li> <p>Ii: Estimating information   Information estimate for the data.</p> </li> <li> <p>Ij: Quantifying information   Quantifies the information content.</p> </li> </ul>"},{"location":"mg/mg_arguments/#4-entropy","title":"4. Entropy","text":"<ul> <li> <p>ei: Estimating entropy   Entropy estimate for the data.</p> </li> <li> <p>ej: Quantifying entropy   Quantifies the entropy content.</p> </li> <li> <p>re: Residual entropy   The remaining entropy after estimation, representing the difference between quantification and estimation entropy.</p> </li> </ul>"},{"location":"mg/mg_arguments/#5-loss-functions","title":"5. Loss Functions","text":"<ul> <li>Hc loss: Gnostic mean relevance loss   A loss function based on gnostic relevance, where \\( c \\) can be \\( i \\) or \\( j \\).</li> </ul>"},{"location":"mg/mg_arguments/#further-reading","title":"Further reading","text":"<p>For more detailed mathematical background, see the foundational texts on Mathematical Gnostics and the documentation of the Machine Gnostics library.</p>"},{"location":"mg/principles/","title":"Principles of Advanced Data Analysis in Machine Gnostics","text":"<p>Machine Gnostics is grounded in the philosophy of Mathematical Gnostics, which emphasizes extracting the maximum information from data while respecting its objectivity and inherent structure. These principles are especially relevant for modern machine learning and data science, where robust, data-driven insights are crucial.</p> <p>Below are the core principles of advanced data analysis as practiced in Machine Gnostics, adapted for practical use in machine learning and data science:</p>"},{"location":"mg/principles/#key-principles","title":"Key Principles","text":""},{"location":"mg/principles/#1-respect-the-objectivity-of-data","title":"1. Respect the Objectivity of Data","text":"<ul> <li>Avoid imposing unjustified models: Do not force data into a priori statistical models or distributions without evidence.</li> <li>Do not trim or discard outliers without justification: Outliers may contain valuable information about the system or process.</li> <li>Acknowledge non-homogeneity: Recognize and address the presence of outliers and sample non-homogeneity rather than ignoring them.</li> <li>Use proper aggregation: Aggregate data in ways that respect the underlying structure and axioms of gnostic theory.</li> <li>Respect data finiteness: Do not treat finite samples as if they were infinite populations.</li> </ul>"},{"location":"mg/principles/#2-make-use-of-all-available-data","title":"2. Make Use of All Available Data","text":"<ul> <li>Include censored and incomplete data: Do not ignore data just because it is partially observed.</li> <li>Weight outliers and inliers appropriately: Assign justified weights to suspected outliers and inliers (noise), rather than excluding them outright.</li> <li>Exclude data only with evidence: Remove data points only if their impact is negligible or their origin is invalid.</li> <li>Consider side effects: Be aware of and account for side effects caused by the processes generating the data.</li> </ul>"},{"location":"mg/principles/#3-let-the-data-decide","title":"3. Let the Data Decide","text":"<ul> <li>Allow data to determine its own structure: Let the data reveal its group membership, homogeneity, bounds, and metric space.</li> <li>Data-driven uncertainty: Evaluate uncertainty using the data\u2019s own properties, not just statistical assumptions.</li> <li>Interdependence and distribution: Let the data inform you about its interdependence, distribution, and density functions.</li> <li>Separate uncertainty from variability: Distinguish between uncertainty and true variability in the data.</li> </ul>"},{"location":"mg/principles/#4-individualized-weighting","title":"4. Individualized Weighting","text":"<ul> <li>Assign weights at the data point level: Each data item should be weighted based on its own value, not just the sample it belongs to.</li> </ul>"},{"location":"mg/principles/#5-use-statistical-methods-judiciously","title":"5. Use Statistical Methods Judiciously","text":"<ul> <li>Justify statistical assumptions: Only use statistical methods when their assumptions are met by the data.</li> <li>Embrace non-statistical methods: When statistical assumptions fail, use robust, non-statistical approaches.</li> </ul>"},{"location":"mg/principles/#6-prefer-robust-methods","title":"6. Prefer Robust Methods","text":"<ul> <li>Robust estimation: Use robust estimation and identification methods over non-robust ones, especially in the presence of outliers or non-normal data.</li> <li>Choose the right robustness: Select the type of robustness (inner/outer) appropriate for your task.</li> </ul>"},{"location":"mg/principles/#7-prefer-distributions-over-point-estimates","title":"7. Prefer Distributions Over Point Estimates","text":"<ul> <li>Use distribution functions: Where possible, use full distributions rather than single-point estimates for data characteristics.</li> </ul>"},{"location":"mg/principles/#8-ensure-comparability","title":"8. Ensure Comparability","text":"<ul> <li>Compare like with like: Only compare objects or samples that behave according to the same model.</li> </ul>"},{"location":"mg/principles/#9-seek-explanations-not-excuses","title":"9. Seek Explanations, Not Excuses","text":"<ul> <li>Don\u2019t blame randomness: Investigate and explain uncertainty using data and available information, rather than attributing everything to randomness.</li> </ul>"},{"location":"mg/principles/#10-apply-realistic-and-theoretically-sound-criteria","title":"10. Apply Realistic and Theoretically Sound Criteria","text":"<ul> <li>Optimize using information/entropy: Use information-theoretic criteria for optimization and evaluation.</li> <li>Follow optimal data transformation paths: Respect theoretically proven optimal methods for data transformation and estimation.</li> </ul>"},{"location":"mg/principles/#11-maintain-an-open-and-critical-mindset","title":"11. Maintain an Open and Critical Mindset","text":"<ul> <li>Avoid methodological conservatism: Be open to new methods and approaches.</li> <li>Challenge expectations: Do not insist on preconceived outcomes or reject unexpected results without further analysis.</li> <li>Prioritize thoughtful analysis: The best data treatment may require more effort and deeper thinking.</li> </ul>"},{"location":"mg/principles/#why-these-principles-matter-in-machine-learning-data-science","title":"Why These Principles Matter in Machine Learning &amp; Data Science","text":"<ul> <li>Robustness: Machine Gnostics methods are designed to be robust to outliers, noise, and non-standard data distributions, making them ideal for real-world data.</li> <li>Data-Driven: The approach lets the data guide the analysis, reducing bias from unjustified assumptions.</li> <li>Comprehensive Use of Data: No data is wasted\u2014every point is considered for its potential information value.</li> <li>Transparency: By letting the data decide, results are more interpretable and trustworthy.</li> </ul> <p>Summary for New Users</p> <ul> <li>Don\u2019t force your data into ill-fitting models.</li> <li>Use all your data, including outliers and incomplete points, with justified weighting.</li> <li>Let the data reveal its own structure, uncertainty, and relationships.</li> <li>Prefer robust, information-theoretic methods when possible.</li> <li>Be open-minded and critical\u2014let the data, not your expectations, drive your analysis.</li> </ul> <p>Machine Gnostics provides a principled, robust, and data-centric foundation for advanced data analysis in machine learning and data science.</p>"},{"location":"mg/principles/#references","title":"References","text":""},{"location":"models/ml_models/","title":"Models - Machine Learning (Machine Gnostics)","text":""},{"location":"models/ml_models/#welcome-to-machine-gnostics-machine-learning-models","title":"Welcome to Machine Gnostics Machine Learning Models","text":"<p>Machine Gnostics provides a growing suite of machine learning models for transparent, robust, and diagnostic predictive analytics. This section introduces the core supervised learning tools available today, and highlights our ongoing development of new models across supervised, unsupervised, and advanced categories.</p> <p>Our goal is to deliver interpretable, assumption-free machine learning solutions that combine classic algorithms with gnostic diagnostics. Whether you are working on classification, regression, clustering, or other tasks, Machine Gnostics models help you understand both predictions and underlying data structure.</p> <p>NOTE</p> <p>We are actively developing additional machine learning models in all categories\u2014including supervised, unsupervised, and more. Stay tuned for updates as new tools and documentation become available.</p> <p>We are open to collaboration and new ideas. If you\u2019re interested in contributing, sharing feedback, or exploring partnerships, feel free to connect with us\u2014your insights and creativity are always welcome!</p>"},{"location":"models/ml_models/#key-machine-learning-model-categories","title":"Key Machine Learning Model Categories","text":"<ul> <li>Classification Models </li> <li> <p>Logistic Regression   Reliable binary and multiclass classification with gnostic diagnostics.</p> </li> <li> <p>Regression Models </p> </li> <li>Linear Regression  </li> <li> <p>Polynomial Regression   Flexible regression tools for linear and nonlinear relationships.</p> </li> <li> <p>Supervised Learning Utilities </p> </li> <li>Cross-Validation  </li> <li>Train/Test Split   Essential for model validation and reproducible experiments.</li> </ul>"},{"location":"models/ml_models/#why-use-machine-gnostics-machine-learning-models","title":"Why Use Machine Gnostics Machine Learning Models?","text":"<ul> <li>Transparent: Built-in diagnostics and error analysis for every model.</li> <li>Assumption-Free: No strict requirements on data distribution or linearity.</li> <li>Robust: Handles outliers, non-normality, and real-world data challenges.</li> <li>Extensible: Integrates seamlessly with Python data science and ML workflows.</li> <li>Expanding: New models and features are continuously being added.</li> </ul>"},{"location":"models/ml_models/#getting-started","title":"Getting Started","text":"<p>Explore the documentation for each model to learn about their features, usage patterns, and example workflows. - Logistic Regression - Linear Regression - Polynomial Regression - Cross-Validation - Train/Test Split  </p> <p>Each page provides a detailed overview, key features, parameters, example usage, and references.</p>"},{"location":"models/ml_models/#next-steps","title":"Next Steps","text":"<ul> <li>Browse individual model pages for in-depth documentation and code examples.</li> <li>Try out example notebooks in the examples folder for hands-on learning.</li> <li>Integrate models into your own machine learning pipeline for robust, diagnostic predictive analytics.</li> <li>Check back regularly for new models and updates as our development continues.</li> </ul> <p>\"In Machine Gnostics, every model is a step toward deeper understanding\u2014of your data, your process, and your discoveries.\"</p>"},{"location":"models/cls/log_reg/","title":"LogisticRegressor: Robust Logistic Regression with Machine Gnostics","text":"<p>The <code>LogisticRegressor</code> is a robust and flexible binary classification model built on the Machine Gnostics framework. It is designed to handle outliers, heavy-tailed distributions, and non-Gaussian noise, making it suitable for real-world data challenges. The model supports polynomial feature expansion, robust weighting, early stopping, and seamless MLflow integration for experiment tracking and deployment.</p>"},{"location":"models/cls/log_reg/#overview","title":"Overview","text":"<p>Machine Gnostics LogisticRegressor brings deterministic, event-level modeling to binary classification. By leveraging gnostic algebra and geometry, it provides robust, interpretable, and reproducible results, even in challenging scenarios.</p> <p>Highlights:</p> <ul> <li>Outlier Robustness: Gnostic weighting reduces the impact of noisy or corrupted samples.</li> <li>Polynomial Feature Expansion: Configurable degree for nonlinear decision boundaries.</li> <li>Flexible Probability Output: Choose between gnostic-based or standard sigmoid probabilities.</li> <li>Early Stopping: Efficient training via monitoring of loss and entropy.</li> <li>MLflow Integration: Supports experiment tracking and deployment.</li> <li>Model Persistence: Save and load models easily with joblib.</li> </ul>"},{"location":"models/cls/log_reg/#key-features","title":"Key Features","text":"<ul> <li>Robust to outliers and non-Gaussian noise</li> <li>Polynomial feature expansion (configurable degree)</li> <li>Flexible probability output: gnostic or sigmoid</li> <li>Customizable data scaling (auto or manual)</li> <li>Early stopping based on residual entropy or log loss</li> <li>Full training history tracking (loss, entropy, coefficients, weights)</li> <li>MLflow integration for model tracking and deployment</li> <li>Save and load model using joblib</li> </ul>"},{"location":"models/cls/log_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>degree</code> int 1 Degree of the polynomial for feature expansion (1 = linear). <code>max_iter</code> int 100 Maximum number of training iterations. <code>tol</code> float 1e-3 Convergence threshold for loss or coefficient changes. <code>scale</code> {'auto', float} 'auto' Scaling mode for gnostic transformation. <code>early_stopping</code> bool True Enables early stopping based on convergence criteria. <code>history</code> bool True Records training history at each iteration. <code>proba</code> {'gnostic','sigmoid'} 'gnostic' Probability output mode. <code>verbose</code> bool False Prints progress and debug information. <code>data_form</code> str 'a' Input data form: <code>'a'</code> (additive), <code>'m'</code> (multiplicative). <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"models/cls/log_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>ndarray</code>   Final learned polynomial regression coefficients.</li> <li>weights: <code>ndarray</code>   Final sample weights after convergence.</li> <li>_history: <code>list of dict</code>   Training history, including loss, entropy, coefficients, and weights at each iteration.</li> </ul>"},{"location":"models/cls/log_reg/#methods","title":"Methods","text":""},{"location":"models/cls/log_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the model to training data using polynomial expansion and robust loss minimization.</p> <ul> <li>X: array-like, pandas.DataFrame, or numpy.ndarray of shape <code>(n_samples, n_features)</code>   Training input samples.</li> <li>y: array-like or numpy.ndarray of shape <code>(n_samples,)</code>   Target binary labels (0 or 1).</li> </ul> <p>Returns: <code>self</code> (for method chaining)</p>"},{"location":"models/cls/log_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts class labels (0 or 1) for new input samples using the trained model.</p> <ul> <li>X: array-like, pandas.DataFrame, or numpy.ndarray of shape <code>(n_samples, n_features)</code>   Input samples for prediction.</li> </ul> <p>Returns: <code>y_pred</code>: numpy.ndarray of shape <code>(n_samples,)</code> Predicted binary class labels.</p>"},{"location":"models/cls/log_reg/#predict_probax","title":"<code>predict_proba(X)</code>","text":"<p>Predicts probabilities for new input samples using the trained model.</p> <ul> <li>X: array-like, pandas.DataFrame, or numpy.ndarray of shape <code>(n_samples, n_features)</code>   Input samples for probability prediction.</li> </ul> <p>Returns: <code>proba</code>: numpy.ndarray of shape <code>(n_samples,)</code> Predicted probabilities for the positive class (label 1).</p>"},{"location":"models/cls/log_reg/#save_modelpath","title":"<code>save_model(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cls/log_reg/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns: Instance of <code>LogisticRegressor</code> with loaded parameters.</p>"},{"location":"models/cls/log_reg/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models.classification import LogisticRegressor\n\n# Initialize the model\nmodel = LogisticRegressor(degree=2, proba='gnostic', verbose=True)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict class labels\ny_pred = model.predict(X_test)\n\n# Predict probabilities\ny_proba = model.predict_proba(X_test)\n\n# Access coefficients and weights\nprint(\"Coefficients:\", model.coefficients)\nprint(\"Weights:\", model.weights)\n\n# Save the model\nmodel.save_model(\"my_logreg_model\")\n\n# Load the model\nloaded = LogisticRegressor.load_model(\"my_logreg_model\")\ny_pred2 = loaded.predict(X_test)\n</code></pre>"},{"location":"models/cls/log_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records training history at each iteration, accessible via <code>model._history</code>. Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>loss</code>: Loss value (gnostic or log loss)</li> <li><code>entropy</code>: Residual entropy value</li> <li><code>coefficients</code>: Regression coefficients at this iteration</li> <li><code>weights</code>: Sample weights at this iteration</li> </ul> <p>This enables detailed analysis and visualization of the training process.</p>"},{"location":"models/cls/log_reg/#example-notebooks","title":"Example Notebooks","text":"<ul> <li>Example 1</li> <li>Example 2</li> </ul>"},{"location":"models/cls/log_reg/#notes","title":"Notes","text":"<ul> <li>The model supports numpy arrays, pandas DataFrames, and pyspark DataFrames as input.</li> <li>For best results, ensure input features are appropriately scaled and encoded.</li> <li>Supports integration with MLflow for experiment tracking and deployment.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-05-01</p>"},{"location":"models/reg/lin_reg/","title":"LinearRegressor: Robust Linear Regression with Machine Gnostics","text":"<p>The <code>LinearRegressor</code> is a robust linear regression model built on the Machine Gnostics framework. Unlike traditional statistical models that rely on probabilistic assumptions, this model uses algebraic and geometric structures to provide deterministic, resilient, and interpretable regression for real-world data.</p>"},{"location":"models/reg/lin_reg/#overview","title":"Overview","text":"<p>The Machine Gnostics LinearRegressor is designed for robust regression tasks, especially where data may contain outliers, noise, or non-Gaussian distributions. It leverages the core principles of Mathematical Gnostics (MG) to deliver reliable results even in challenging scenarios.</p> <ul> <li>Deterministic &amp; Finite: No randomness or probability; all computations are reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events.</li> <li>Algebraic Inference: Utilizes gnostic algebra and error geometry for robust learning.</li> <li>Resilient: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> <li>Flexible: Supports numpy arrays, pandas DataFrames, and pyspark DataFrames.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/reg/lin_reg/#key-features","title":"Key Features","text":"<ul> <li>Fits a linear regression model</li> <li>Robust to outliers and non-Gaussian noise</li> <li>Iterative optimization with early stopping and convergence tolerance</li> <li>Adaptive sample weighting using gnostic loss</li> <li>Training history tracking for analysis and visualization</li> <li>Customizable loss functions and scaling strategies</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/reg/lin_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>scale</code> {'auto', int, float} 'auto' Scaling method or value for input features. <code>max_iter</code> int 100 Maximum number of optimization iterations. <code>tol</code> float 1e-3 Tolerance for convergence. <code>mg_loss</code> str 'hi' Gnostic loss function to use (<code>'hi'</code>, <code>'fi'</code>, etc.). <code>early_stopping</code> bool True Whether to stop early if convergence is detected. <code>verbose</code> bool False If True, prints progress and diagnostics during fitting. <code>data_form</code> str 'a' Internal data representation format. <code>gnostic_characteristics</code> bool True If True, computes and records gnostic properties (fi, hi, etc.). <code>history</code> bool True If True, records the optimization history for analysis. <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"models/reg/lin_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>np.ndarray</code>Fitted linear regression coefficients.</li> <li>weights: <code>np.ndarray</code>Final sample weights after robust fitting.</li> <li>params: <code>list of dict</code>Parameter snapshots (loss, weights, gnostic properties) at each iteration.</li> <li>_history: <code>list</code>Internal optimization history (if enabled).</li> <li>degree, max_iter, tol, mg_loss, early_stopping, verbose, scale, data_form, gnostic_characteristics:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"models/reg/lin_reg/#methods","title":"Methods","text":""},{"location":"models/reg/lin_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the linear regressor to input features <code>X</code> and targets <code>y</code> using robust, gnostic loss minimization. Iteratively optimizes coefficients and sample weights, optionally recording history.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>Input features.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Target values.</li> </ul> <p>Returns: <code>self</code> (fitted model instance)</p>"},{"location":"models/reg/lin_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts target values for new input features using the trained model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features for prediction.</li> </ul> <p>Returns: <code>y_pred</code>: <code>np.ndarray</code>, shape <code>(n_samples,)</code> Predicted target values.</p>"},{"location":"models/reg/lin_reg/#scorex-y-casei","title":"<code>score(X, y, case='i')</code>","text":"<p>Computes the robust (gnostic) R\u00b2 score for the linear regressor model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>Input features for scoring.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>True target values.</li> <li>case: <code>str</code>, default <code>'i'</code>   Specifies the case or variant of the R\u00b2 score to compute.</li> </ul> <p>Returns: <code>score</code>: <code>float</code> Robust R\u00b2 score of the model on the provided data.</p>"},{"location":"models/reg/lin_reg/#save_modelpath","title":"<code>save_model(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/reg/lin_reg/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns: Instance of <code>LinearRegressor</code> with loaded parameters.</p>"},{"location":"models/reg/lin_reg/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models.regression import LinearRegressor\n\n# Initialize the model\nmodel = LinearRegressor(max_iter=100, mg_loss='hi', verbose=True)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Score\nr2 = model.score(X_test, y_test)\nprint(f\"Robust R2 score: {r2}\")\n\n# Access coefficients and weights\nprint(\"Coefficients:\", model.coefficients)\nprint(\"Weights:\", model.weights)\n</code></pre>"},{"location":"models/reg/lin_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model.params</code> and <code>model._history</code>. Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>loss</code>: Gnostic loss value</li> <li><code>coefficients</code>: Regression coefficients at this iteration</li> <li><code>rentropy</code>: Rentropy value (residual entropy)</li> <li><code>weights</code>: Sample weights at this iteration</li> <li><code>gnostic_characteristics</code>: (if enabled) fi, hi, etc.</li> </ul> <p>This enables in-depth analysis and visualization of the training process.</p>"},{"location":"models/reg/lin_reg/#example-notebooks","title":"Example Notebooks","text":"<ul> <li>Example 1</li> <li>Example 2</li> </ul>"},{"location":"models/reg/lin_reg/#notes","title":"Notes","text":"<ul> <li>The model is robust to outliers and suitable for datasets with non-Gaussian noise.</li> <li>Supports integration with mlflow for experiment tracking and deployment.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-05-01</p>"},{"location":"models/reg/poly_reg/","title":"PolynomialRegressor: Robust Polynomial Regression with Machine Gnostics","text":"<p>The <code>PolynomialRegressor</code> is a robust polynomial regression model built on the principles of Mathematical Gnostics. It is designed to provide deterministic, interpretable, and resilient regression in the presence of outliers, noise, and non-Gaussian data distributions. Unlike traditional statistical models, this regressor leverages algebraic and geometric concepts from Mathematical Gnostics, focusing on event-level modeling and robust loss minimization.</p>"},{"location":"models/reg/poly_reg/#overview","title":"Overview","text":"<ul> <li>Robust to Outliers: Uses gnostic loss functions and adaptive weights to minimize the influence of outliers and corrupted samples.</li> <li>Polynomial Feature Expansion: Supports configurable polynomial degrees for flexible modeling.</li> <li>Iterative Optimization: Employs iterative fitting with early stopping and convergence checks.</li> <li>Custom Gnostic Loss: Minimizes a user-selected gnostic loss (<code>'hi'</code>, <code>'hj'</code>, etc.) for event-level robustness.</li> <li>Detailed Training History: Optionally records loss, weights, entropy, and gnostic characteristics at each iteration.</li> <li>Easy Integration: Compatible with numpy arrays and supports model persistence.</li> </ul>"},{"location":"models/reg/poly_reg/#key-features","title":"Key Features","text":"<ul> <li>Robust regression using gnostic loss functions</li> <li>Flexible polynomial degree (linear and higher-order)</li> <li>Adaptive sample weighting</li> <li>Early stopping and convergence tolerance</li> <li>Training history tracking for analysis and visualization</li> <li>Handles non-Gaussian noise and outliers</li> <li>Compatible with numpy arrays</li> </ul>"},{"location":"models/reg/poly_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>degree</code> int 2 Degree of the polynomial to fit. <code>scale</code> {'auto', int, float} 'auto' Scaling method or value for input features. <code>max_iter</code> int 100 Maximum number of optimization iterations. <code>tol</code> float 1e-3 Tolerance for convergence. <code>mg_loss</code> str 'hi' Gnostic loss function to use (<code>'hi'</code>, <code>'fi'</code>, etc.). <code>early_stopping</code> bool True Whether to stop early if convergence is detected. <code>verbose</code> bool False If True, prints progress and diagnostics during fitting. <code>data_form</code> str 'a' Internal data representation format. <code>gnostic_characteristics</code> bool True If True, computes and records gnostic properties (fi, hi, etc.). <code>history</code> bool True If True, records the optimization history for analysis. <code>verbose</code> bool True Print detailed progress, warnings, and results ---"},{"location":"models/reg/poly_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>np.ndarray</code>   Fitted polynomial regression coefficients.</li> <li>weights: <code>np.ndarray</code>   Final sample weights after robust fitting.</li> <li>params: <code>list of dict</code>   Parameter snapshots (loss, weights, gnostic properties) at each iteration.</li> <li>_history: <code>list</code>   Internal optimization history (if enabled).</li> <li>degree, max_iter, tol, mg_loss, early_stopping, verbose, scale, data_form, gnostic_characteristics:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"models/reg/poly_reg/#methods","title":"Methods","text":""},{"location":"models/reg/poly_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the polynomial regressor to input features <code>X</code> and targets <code>y</code> using robust, gnostic loss minimization. Iteratively optimizes coefficients and sample weights, optionally recording history.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Target values.</li> </ul> <p>Returns: <code>self</code> (fitted model instance)</p>"},{"location":"models/reg/poly_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts target values for new input features using the trained model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features for prediction.</li> </ul> <p>Returns: <code>y_pred</code>: <code>np.ndarray</code>, shape <code>(n_samples,)</code> Predicted target values.</p>"},{"location":"models/reg/poly_reg/#scorex-y-casei","title":"<code>score(X, y, case='i')</code>","text":"<p>Computes the robust (gnostic) R\u00b2 score for the polynomial regressor model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features for scoring.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   True target values.</li> <li>case: <code>str</code>, default <code>'i'</code>   Specifies the case or variant of the R\u00b2 score to compute.</li> </ul> <p>Returns: <code>score</code>: <code>float</code> Robust R\u00b2 score of the model on the provided data.</p>"},{"location":"models/reg/poly_reg/#save_modelpath","title":"<code>save_model(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/reg/poly_reg/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns: Instance of <code>PolynomialRegressor</code> with loaded parameters.</p>"},{"location":"models/reg/poly_reg/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models.regression import PolynomialRegressor\n\n# Initialize the model\nmodel = PolynomialRegressor(degree=2, mg_loss='hi', verbose=True)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Score\nr2 = model.score(X_test, y_test)\nprint(f'Robust R2 score: {r2}')\n\n# Access coefficients and weights\nprint(\"Coefficients:\", model.coefficients)\nprint(\"Weights:\", model.weights)\n</code></pre>"},{"location":"models/reg/poly_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model.params</code> and <code>model._history</code>. Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>loss</code>: Gnostic loss value</li> <li><code>coefficients</code>: Regression coefficients at this iteration</li> <li><code>rentropy</code>: Rentropy value (residual entropy)</li> <li><code>weights</code>: Sample weights at this iteration</li> <li><code>gnostic_characteristics</code>: (if enabled) fi, hi, etc.</li> </ul> <p>This enables in-depth analysis and visualization of the training process.</p>"},{"location":"models/reg/poly_reg/#example-notebooks","title":"Example Notebooks","text":"<ul> <li>Example 1</li> <li>Example 2</li> </ul>"},{"location":"models/reg/poly_reg/#notes","title":"Notes","text":"<ul> <li>The model is robust to outliers and suitable for datasets with non-Gaussian noise.</li> <li>Implements advanced machine learning techniques based on Mathematical Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-05-01</p>"},{"location":"models/sup/cross_val/","title":"CrossValidator: Custom k-Fold Cross-Validation","text":"<p>The <code>CrossValidator</code> class provides a simple, flexible implementation of k-fold cross-validation for evaluating machine learning models. It is designed to work with any model that implements <code>fit(X, y)</code> and <code>predict(X)</code> methods, and supports custom scoring functions for regression or classification tasks.</p>"},{"location":"models/sup/cross_val/#overview","title":"Overview","text":"<p>Cross-validation is a robust technique for assessing the generalization performance of machine learning models. The <code>CrossValidator</code> class splits your dataset into <code>k</code> folds, trains the model on <code>k-1</code> folds, and evaluates it on the remaining fold, repeating this process for each fold. The results are aggregated to provide a reliable estimate of model performance.</p>"},{"location":"models/sup/cross_val/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model</code> object \u2014 A machine learning model with <code>fit(X, y)</code> and <code>predict(X)</code> methods. <code>X</code> array-like \u2014 Feature matrix of shape <code>(n_samples, n_features)</code>. <code>y</code> array-like \u2014 Target labels of shape <code>(n_samples,)</code>. <code>k</code> int 5 Number of folds for cross-validation. <code>shuffle</code> bool True Whether to shuffle the dataset before splitting into folds. <code>random_seed</code> int/None None Seed for reproducible shuffling (ignored if <code>shuffle=False</code>)."},{"location":"models/sup/cross_val/#attributes","title":"Attributes","text":"<ul> <li>folds: <code>list of tuple</code>   List of <code>(train_indices, test_indices)</code> for each fold.</li> </ul>"},{"location":"models/sup/cross_val/#methods","title":"Methods","text":""},{"location":"models/sup/cross_val/#split","title":"<code>split()</code>","text":"<p>Splits the dataset into <code>k</code> folds.</p> <ul> <li>Returns:   <code>folds</code>: list of tuple   Each tuple contains <code>(train_indices, test_indices)</code> for a fold.</li> </ul>"},{"location":"models/sup/cross_val/#evaluatescoring_func","title":"<code>evaluate(scoring_func)</code>","text":"<p>Performs k-fold cross-validation and returns evaluation scores.</p> <ul> <li>Parameters:<code>scoring_func</code>: callableA function that takes <code>y_true</code> and <code>y_pred</code> and returns a numeric score (e.g., <code>mean_squared_error</code>, <code>accuracy_score</code>).</li> <li>Returns:   <code>scores</code>: list of float   Evaluation scores for each fold.</li> </ul>"},{"location":"models/sup/cross_val/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models import CrossValidator, LinearRegressor\nfrom machinegnostics.metircs import mean_squared_error\nimport numpy as np\n\n# Generate random data\nX = np.random.rand(100, 10)\ny = np.random.rand(100)\n\n# Initialize model and cross-validator\nmodel = LinearRegressor()\ncv = CrossValidator(model, X, y, k=5, shuffle=True, random_seed=42)\n\n# Evaluate using mean squared error\nscores = cv.evaluate(mean_squared_error)\nprint(\"Cross-Validation Scores:\", scores)\nprint(\"Mean Score:\", np.mean(scores))\n</code></pre>"},{"location":"models/sup/cross_val/#notes","title":"Notes","text":"<ul> <li>The model is re-initialized and trained from scratch for each fold.</li> <li>Supports any model with <code>fit</code> and <code>predict</code> methods.</li> <li>Works with any scoring function that accepts <code>y_true</code> and <code>y_pred</code>.</li> <li>Shuffling with a fixed <code>random_seed</code> ensures reproducible splits.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-05-01</p>"},{"location":"models/sup/train_test_split/","title":"train_test_split: Random Train/Test Data Splitter","text":"<p>The <code>train_test_split</code> function provides a simple and flexible way to split your dataset into random training and testing subsets. It is compatible with numpy arrays and can also handle lists or tuples as input. This function is essential for evaluating machine learning models on unseen data and is a core utility in most ML workflows.</p>"},{"location":"models/sup/train_test_split/#overview","title":"Overview","text":"<p>Splitting your data into training and testing sets is a fundamental step in machine learning. The <code>train_test_split</code> function allows you to:</p> <ul> <li>Randomly partition your data into train and test sets.</li> <li>Specify the proportion or absolute number of test samples.</li> <li>Shuffle your data for unbiased splitting.</li> <li>Use a random seed for reproducibility.</li> <li>Split both features (<code>X</code>) and targets (<code>y</code>) in a consistent manner.</li> </ul>"},{"location":"models/sup/train_test_split/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>X</code> array-like \u2014 Feature data to be split. Must be indexable and of consistent length. <code>y</code> array-like or None None Target data to be split alongside X. Must be same length as X. <code>test_size</code> float or int 0.25 If float, fraction of data for test set (0.0 &lt; test_size &lt; 1.0). If int, absolute number of test samples. <code>shuffle</code> bool True Whether to shuffle the data before splitting. <code>random_seed</code> int or None None Controls the shuffling for reproducibility."},{"location":"models/sup/train_test_split/#returns","title":"Returns","text":"<ul> <li> <p>X_train, X_test: <code>np.ndarray</code>   Train-test split of X.</p> </li> <li> <p>y_train, y_test: <code>np.ndarray</code> or <code>None</code>   Train-test split of y. If y is None, these will also be None.</p> </li> </ul>"},{"location":"models/sup/train_test_split/#raises","title":"Raises","text":"<ul> <li> <p>ValueError   If inputs are invalid or <code>test_size</code> is not appropriate.</p> </li> <li> <p>TypeError   If <code>test_size</code> is not a float or int.</p> </li> </ul>"},{"location":"models/sup/train_test_split/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.models import train_test_split\n\n# Create sample data\nX = np.arange(20).reshape(10, 2)\ny = np.arange(10)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, shuffle=True, random_seed=42\n)\n\nprint(\"X_train:\", X_train)\nprint(\"X_test:\", X_test)\nprint(\"y_train:\", y_train)\nprint(\"y_test:\", y_test)\n</code></pre>"},{"location":"models/sup/train_test_split/#notes","title":"Notes","text":"<ul> <li>If <code>y</code> is not provided, only <code>X</code> will be split and <code>y_train</code>, <code>y_test</code> will be <code>None</code>.</li> <li>If <code>test_size</code> is a float, it must be between 0.0 and 1.0 (exclusive).</li> <li>If <code>test_size</code> is an int, it must be between 1 and <code>len(X) - 1</code>.</li> <li>Setting <code>shuffle=False</code> will split the data in order, without randomization.</li> <li>Use <code>random_seed</code> for reproducible splits.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-05-01</p>"},{"location":"ref/references/","title":"References","text":"<p>Machine Gnostics Publications</p> <p>Publications on Machine Gnostics will be available soon. This website is designed to provide the essential fundamentals to help you get started.</p> <p>Below is a curated list of key publications where the concept of Mathematical Gnostics is introduced, explained, and applied in research and practice.</p>"},{"location":"ref/references/#books","title":"Books","text":"<p> Kovanic, P.; Humber, M.B. The Economics of Information\u2014Mathematical Gnostics for Data Analysis. 2015. </p> <p> Kovanic, P. Mathematical Gnostics, 2023. DOI: 10.1201/9780429441196 </p>"},{"location":"ref/references/#research-papers","title":"Research Papers","text":"<p> Parmar, N.; Bendov\u00e1, M.; Wagner, Z., Heat capacity measurements by a Setaram \u03bcDSC3 evo microcalorimeter: Estimation of deviation in the measurement, advanced data analysis by mathematical gnostics, and prediction by the artificial neural network. J Therm Anal Calorim 150, 313\u2013325 (2025). https://doi.org/10.1007/s10973-024-13505-w </p> <p> Parmar, N.; Bendov\u00e1, M.; Wagner, Z.; P\u011bnkavov\u00e1, V.; Douihri, I.; Jacquemin, J., Carbon nanotube-based ionanofluids for efficient energy storage: Thermophysical properties\u2019 determination and advanced data analysis.  Industrial &amp; Engineering Chemistry Research 2021, 60 (20), 7714\u20137728. DOI: 10.1021/acs.iecr.0c06008 </p> <p> Parmar, N. et al., A study of changes in the heat capacity of carbon nanotube-based ionanofluids prepared from a series of imidazolium ionic liquids.  https://doi.org/10.1039/D2CP02110B </p> <p> Wagner, Z.; Bendov\u00e1, M.; Rotrekl, J.; Sykorova, A.; Canji, M.; Parmar, N., Density and sound velocity measurement by an Anton Paar DSA 5000 density meter: Precision and long-time stability.  J Mol Liq 329, 2021, 115547. ISSN 0167-7322. </p> <p> Wagner, Z.; Bendov\u00e1, M.; Rotrekl, J.; Parmar, N.; Koc\u0131, S.; Vrbka, P., Thermochemical properties of menthol and terpineol.  J Solution Chem 49, 1267\u20131278, 2020. </p>"},{"location":"stories/history/","title":"From Mathematical Gnostics to Machine Gnostics: A History","text":""},{"location":"stories/history/#introduction","title":"Introduction","text":"<p>Mathematical Gnostics is a unique approach to understanding data uncertainty\u2014one that challenges conventional statistical thinking. Rather than viewing uncertainty as indeterministic or purely random, Mathematical Gnostics treats every data point as the result of measurable, material causes, even if not all influencing factors are directly observable. This philosophy underpins the Machine Gnostics library, which brings these ideas into the realm of modern data analysis and machine learning.</p> <p>This page traces the origins of Mathematical Gnostics, its sources of inspiration, the birth of Machine Gnostics, and the impact these ideas are having today.</p>"},{"location":"stories/history/#historical-background","title":"Historical Background","text":"<p>The origins of Mathematical Gnostics trace back to the early 1980s, when Pavel Kovanic, a researcher at the Institute of Information Theory and Automation of the Czechoslovak Academy of Sciences in Prague, first published his pioneering work. His approach was met with skepticism by mainstream statisticians, as it departed radically from accepted paradigms. Nevertheless, the theory continued to develop and gain traction through both theoretical advances and practical applications.</p> <p>Key milestones in the development of Mathematical Gnostics include:</p> <ul> <li>As a new paradigm of data variability, mathematical gnostics has been developing since the end of the 1970s and applied in a number of research projects.</li> <li>The first printed publications in 1984 and presentation at an international conference.</li> <li>Practical applications in economics and financial analysis, leading to a series of books and publications.</li> <li>International collaborations, notably with American professor M.B. Humber, resulting in a co-authored book (unfinished due to his passing) and further dissemination of the theory.</li> <li>Early private implementations of gnostic software in several languages, including BASIC, C, S-PLUS, and limited C++ versions.</li> <li>The first free/public software implementation in R, which may still be in use today.</li> <li>A significant step was taken by Z. Wagner, who developed an independent version of gnostic software in the Octave language. This software was first applied to both offline and fully automatic online analysis of particle size distribution of atmospheric aerosol, and then used within the TTSM team at the Institute of Chemical Processes of the Czech Academy of Sciences. Although it had very limited user documentation, it supported research-level applications and contributed to a series of articles published in top journals.</li> <li>Application of gnostic methods in environmental and medical research, including participation in major European Union projects.</li> </ul> <p>For more information on the history and development of mathematical gnostics, visit: math-gnostics.eu</p>"},{"location":"stories/history/#sources-of-inspiration","title":"Sources of Inspiration","text":"<p>Unlike traditional statistical methods, which rely on large data samples and probabilistic assumptions, Mathematical Gnostics is rooted in the laws of nature and the analysis of individual data items\u2014even in small samples. This non-statistical approach treats data uncertainty as a consequence of real, measurable conditions, not as mere randomness.</p> <p>The development of Mathematical Gnostics was inspired by both the limitations of classical statistics and the foundational principles of several scientific disciplines:</p> <ul> <li>Theory of General Systems: The gnostic cycle of observation and feedback, emphasizing knowledge (from the Greek \u201cgnosis\u201d) rather than a priori assumptions.</li> <li>Theory of Measurement: H. von Helmholtz\u2019s work on quantification and the mathematical structure of measurement.</li> <li>Geometry: The use of non-Euclidean geometries (Riemannian, Minkowskian) to model uncertainty and variability.</li> <li>Relativistic Physics: Insights from Einstein\u2019s mechanics, connecting data variability to the movement of relativistic particles.</li> <li>Thermodynamics: The original thermodynamic concept of entropy, as introduced by R. Clausius, applied to data uncertainty.</li> <li>Electromagnetism: J.C. Maxwell\u2019s theories, including the concept of Maxwell\u2019s demon, linking entropy and information.</li> <li>Matrix Algebra: The manipulation of data structures and operations, as formalized in modern algebra and implemented in computational tools.</li> </ul> <p>These inspirations, drawn from the natural sciences, provide a robust and universal foundation for understanding data uncertainty\u2014making Mathematical Gnostics a powerful alternative to classical statistics.</p> <p>\u201cLet data speak for themselves.\u201d This guiding principle reflects the core philosophy of Mathematical Gnostics: to extract maximum information from data, relying on the data values themselves, and to model uncertainty in a way that is consistent with the laws of nature.</p>"},{"location":"stories/history/#the-birth-of-machine-gnostics","title":"The Birth of Machine Gnostics","text":"<p>The realization that the robust, nature-inspired foundation of Mathematical Gnostics could serve as the basis for a new generation of artificial intelligence and machine learning was a key motivation for the creation of Machine Gnostics. In 2022, Dr. Nirmal Parmar began exploring the integration of mathematical gnostics with modern machine learning, seeking to create models and algorithms that are both assumption-free and deeply aligned with the laws of nature.</p> <p>This work led to the birth of Machine Gnostics: an open source project dedicated to providing a unified framework for data analysis models, machine learning models, and\u2014looking ahead\u2014a future deep learning framework. Machine Gnostics aims to empower researchers, engineers, and practitioners with tools that combine the rigor of mathematical gnostics with the flexibility and power of contemporary AI.</p> <p>By making Machine Gnostics open source, the project invites collaboration and innovation from the global community, ensuring that the theory and its applications continue to evolve and serve a wide range of scientific and practical needs.</p> <p>The impact of this integration is already being felt, and the vision for Machine Gnostics continues to grow.</p>"},{"location":"stories/history/#impact-and-vision","title":"Impact and Vision","text":"<p>The journey from Mathematical Gnostics to Machine Gnostics marks a significant shift in how we approach data uncertainty, analysis, and artificial intelligence. By grounding our methods in the laws of nature and embracing a non-statistical, axiomatic foundation, we have opened new avenues for robust, assumption-free data science. Machine Gnostics is already empowering researchers and practitioners to extract deeper insights from data\u2014whether in small samples, complex systems, or real-world applications where traditional statistics fall short.</p> <p>Looking ahead, the vision for Machine Gnostics is ambitious: to become a universal framework for data analysis, machine learning, and, ultimately, deep learning. By remaining open source and community-driven, Machine Gnostics invites collaboration, innovation, and critical feedback from scientists, engineers, and thinkers worldwide. Together, we can continue to push the boundaries of what is possible in data-driven discovery.</p>"},{"location":"stories/history/#testimonials","title":"Testimonials","text":""},{"location":"stories/history/#dr-zdenek-wagner","title":"Dr. Zden\u011bk Wagner","text":"<p>'Listening to the data'</p> <p>New discoveries and paradigms usually arrive when there is a need to get a solution to an unsolvable problem. They have their time to come. Many years ago I had to solve a complex task, regression of high pressure vapour-liquid equilibria (HPVLE). It poses a lot of difficulties with convergence because at the mixture critical point some derivatives are infinite, with slightly modified interaction parameters in the mixing rules the equations have no solution, and, last but not least, values measured by different authors are not always in agreement and thus oscillations in the iterative algorithm can occur. The statistical tools did not work for me. At that time I visited a seminar and met Dr. Pavel Kovanic who presented his novel paradigm of uncertainty, mathematical gnostics (MG). We discussed almost the whole night because it looked promising, especially with the key rule, let the data speak for themselves, i.e. do not force them to have a particular distribution of measurement errors but get the shape of the distribution function from the data. We met uncountable number of time after the seminar and I received full support from him. I first tried MG on simple tasks, I made my first three programs (the next was always a little more complex and more versatile than the preceding one) and found that it really can solve the tasks better.</p> <p>It was not easy to persuade statisticians that MG is a valid tool especially because HPVLE was still difficult. And another need came. A colleague who dealt with atmospheric aerosols had twenty thousand data sets and did not know how to analyze them. He told me that he would believe MG if I analyze the data and get useful results. And after some time I told him that there is something strange every day approximately half an hour before midnight and half an hour after midnight. And his response was: \u201cOh, these bad guys!\u201d The data were measured at the top of a mountain by several groups and many instruments. They agreed that they would leave the cars below the mountain in order not to take samples of the exhaust gases. People from one group had to change the filters in the instrument exactly at the midnight and they decided to go up to the top by cars because nobody sees them. And analysis of the data by MG found it. Since then I have analyzed many millions of data sets for the aerosol community.</p> <p>Yet another need came when we were in danger of losing a job. At that time my new colleague, Dr. Magdalena Bendov\u00e1, got interest in MG. We decided that we should show that we have special knowledge. Being inspired by Nassim Nicholas Taleb we turned MG into our \u201cblack swan\u201d. Explaining her this new paradigm helped me to realize what the reviewers would understand. Together we managed to promote MG so that it is now accepted by reviewers of good scientific journals. MG then became our main tool for data analysis and recently Magdalena's PhD student, Dr. Nirmal Parmar, opens a new horizon for novel applications. And I still learn how to teach this subject.</p>"},{"location":"stories/history/#dr-magdalena-bendova","title":"Dr. Magdalena Bendov\u00e0","text":"<p>'My Journey with Mathematical Gnostics'</p> <p>When I joined the Eduard H\u00e1la Laboratory of Thermodynamics in 2003, I had no idea that something like the theory of Mathematical Gnostics even existed. It was only when I began discussing how to analyze the phase equilibrium data I had just measured with my colleague, Dr. Zden\u011bk Wagner, that the path revealed itself. He suggested trying a robust regression method to estimate parameters of the thermodynamic models I proposed to use, using MG. That led to our first paper together and to my introduction to both the method and its author, Dr. Pavel Kovanic.</p> <p>In that first publication, Zden\u011bk tried his best to present the robust regression along a gnostic influence function in the simplest way possible, just enough so that the reviewers that were used to using statistical methods wouldn\u2019t get too mad at us. Years have passed, and such precautions are no longer necessary.</p> <p>Over time, I learned how to use the custom-made scripts that Zden\u011bk had coded; to analyze my data, to critically assess it, and to detect outliers. This led to many more papers, each allowing us to push the boundaries of MG (and of the referees\u2019 comprehension) further. For my part, I know I have only scratched the surface of this remarkable theory. Yet I remain convinced that it is one of the most powerful tools in data analysis. Not a rival to statistics, but rather a complementary approach that fills the gaps where traditional statistical methods fall short.</p> <p>I\u2019m especially proud that our Ph.D. student, Nirmal Parmar, who has since soared toward his own horizons, embraced this theory and made it his own. He\u2019s now exploring how to combine machine learning with mathematical gnostics. It won\u2019t be smooth sailing. The theory is complex and the project ambitious. But I am confident that, with more successful implementations, it will reach a broader audience and gain recognition in both the scientific and industrial communities for its profound usefulness.</p>"},{"location":"stories/history/#acknowledgments","title":"Acknowledgments","text":"<p>Acknowledgments</p> <p>The development of Mathematical Gnostics and its evolution into Machine Gnostics would not have been possible without the dedication and insight of many individuals. I would like to express my deepest gratitude to:</p> <p>-Dr. Pavel Kovanic (1942\u20132023), for his foundational work and vision in creating Mathematical Gnostics. His legacy continues to inspire this project and the broader scientific community. His guidance and pioneering spirit remain a guiding light for all who build upon his work.</p> <p>-Dr. Magdalena Bendov\u00e1, my PhD supervisor, for her guidance, encouragement, and support throughout my research journey.</p> <p>-Dr. Zden\u011bk Wagner, my expert supervisor, whose expertise in Mathematical Gnostics and his development of the Octave software for data analysis were instrumental in my understanding of the field. His mentorship inspired me to extend these ideas further and integrate them with machine learning and artificial intelligence.</p> <p>I am also grateful to all colleagues, collaborators, and students who have contributed ideas, feedback, and encouragement along the way. The open source community, with its spirit of sharing and innovation, continues to inspire the ongoing growth of this project.</p> <p>Dr. Nirmal Parmar</p> <p>If you are interested in contributing, collaborating, or simply learning more, we welcome you to join us on this journey.</p> <p>Explore the documentation, try the tools, and help shape the future of Machine Gnostics.</p>"},{"location":"tutorials/1_da_basics/","title":"Machine Gnostics Measures","text":"<p>Note</p> <p>Why Machine Gnostics? Unlike classical statistics, which rely on probabilistic averages, Machine Gnostics computes measures like mean, median, standard deviation, and variance using irrelevance and fidelity from gnostic theory. This approach is assumption-free and robust to outliers, revealing the true diagnostic properties of your data.</p>"},{"location":"tutorials/1_da_basics/#1-sample-data","title":"1. Sample Data","text":"<p>Let\u2019s start with a small dataset that includes an outlier, to see how Machine Gnostics handles challenging real-world data.</p> <p>Sample Data</p> <pre><code>import numpy as np\n\n# Example data with an outlier\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nprint(\"Data:\", data)\n</code></pre>"},{"location":"tutorials/1_da_basics/#2-gnostic-mean","title":"2. Gnostic Mean","text":"<p>The gnostic mean is robust to outliers and does not assume a specific data distribution.</p> <p>Gnostic Mean</p> <pre><code>import machinegnostics as mg\n\nmean = mg.mean(data)\nprint(\"Gnostic Mean:\", mean)\n</code></pre>"},{"location":"tutorials/1_da_basics/#3-gnostic-median","title":"3. Gnostic Median","text":"<p>The gnostic median provides a robust central value, even for small or skewed samples.</p> <p>Gnostic Median</p> <pre><code>import machinegnostics as mg\n\nmedian = mg.median(data)\nprint(\"Gnostic Median:\", median)\n</code></pre>"},{"location":"tutorials/1_da_basics/#4-gnostic-standard-deviation","title":"4. Gnostic Standard Deviation","text":"<p>Unlike classical standard deviation, the gnostic version returns a lower and upper bound, reflecting uncertainty more realistically.</p> <p>Gnostic Standard Deviation</p> <pre><code>import machinegnostics as mg\n\nstd_dev_lb, std_dev_ub = mg.std(data)\nprint(\"Gnostic Std Dev (lower, upper):\", std_dev_lb, std_dev_ub)\n</code></pre>"},{"location":"tutorials/1_da_basics/#5-gnostic-variance","title":"5. Gnostic Variance","text":"<p>Gnostic variance is always between 0 and 1, as it is calculated using irrelevance rather than squared deviations.</p> <p>Gnostic Variance</p> <pre><code>import machinegnostics as mg\n\nvar = mg.variance(data)\nprint(\"Gnostic Variance:\", var)\n</code></pre>"},{"location":"tutorials/1_da_basics/#tips","title":"Tips","text":"<ul> <li>Robustness: Try changing or adding more outliers to your data and see how the gnostic measures respond compared to classical statistics.</li> <li>Integration: All functions follow standard Python/NumPy conventions and can be used in data science workflows.</li> <li>Documentation: See the API Reference for advanced options and parameter tuning.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/2_da_gdf/","title":"Gnostic Distribution Functions","text":"<p>Gnostic Distribution Functions (GDF) are a new class of probability and density estimators designed for robust, flexible, and assumption-free data analysis. Unlike traditional statistical distributions, GDFs do not require any prior assumptions about the underlying data distribution. Instead, they allow the data to \"speak for themselves,\" making them especially powerful for small, noisy, or uncertain datasets.</p> <p>More information available here.</p>"},{"location":"tutorials/2_da_gdf/#egdf-estimating-global-distribution-function","title":"EGDF - Estimating Global Distribution Function","text":"<p>The EGDF provides a robust global estimate of the distribution function for your data.</p> <p>Estimating Global Distribution Function</p> <pre><code>from machinegnostics.magcal import EGDF\n\n# create an EGDF object\negdf = EGDF(verbose=False)\n\n# fit the data\negdf.fit(data=data)\n\n# plot GDF\negdf.plot(bounds=True)\n\n# check parameters\nprint(egdf.params)\n</code></pre> <p></p>"},{"location":"tutorials/2_da_gdf/#eldf-estimating-local-distribution-function","title":"ELDF - Estimating Local Distribution Function","text":"<p>The ELDF focuses on local properties of the data distribution, providing detailed insight into local data behavior.</p> <p>Estimating Local Distribution Function</p> <pre><code>from machinegnostics.magcal import ELDF\n\n# create an ELDF object\neldf = ELDF(verbose=False)\n\n# fit the data\neldf.fit(data=data)\n\n# plot GDF\neldf.plot(bounds=True)\n\n# check parameters\nprint(eldf.params)\n</code></pre> <p></p>"},{"location":"tutorials/2_da_gdf/#qgdf-quantifying-global-distribution-function","title":"QGDF - Quantifying Global Distribution Function","text":"<p>QGDF quantifies global distribution characteristics, useful for uncertainty quantification and diagnostics.</p> <p>Quantifying Global Distribution Function</p> <pre><code>from machinegnostics.magcal import QGDF\n\n# create a QGDF object\nqgdf = QGDF(verbose=False)\n\n# fit the data\nqgdf.fit(data=data)\n\n# plot GDF\nqgdf.plot(bounds=True)\n\n# check parameters\nprint(qgdf.params)\n</code></pre> <p></p>"},{"location":"tutorials/2_da_gdf/#qldf-quantifying-local-distribution-function","title":"QLDF - Quantifying Local Distribution Function","text":"<p>QLDF quantifies local distribution characteristics, providing fine-grained uncertainty and fidelity measures.</p> <p>Quantifying Local Distribution Function</p> <pre><code>from machinegnostics.magcal import QLDF\n\n# create a QLDF object\nqldf = QLDF(verbose=False)\n\n# fit the data\nqldf.fit(data=data)\n\n# plot GDF\nqldf.plot(bounds=True)\n\n# check parameters\nprint(qldf.params)\n</code></pre> <p></p>"},{"location":"tutorials/2_da_gdf/#tips","title":"Tips","text":"<ul> <li>All GDF classes (<code>EGDF</code>, <code>ELDF</code>, <code>QGDF</code>, <code>QLDF</code>) follow a similar API: create an object, fit your data, plot results, and inspect parameters.</li> <li>Use the <code>bounds=True</code> option in <code>.plot()</code> to visualize uncertainty bounds.</li> <li>For more advanced usage and parameter tuning, see the API Reference.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/3_da_ca/","title":"Gnostics Cluster Analysis","text":"<p>The <code>ClusterAnalysis</code> class provides an end-to-end, automated workflow for estimating the main cluster bounds of a dataset using Gnostic Distribution Functions (GDFs) and advanced clustering analysis. This approach is robust, interpretable, and reproducible, making it ideal for scientific, engineering, and data science applications where reliable interval estimation is needed.</p> <p>Key Features:</p> <ul> <li>Fully automated pipeline for cluster-based bound estimation</li> <li>Integrates GDF fitting, homogeneity testing, and cluster analysis</li> <li>Supports both local (ELDF) and global (EGDF) GDFs</li> <li>Handles weighted data, bounded/unbounded domains, and advanced parameterization</li> <li>Detailed error/warning logging and reproducible parameter tracking</li> <li>Optional memory-efficient operation via flushing intermediate results</li> <li>Visualization support for both GDF and cluster analysis results</li> </ul>"},{"location":"tutorials/3_da_ca/#1-basic-usage-automated-cluster-bound-estimation","title":"1. Basic Usage: Automated Cluster Bound Estimation","text":"<p>Let\u2019s see how to estimate the main cluster bounds for a dataset with an outlier.</p> <p>Basic Cluster Analysis</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import ClusterAnalysis\n\n# Example data with an outlier\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nprint(\"Data: \", data)\n\n# Create a ClusterAnalysis object\nca = ClusterAnalysis(verbose=False, flush=False)\n\n# Fit the data and get cluster bounds\nclb, cub = ca.fit(data=data)\n\n# Plot the clusters and data\nca.plot()\n\n# Check results\nca.results()\n\n# Print the cluster bounds\nprint(\"Cluster Lower Bound: \", clb)\nprint(\"Cluster Upper Bound: \", cub)\nprint(\"CLB and CUB present the bounds of the main cluster in the data.\")\n</code></pre> <p>Output:</p> <p></p> <p></p>"},{"location":"tutorials/3_da_ca/#2-advanced-usage-manual-gdf-and-cluster-analysis","title":"2. Advanced Usage: Manual GDF and Cluster Analysis","text":"<p>For advanced users, you can manually fit a GDF and perform cluster analysis for more control and customization.</p> <p>Advanced Cluster Analysis</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import QLDF, ELDF, DataCluster\n\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Step 1: Fit a GDF (local DFs are a good choice for cluster analysis)\nqldf = QLDF()\nqldf.fit(data=data)\n\neldf = ELDF()\neldf.fit(data=data)\n\n# Step 2: Manual Cluster Analysis using DataCluster\ndc = DataCluster(gdf=eldf, verbose=False)\nclb, cub = dc.fit(plot=True)\ndc.results()\n\n# OR, if interested in inliers, use QLDF\ndc = DataCluster(gdf=qldf, verbose=False)\nclb, cub = dc.fit(plot=True)\ndc.results()\n</code></pre> <p>Typical Output:</p> <pre><code>{\n 'gdf_type': 'qldf',\n 'derivative_threshold': 0.01,\n 'slope_percentile': 70,\n 'LCB': -2.24,\n 'UCB': 6.33,\n 'Z0': 2.04,\n 'S_opt': 1.0,\n 'cluster_width': 8.57,\n 'clustering_successful': True,\n 'method_used': 'qldf_w-shape_valley_detection',\n 'normalization_method': 'min_max_normalization',\n 'pdf_shape': 'W-shape',\n 'errors': [],\n 'warnings': []\n}\n</code></pre> <p>ELDF </p> <p>QLDF</p> <p></p> <p>API Reference</p>"},{"location":"tutorials/3_da_ca/#tips","title":"Tips","text":"<ul> <li>Use <code>ClusterAnalysis</code> for a fully automated workflow, or <code>DataCluster</code> for manual, fine-grained control.</li> <li>Both local (ELDF/QLDF) and global (EGDF/QGDF) GDFs can be used depending on your analysis needs.</li> <li>For more advanced usage and parameter tuning, see the API Reference.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/4_da_ia/","title":"Gnostics Interval Analysis","text":"<p>The <code>IntervalAnalysis</code> class provides a robust, automated workflow for estimating meaningful data intervals (such as tolerance and typical intervals) using Gnostic Distribution Functions (GDFs). It is designed for reliability, diagnostics, and adaptive interval estimation in scientific and engineering data analysis.</p> <p>Key Features:</p> <ul> <li>End-to-end marginal interval analysis for GDFs</li> <li>Automated fitting of global (EGDF) and local (ELDF) distributions</li> <li>Homogeneity testing and adaptive re-fitting for non-homogeneous data</li> <li>Robust computation of practical, sample, and cluster-based bounds</li> <li>Detailed diagnostics, warnings, and error tracking</li> <li>Visualization of fitted distributions and estimated intervals</li> </ul>"},{"location":"tutorials/4_da_ia/#1-basic-usage-automated-interval-estimation","title":"1. Basic Usage: Automated Interval Estimation","text":"<p>Let\u2019s estimate robust data intervals for a dataset with an outlier using the automated pipeline.</p> <p>Basic Interval Analysis</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import IntervalAnalysis\n\n# Example data with an outlier\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nprint(\"Data: \", data)\n\n# Create an IntervalAnalysis object\nia = IntervalAnalysis(verbose=False, n_points=100)\n\n# Fit the data\nia.fit(data)\n\n# Plot interval analysis insights\nia.plot()\n\n# Results: gnostics data certification\ndata_certification = ia.results()\nfor key, value in data_certification.items():\n    print(f\"{key}: {value}\")\n</code></pre> <p>ELDF</p> <p></p> <p>Interval Analysis</p> <p></p> <p>Interval Analysis and GDF</p> <p></p> <p>Typical Output:</p> <pre><code>LB: -155.93776968450086\nLSB: -13.5\nDLB: -13.5\nLCB: -7.803030303030303\nLSD: 2.964280971014115\nZL: 0.8182394174126442\nZ0L: 4.9896988533352395\nZ0: 5.48989898989899\nZ0U: 6.0096390584764325\nZU: 9.293449080684455\nUSD: 10.143637329814027\nUCB: 10.0\nDUB: 10.0\nUSB: 10.595725263965676\nUB: 99.47565393644747\n</code></pre>"},{"location":"tutorials/4_da_ia/#2-advanced-usage-manual-gdf-and-interval-analysis","title":"2. Advanced Usage: Manual GDF and Interval Analysis","text":"<p>For advanced users, you can manually fit a GDF and perform interval analysis for more control and customization.</p> <p>Advanced Interval Analysis</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import QLDF, ELDF, DataIntervals\n\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Step 1: Fit a GDF (local DFs are a good choice for interval analysis)\nqldf = QLDF()\nqldf.fit(data=data)\n\neldf = ELDF()\neldf.fit(data=data)\n\n# Step 2: Manual Interval Analysis using DataIntervals\ndata_intervals = DataIntervals(gdf=eldf)\ndata_intervals.fit()\ndata_intervals.plot()\nprint(data_intervals.results())\n\n# OR, if interested in inliers, use QLDF\ndata_intervals_inliers = DataIntervals(gdf=qldf, verbose=False)\ndata_intervals_inliers.fit()\ndata_intervals_inliers.plot()\nprint(data_intervals_inliers.results())\n</code></pre> <p>Typical Output (ELDF):</p> <p></p> <pre><code>{'LB': -155.93776968450086,\n 'LSB': None,\n 'DLB': -13.5,\n 'LCB': None,\n 'LSD': 2.882567969218145,\n 'ZL': 0.8103374961119698,\n 'Z0L': 4.938000938000959,\n 'Z0': 5.483483483483482,\n 'Z0U': 6.0232151117425685,\n 'ZU': 9.668716398907417,\n 'USD': 10.43118199977495,\n 'UCB': None,\n 'DUB': 10.0,\n 'USB': None,\n 'UB': 41.7788339826821}\n</code></pre> <p>Typical Output (QLDF):</p> <p></p> <pre><code>{'LB': -155.93776968450086,\n 'LSB': None,\n 'DLB': -13.5,\n 'LCB': None,\n 'LSD': 2.1784866717814797,\n 'ZL': -8.017505807458605,\n 'Z0L': 1.3583846325048605,\n 'Z0': 2.04044550905268,\n 'Z0U': 3.647143462258168,\n 'ZU': 15.521880274960733,\n 'USD': 13.566684391621703,\n 'UCB': None,\n 'DUB': 10.0,\n 'USB': None,\n 'UB': 95.31359676026301}\n</code></pre>"},{"location":"tutorials/4_da_ia/#tips","title":"Tips","text":"<ul> <li>Use <code>IntervalAnalysis</code> for a fully automated workflow, or <code>DataIntervals</code> for manual, fine-grained control.</li> <li>Both local (ELDF/QLDF) and global (EGDF/QGDF) GDFs can be used depending on your analysis needs.</li> <li>For more advanced usage and parameter tuning, see the API Reference.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/5_da_test/","title":"Gnostic Tests","text":"<p>This tutorial covers advanced data diagnostics in the Machine Gnostics framework:</p> <ul> <li>Data Homogeneity Test: Analyze if your data sample is homogeneous using EGDF and PDF analysis.</li> <li>Data Scedasticity Test: Diagnose homoscedasticity or heteroscedasticity using gnostic variance and regression.</li> <li>Data Membership Test: Check if a value can be considered a member of a homogeneous sample.</li> </ul>"},{"location":"tutorials/5_da_test/#data-homogeneity-test","title":"Data Homogeneity Test","text":"<p>Analyze data homogeneity for EGDF objects using probability density function analysis.</p> <p>The homogeneity criterion is based on the mathematical properties and expected PDF behavior of EGDF according to gnostic theory principles. Homogeneous data should produce a distribution with a single density maximum, while non-homogeneous data will exhibit multiple maxima or negative density values.</p> <p>Homogeneity Test with Outlier</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import DataHomogeneity, EGDF\n\n# Example: homogeneous data with an outlier\ndata = np.array([-3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Example: non-homogeneous data\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\negdf = EGDF(verbose=False)\negdf.fit(data=data)\n\ndh = DataHomogeneity(gdf=egdf, verbose=False, flush=True)\nis_homogeneous = dh.fit(plot=True)\nprint(\"Is the data homogeneous? \", is_homogeneous)\ndh.results()\n</code></pre> <p>Output: <pre><code>Is the data homogeneous?  False\n{'has_negative_pdf': np.True_,\n 'num_maxima': 1,\n 'extrema_type': 'maxima',\n 'gdf_type': 'egdf',\n 'is_homogeneous': False,\n ...}\n</code></pre></p>"},{"location":"tutorials/5_da_test/#data-scedasticity-test","title":"Data Scedasticity Test","text":"<p>Gnostic Scedasticity Test for Homoscedasticity and Heteroscedasticity</p> <p>This class provides a method to check for homoscedasticity and heteroscedasticity in data, inspired by fundamental principles rather than standard statistical tests. It uses gnostic variance and gnostic linear regression, which are based on the Machine Gnostics framework.</p> <p>Scedasticity Test</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import DataScedasticity\n\nX = np.array([0., 0.4, 0.8, 1.2, 1.6, 2. ])\ny = np.array([17.89408548, 69.61586934, -7.19890572, 9.37670866, -10.55673099, 16.57855348])\n\nds = DataScedasticity(verbose=False)\nis_homoscedastic = ds.fit(x=X, y=y)\nprint(\"Is the data homoscedastic? \", is_homoscedastic)\n</code></pre> <p>Output: <pre><code>Is the data homoscedastic?  False\n</code></pre></p>"},{"location":"tutorials/5_da_test/#data-membership-test","title":"Data Membership Test","text":"<p>Test whether a value can be considered a member of a homogeneous data sample using the EGDF framework.</p> <p>Data Membership Test</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import DataMembership, EGDF\n\ndata = np.array([-3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\negdf = EGDF(verbose=False)\negdf.fit(data=data)\n\ndm = DataMembership(egdf=egdf, verbose=False)\nlsb, usb = dm.fit()\nprint(f\"Lower and Upper Sample Bounds to check Data Membership: LSB: {lsb}, USB: {usb}\")\n</code></pre> <p>Output: <pre><code>Lower and Upper Sample Bounds to check Data Membership: LSB: -3, USB: 10.013\n</code></pre></p>"},{"location":"tutorials/5_da_test/#tips","title":"Tips","text":"<ul> <li>All diagnostic classes (<code>DataHomogeneity</code>, <code>DataScedasticity</code>, <code>DataMembership</code>) follow a similar API: create an object, fit your data, and inspect results.</li> <li>Use the <code>plot=True</code> option in <code>.fit()</code> for visual diagnostics where available.</li> <li>For advanced usage and parameter tuning, see the API Reference, scedasticity, and membership.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/colab/","title":"Machine Gnostics \u2013 Example Gallery (Google Colab)","text":"<p>You can try Machine Gnostics instantly with Google Colab notebooks! Click any link below, then select \"Open with Google Colab\" to launch the notebook in your browser\u2014no installation required.</p> <p>Explore practical examples and Jupyter notebooks demonstrating the use of Machine Gnostics for data analysis and machine learning. Each example includes code, explanations, and links to downloadable notebooks.</p>"},{"location":"tutorials/colab/#data-analysis-examples","title":"Data Analysis Examples","text":"<ol> <li> <p>Gnostics Measures and Distribution Functions    Learn how to compute and interpret Gnostic measures and fit Gnostic Distribution Functions (GDFs) to your data.</p> </li> <li> <p>Gnostics Tests for Data Samples    Apply Gnostic homogeneity, scedasticity, and membership tests to assess the structure and quality of your data samples.</p> </li> <li> <p>Gnostics Cluster Analysis    Use Gnostic methods to identify main clusters, estimate cluster bounds, and visualize cluster structure in complex datasets.</p> </li> <li> <p>Gnostics Interval Analysis    Perform robust interval estimation (tolerance, typical, and cluster intervals) using GDFs for scientific and engineering data.</p> </li> <li> <p>Gnostics Uncertainty Analysis \u2013 Real Life Example    Analyze uncertainty in real-world data using Gnostic diagnostics, interval analysis, and visualization tools.</p> </li> </ol>"},{"location":"tutorials/colab/#machine-learning-examples","title":"Machine Learning Examples","text":"<ol> <li> <p>Small Data Regression \u2013 Linear Regression    Fit and evaluate a linear regression model on a small dataset, demonstrating robust fitting and prediction with Machine Gnostics.</p> </li> <li> <p>Wine Quality: Multidimensional Linear Regression    Apply linear regression to the wine quality dataset with multiple features, showing how to handle multivariate regression tasks.</p> </li> <li> <p>Small Data Polynomial Regression    Explore polynomial regression on a small dataset, including fitting, prediction, and the impact of nonlinear relationships.</p> </li> <li> <p>Wine Quality: Multidimensional Polynomial Regression    Perform advanced polynomial regression on the wine quality dataset, highlighting feature expansion and robust modeling.</p> </li> <li> <p>Basic Binary Logistic Regression    Train and evaluate a robust logistic regression classifier on synthetic moon data, including probability estimation and decision boundaries.</p> </li> <li> <p>Polynomial Regression with MLflow Integration    Integrate polynomial regression with MLflow for experiment tracking, reproducibility, and deployment in a real-world workflow.</p> </li> </ol>"},{"location":"tutorials/colab/#access-the-notebooks","title":"Access the Notebooks","text":"<p>You can download or view all Jupyter notebooks from the examples directory on Google Drive.</p> <p>More Information</p> <p>For more details on the Machine Gnostics Foundation, visit the Learn section.</p> <p>Next: Try running these notebooks in the Google Colab cloud environment to explore Machine Gnostics by yourself!</p>"},{"location":"tutorials/examples/","title":"Machine Gnostics \u2013 Example Gallery","text":"<p>Explore practical examples and Jupyter notebooks demonstrating the use of Machine Gnostics for data analysis and machine learning. Each example includes code, explanations, and links to downloadable notebooks.</p>"},{"location":"tutorials/examples/#data-analysis-examples","title":"Data Analysis Examples","text":"<ol> <li> <p>Gnostics Measures and Distribution Functions    Learn how to compute and interpret Gnostic measures and fit Gnostic Distribution Functions (GDFs) to your data.</p> </li> <li> <p>Gnostics Tests for Data Samples    Apply Gnostic homogeneity, scedasticity, and membership tests to assess the structure and quality of your data samples.</p> </li> <li> <p>Gnostics Cluster Analysis    Use Gnostic methods to identify main clusters, estimate cluster bounds, and visualize cluster structure in complex datasets.</p> </li> <li> <p>Gnostics Interval Analysis    Perform robust interval estimation (tolerance, typical, and cluster intervals) using GDFs for scientific and engineering data.</p> </li> <li> <p>Gnostics Uncertainty Analysis \u2013 Real Life Example    Analyze uncertainty in real-world data using Gnostic diagnostics, interval analysis, and visualization tools.</p> </li> </ol>"},{"location":"tutorials/examples/#machine-learning-examples","title":"Machine Learning Examples","text":"<ol> <li> <p>Small Data Regression \u2013 Linear Regression    Fit and evaluate a linear regression model on a small dataset, demonstrating robust fitting and prediction with Machine Gnostics.</p> </li> <li> <p>Wine Quality: Multidimensional Linear Regression    Apply linear regression to the wine quality dataset with multiple features, showing how to handle multivariate regression tasks.</p> </li> <li> <p>Small Data Polynomial Regression    Explore polynomial regression on a small dataset, including fitting, prediction, and the impact of nonlinear relationships.</p> </li> <li> <p>Wine Quality: Multidimensional Polynomial Regression    Perform advanced polynomial regression on the wine quality dataset, highlighting feature expansion and robust modeling.</p> </li> <li> <p>Basic Binary Logistic Regression    Train and evaluate a robust logistic regression classifier on synthetic moon data, including probability estimation and decision boundaries.</p> </li> <li> <p>Polynomial Regression with MLflow Integration    Integrate Polynomial regression with MLflow for experiment tracking, reproducibility, and deployment in a real-world workflow.</p> </li> </ol>"},{"location":"tutorials/examples/#access-the-notebooks","title":"Access the Notebooks","text":"<p>You can download or view the Jupyter notebooks for each example from the examples directory in the repository.</p> <p>More Information</p> <p>For more details on the Machine Gnostics Foundation, visit Learn section.</p> <p>Next: Try running these notebooks locally or in your favorite cloud environment to explore the power of Machine Gnostics in action!</p>"},{"location":"tutorials/ml_log_reg_1/","title":"Machine Gnostics Logistic Regression","text":"<p>The <code>LogisticRegressor</code> is a robust and flexible binary classification model built on the Machine Gnostics framework. It is designed to handle outliers, heavy-tailed distributions, and non-Gaussian noise, making it suitable for real-world data challenges. The model supports polynomial feature expansion, robust weighting, early stopping, and seamless MLflow integration for experiment tracking and deployment.</p> <p>Key Features:</p> <ul> <li>Robust to outliers and non-Gaussian noise</li> <li>Polynomial feature expansion (configurable degree)</li> <li>Flexible probability output: gnostic or sigmoid</li> <li>Customizable data scaling (auto or manual)</li> <li>Early stopping based on residual entropy or log loss</li> <li>Full training history tracking (loss, entropy, coefficients, weights)</li> <li>MLflow integration for model tracking and deployment</li> <li>Save and load model using joblib</li> </ul>"},{"location":"tutorials/ml_log_reg_1/#1-overview","title":"1. Overview","text":"<p>Machine Gnostics LogisticRegressor brings deterministic, event-level modeling to binary classification. By leveraging gnostic algebra and geometry, it provides robust, interpretable, and reproducible results, even in challenging scenarios.</p> <p>Highlights:</p> <ul> <li>Outlier Robustness: Gnostic weighting reduces the impact of noisy or corrupted samples.</li> <li>Polynomial Feature Expansion: Configurable degree for nonlinear decision boundaries.</li> <li>Flexible Probability Output: Choose between gnostic-based or standard sigmoid probabilities.</li> <li>Early Stopping: Efficient training via monitoring of loss and entropy.</li> <li>MLflow Integration: Supports experiment tracking and deployment.</li> <li>Model Persistence: Save and load models easily with joblib.</li> </ul>"},{"location":"tutorials/ml_log_reg_1/#2-gnostic-logistic-regression","title":"2. Gnostic Logistic Regression","text":"<p>Let\u2019s see how to use the Machine Gnostics LogisticRegressor for robust binary classification on a synthetic dataset.</p> <p>Moon Data</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef make_moons_manual(n_samples=100, noise=0.1):\n    n_samples_out = n_samples // 2\n    n_samples_in = n_samples - n_samples_out\n\n# First half moon\n    outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples_out))\n    outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples_out))\n\n# Second half moon\n    inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, n_samples_in))\n    inner_circ_y = -np.sin(np.linspace(0, np.pi, n_samples_in)) - 0.5\n\nX = np.vstack([\n        np.stack([outer_circ_x, outer_circ_y], axis=1),\n        np.stack([inner_circ_x, inner_circ_y], axis=1)\n    ])\n    y = np.array([0] * n_samples_out + [1] * n_samples_in)\n\n# Add noise\n    X += np.random.normal(scale=noise, size=X.shape)\n\nreturn X, y\n\n# Example usage\nX, y = make_moons_manual(n_samples=300, noise=0.4)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)\nplt.title(\"Custom make_moons\")\nplt.show()\n</code></pre> <p></p> <p>Basic Logistic Regression (Sigmoid Probability)</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom machinegnostics.models.classification import LogisticRegressor\nfrom machinegnostics.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n\n# using gnostic influenced sigmoid function for probability estimation\nmodel = LogisticRegressor(degree=3,verbose=True, early_stopping=True, proba='sigmoid', tol=0.1, max_iter=100)\nmodel.fit(X, y)\nproba_gnostic = model.predict_proba(X)\ny_pred_gnostic = model.predict(X)\n\n# --- Plot probability contour and predictions ---\nfig, ax = plt.subplots(figsize=(7, 6))\n\ndef plot_proba_contour(ax, model, X, title):\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                        np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    zz = model.predict_proba(grid)\n    zz = zz.reshape(xx.shape)\n\nim = ax.imshow(zz, extent=(x_min, x_max, y_min, y_max), origin='lower',\n                aspect='auto', cmap='Greens', alpha=0.5, vmin=0, vmax=1)\n    plt.colorbar(im, ax=ax, label='Predicted Probability')\n    ax.contour(xx, yy, zz, levels=[0.5], colors='k', linewidths=2)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_title(title)\n\nplot_proba_contour(ax, model, X, \"Gnostic Logistic Regression\")\nax.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=40, label='True label', alpha=0.9)\nax.scatter(X[:, 0], X[:, 1], c=y_pred_gnostic, cmap='cool', marker='x', s=60, label='Predicted class', alpha=0.7)\nax.legend()\nax.grid(True)\nplt.tight_layout()\nplt.show()\n\n# --- Evaluation ---\nprint('Gnostic Logistic Regression Evaluation:')\nprint(\"Accuracy:\", accuracy_score(y, y_pred_gnostic))\nprint(\"Precision:\", precision_score(y, y_pred_gnostic))\nprint(\"Recall:\", recall_score(y, y_pred_gnostic))\nprint(\"F1-score:\", f1_score(y, y_pred_gnostic))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, y_pred_gnostic))\nprint(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gnostic))\n</code></pre> <p>Output</p> <p></p> <pre><code>Gnostic Logistic Regression Evaluation:\nAccuracy: 0.9733333333333334\nPrecision: 0.9797297297297297\nRecall: 0.9666666666666667\nF1-score: 0.9731543624161074\n\nConfusion Matrix:\n [[147   3]\n [  5 145]]\n\nClassification Report:\n Class           Precision    Recall  F1-score   Support\n========================================================\n0                    0.97      0.98      0.97       150\n1                    0.98      0.97      0.97       150\n========================================================\nAvg/Total            0.97      0.97      0.97       300\n</code></pre>"},{"location":"tutorials/ml_log_reg_1/#3-gnostic-probability-output","title":"3. Gnostic Probability Output","text":"<p>The <code>proba</code> argument in <code>LogisticRegressor</code> can be set to <code>'gnostics'</code> to use a gnostic-based probability estimation, which is more robust to outliers and non-Gaussian data.</p> <p>Advanced: Gnostic Probability Output</p> <pre><code># using gnostic probability estimation\nmodel = LogisticRegressor(degree=3,verbose=True, early_stopping=True, max_iter=100, proba='gnostic', tol=0.1)\nmodel.fit(X, y)\nproba_gnostic = model.predict_proba(X)\ny_pred_gnostic = model.predict(X)\n\n# --- Plot probability contour and predictions ---\nfig, ax = plt.subplots(figsize=(7, 6))\n\ndef plot_proba_contour(ax, model, X, title):\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                        np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    zz = model.predict_proba(grid)\n    zz = zz.reshape(xx.shape)\n\nim = ax.imshow(zz, extent=(x_min, x_max, y_min, y_max), origin='lower',\n                aspect='auto', cmap='Greens', alpha=0.5, vmin=0, vmax=1)\n    plt.colorbar(im, ax=ax, label='Predicted Probability')\n    ax.contour(xx, yy, zz, levels=[0.5], colors='k', linewidths=2)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_title(title)\n\nplot_proba_contour(ax, model, X, \"Gnostic Logistic Regression\")\nax.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=40, label='True label', alpha=0.9)\nax.scatter(X[:, 0], X[:, 1], c=y_pred_gnostic, cmap='cool', marker='x', s=60, label='Predicted class', alpha=0.7)\nax.legend()\nax.grid(True)\nplt.tight_layout()\nplt.show()\n\n# --- Evaluation ---\nprint('Gnostic Logistic Regression Evaluation:')\nprint(\"Accuracy:\", accuracy_score(y, y_pred_gnostic))\nprint(\"Precision:\", precision_score(y, y_pred_gnostic))\nprint(\"Recall:\", recall_score(y, y_pred_gnostic))\nprint(\"F1-score:\", f1_score(y, y_pred_gnostic))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, y_pred_gnostic))\nprint(\"\\nClassification Report:\\n\", classification_report(y, y_pred_gnostic))\n</code></pre> <p>Output</p> <p></p> <pre><code>Gnostic Logistic Regression Evaluation:\nAccuracy: 0.9433333333333334\nPrecision: 0.965034965034965\nRecall: 0.92\nF1-score: 0.9419795221843004\n\nConfusion Matrix:\n [[145   5]\n [ 12 138]]\n\nClassification Report:\n Class           Precision    Recall  F1-score   Support\n========================================================\n0                    0.92      0.97      0.94       150\n1                    0.97      0.92      0.94       150\n========================================================\nAvg/Total            0.94      0.94      0.94       300\n</code></pre> <p>Note:</p> <ul> <li>The <code>proba</code> argument controls the probability estimation method: <code>'sigmoid'</code> (default) for standard logistic regression, <code>'gnostics'</code> for robust, gnostic-based probabilities.</li> <li>Use gnostic probabilities for datasets with outliers or non-Gaussian noise for more reliable classification.</li> </ul>"},{"location":"tutorials/ml_log_reg_1/#tips","title":"Tips","text":"<ul> <li>Use <code>LogisticRegressor</code> for robust, interpretable binary classification, especially when data may contain outliers or non-Gaussian noise.</li> <li>Set <code>proba='gnostics'</code> for robust probability estimation.</li> <li>Adjust the <code>degree</code> parameter for nonlinear decision boundaries.</li> <li>Enable <code>early_stopping</code> for efficient training.</li> <li>For more advanced usage and parameter tuning, see the API Reference.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/ml_lr_1/","title":"Machine Gnostics Linear Regression","text":"<p>The <code>LinearRegressor</code> is a robust linear regression model built on the Machine Gnostics framework. Unlike traditional statistical models that rely on probabilistic assumptions, this model uses algebraic and geometric structures to provide deterministic, resilient, and interpretable regression for real-world data.</p> <p>Key Features:</p> <ul> <li>Deterministic &amp; Finite: No randomness or probability; all computations are reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events.</li> <li>Algebraic Inference: Utilizes gnostic algebra and error geometry for robust learning.</li> <li>Resilient: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> <li>Flexible: Supports numpy arrays, pandas DataFrames, and pyspark DataFrames.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"tutorials/ml_lr_1/#1-basic-usage-robust-linear-regression","title":"1. Basic Usage: Robust Linear Regression","text":"<p>Let\u2019s compare the Machine Gnostics LinearRegressor with standard regression models on a dataset with noise and an outlier.</p> <p>Linear Regression</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# machine gnostics imports\nfrom machinegnostics.models.regression import LinearRegressor\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate clean data\nX = np.linspace(0, 2, 10).reshape(-1, 1)\ny_clean = 10 * X.ravel() + 1  # True linear relationship\n\n# Add controlled noise and outliers\nnoise = np.random.normal(0, 2, 10)\ny_noisy = y_clean + noise\ny_noisy[2] = y_noisy[2] + 28.0  # Add outlier\n\n# Create test points for smooth curve\nX_test = np.linspace(0, 2, 100).reshape(-1, 1)\n\ndegree = 3  # Using degree 1 for linear relationship\n\n# Regular polynomial regression\npoly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_reg.fit(X, y_noisy)\ny_pred_regular = poly_reg.predict(X)\ny_pred_regular_test = poly_reg.predict(X_test)\n\n# Fit sklearn LinearRegression (no polynomial features, just linear)\nsk_model = LinearRegression()\nsk_model.fit(X, y_noisy)\ny_pred_sk = sk_model.predict(X)\ny_pred_sk_test = sk_model.predict(X_test)\n\n# Fit Machine Gnostics LinearRegressor\nmg_linreg = LinearRegressor()\nmg_linreg.fit(X, y_noisy)\ny_pred_mg_linreg = mg_linreg.predict(X)\ny_pred_mg_linreg_test = mg_linreg.predict(X_test)\n\n# Create figure with subplots\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 18))\n\n# Plot regression curves\nax1.plot(X, y_clean, 'g-', label='True Relationship', linewidth=0.5)\nax1.scatter(X, y_noisy, color='gray', label='Noisy Data', zorder=2)\nax1.scatter(X[2], y_noisy[2], color='red', s=100, label='Outlier', zorder=3)\nax1.plot(X_test, y_pred_regular_test, 'b--', label='Regular Polynomial', zorder=1)\nax1.plot(X_test, y_pred_sk_test, 'm-.', label='Sklearn Linear', zorder=1)\nax1.plot(X_test, y_pred_mg_linreg_test, 'c:', label='MG Linear', zorder=1)\nax1.set_xlabel('X')\nax1.set_ylabel('y')\nax1.set_title('Comparison of Regression Methods')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot residuals from true relationship\nresiduals_regular = y_pred_regular - y_clean\nresiduals_sk = y_pred_sk - y_clean\nresiduals_mg_linreg = y_pred_mg_linreg - y_clean\nax2.scatter(X, residuals_regular, color='blue', label='Regular Residuals', alpha=0.4)\nax2.scatter(X, residuals_sk, color='magenta', label='Sklearn Linear Residuals', alpha=0.4)\nax2.scatter(X, residuals_mg_linreg, color='cyan', label='MG Linear Residuals', alpha=0.4)\nax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax2.set_xlabel('X')\nax2.set_ylabel('Residuals from True')\nax2.set_title('Residuals from True Relationship')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot sample weights\nax3.stem(X.ravel(), mg_linreg.weights, label='MG Linear Weights')\nax3.set_xlabel('X')\nax3.set_ylabel('Weight')\nax3.set_title('Sample Weights from MG Linear Regression')\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print performance metrics\nprint(\"Coefficients:\")\nprint(f\"True:           [Intercept: 1, Slope: 10]\")\nprint(f\"Regular:        {poly_reg.named_steps['linearregression'].coef_}\")\nprint(f\"Sklearn Linear: {sk_model.coef_}\")\nprint(f\"MG Linear:      {mg_linreg.coefficients}\")\n\n# Calculate MSE against true relationship\nmse_regular = np.mean((y_pred_regular[:-2] - y_clean[:-2])**2)\nmse_sk = np.mean((y_pred_sk[:-2] - y_clean[:-2])**2)\nmse_mg_linreg = np.mean((y_pred_mg_linreg[:-2] - y_clean[:-2])**2)\nprint(\"\\nMSE against true relationship (excluding outliers):\")\nprint(f\"Regular Polynomial: {mse_regular:.4f}\")\nprint(f\"Sklearn Linear:     {mse_sk:.4f}\")\nprint(f\"MG Linear:          {mse_mg_linreg:.4f}\")\n</code></pre> <p>Output:</p> <p></p> <pre><code>Coefficients:\nTrue:           [Intercept: 1, Slope: 10]\nRegular:        [  0.          51.13182115 -54.84299636  17.369313  ]\nSklearn Linear: [6.11971484]\nMG Linear:      [2.40285216 9.59521403]\n\nMSE against true relationship (excluding outliers):\nRegular Polynomial: 38.5847\nSklearn Linear:     24.6826\nMG Linear:          1.2263\n</code></pre>"},{"location":"tutorials/ml_lr_1/#tips","title":"Tips","text":"<ul> <li>Use <code>LinearRegressor</code> for robust, interpretable linear regression, especially when data may contain outliers or non-Gaussian noise.</li> <li>All Machine Gnostics models are deterministic and reproducible\u2014no randomness in fitting.</li> <li>For more advanced usage and parameter tuning, see the API Reference.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/ml_plr_1/","title":"Machine Gnostics Polynomial Regression","text":"<p>The <code>PolynomialRegressor</code> is a robust polynomial regression model built on the principles of Mathematical Gnostics. It is designed to provide deterministic, interpretable, and resilient regression in the presence of outliers, noise, and non-Gaussian data distributions. Unlike traditional statistical models, this regressor leverages algebraic and geometric concepts from Mathematical Gnostics, focusing on event-level modeling and robust loss minimization.</p> <p>Key Features:</p> <ul> <li>Robust to Outliers: Uses gnostic loss functions and adaptive weights to minimize the influence of outliers and corrupted samples.</li> <li>Polynomial Feature Expansion: Supports configurable polynomial degrees for flexible modeling.</li> <li>Iterative Optimization: Employs iterative fitting with early stopping and convergence checks.</li> <li>Custom Gnostic Loss: Minimizes a user-selected gnostic loss ('hi', 'hj', etc.) for event-level robustness.</li> <li>Detailed Training History: Optionally records loss, weights, entropy, and gnostic characteristics at each iteration.</li> <li>Easy Integration: Compatible with numpy arrays and supports model persistence.</li> </ul>"},{"location":"tutorials/ml_plr_1/#1-basic-usage-robust-polynomial-regression","title":"1. Basic Usage: Robust Polynomial Regression","text":"<p>Let\u2019s compare the Machine Gnostics PolynomialRegressor with standard polynomial regression on a dataset with outliers.</p> <p>Basic Polynomial Regression</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom machinegnostics.models.regression import PolynomialRegressor\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate data\nX = np.linspace(0, 2, 10).reshape(-1, 1)\ny = 2.0 * np.exp(1.8 * X.ravel()) + np.random.normal(0, 0.2, 10)\ny[8:] += [80.0, -8.0]  # Add outliers\n\n# Create test points for smooth curve\nX_test = np.linspace(0, 2, 100).reshape(-1, 1)\n\n# Fit regular polynomial regression\ndegree = 2\npoly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_reg.fit(X, y)\ny_pred_regular = poly_reg.predict(X)\ny_pred_regular_test = poly_reg.predict(X_test)\n\n# Fit robust Machine Gnostics regression\nmg_model = PolynomialRegressor(degree=degree)\nmg_model.fit(X, y.flatten())\ny_pred_robust = mg_model.predict(X)\ny_pred_robust_test = mg_model.predict(X_test)\nprint(f'model coeff: {mg_model.coefficients}')\n\n# Calculate residuals\nresiduals_regular = y - y_pred_regular\nresiduals_robust = y - y_pred_robust\n\n# Create figure with subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 15), height_ratios=[2, 1])\n\n# Plot regression curves\nax1.scatter(X, y, color='gray', label='Data', zorder=2)\nax1.scatter(X[8:], y[8:], color='red', s=100, label='Outliers', zorder=3)\nax1.plot(X_test, y_pred_regular_test, 'b--', label='Regular Polynomial', zorder=1)\nax1.plot(X_test, y_pred_robust_test, 'r-', label='Robust MG Regression', zorder=1)\nax1.set_xlabel('X')\nax1.set_ylabel('y')\nax1.set_title('Comparison: Regular vs Robust Machine Gnostics Polynomial Regression')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot residuals\nax2.scatter(X, residuals_regular, color='blue', label='Regular Residuals', alpha=0.6)\nax2.scatter(X, residuals_robust, color='red', label='Robust Residuals', alpha=0.6)\nax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nax2.set_xlabel('X')\nax2.set_ylabel('Residuals')\nax2.set_title('Residual Plot')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print mean squared error for both methods (excluding outliers)\nmse_regular = np.mean((y_pred_regular[:-2] - y[:-2])**2)\nmse_robust = np.mean((y_pred_robust[:-2] - y[:-2])**2)\nprint(f\"MSE (excluding outliers):\")\nprint(f\"Regular Polynomial: {mse_regular:.4f}\")\nprint(f\"Robust MG Regression: {mse_robust:.4f}\")\n\n# Print max absolute residuals (excluding outliers)\nmax_resid_regular = np.max(np.abs(residuals_regular[:-2]))\nmax_resid_robust = np.max(np.abs(residuals_robust[:-2]))\nprint(f\"\\nMax Absolute Residuals (excluding outliers):\")\nprint(f\"Regular Polynomial: {max_resid_regular:.4f}\")\nprint(f\"Robust MG Regression: {max_resid_robust:.4f}\")\n</code></pre> <p>Output:</p> <p></p> <pre><code>MSE (excluding outliers):\nRegular Polynomial: 63.8383\nRobust MG Regression: 1.0044\n\nMax Absolute Residuals (excluding outliers):\nRegular Polynomial: 17.5910\nRobust MG Regression: 1.3305\n</code></pre>"},{"location":"tutorials/ml_plr_1/#2-custom-gnostic-loss-and-training-history","title":"2. Custom Gnostic Loss and Training History","text":"<p>For advanced users, the <code>PolynomialRegressor</code> supports custom gnostic loss functions, adaptive weighting, and detailed training history for analysis and visualization.</p> <p>Advanced: Custom Loss and Training History</p> <pre><code># gnostic loss hi or hj     \nmg_model = PolynomialRegressor(degree=2, mg_loss='hi', history=True)     \nmg_model.fit(X, y)     \n# Access training history     \nhistory = mg_model._history     \nprint(history)\n</code></pre>"},{"location":"tutorials/ml_plr_1/#3-cross-validation-and-gnostic-mean-squared-error","title":"3. Cross-Validation and Gnostic Mean Squared Error","text":"<p>Cross-validation is essential for evaluating model generalization. Machine Gnostics provides a <code>CrossValidator</code> for robust, assumption-free validation, and a gnostic version of mean squared error (MSE) that uses the gnostic mean instead of the statistical mean.</p> <p>The gnostic mean is a robust, assumption-free measure designed to provide deeper insight and reliability, especially in the presence of outliers or non-normal data. This ensures that error metrics reflect the true structure and diagnostic properties of your data, in line with the principles of Mathematical Gnostics.</p> <p>Cross-Validation with Gnostic and Regular Metrics</p> <pre><code># cross validation example (optional)\n\nfrom machinegnostics.models import CrossValidator\nfrom machinegnostics.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n\n# normal mean squared error\ndef normal_mse(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\n# Define cross-validator\ncv = CrossValidator(model=mg_model, X=X, y=y, k=5, random_seed=42)\n\n# Perform cross-validation with mean absolute error\ncv_results = cv.evaluate(mean_absolute_error)\nprint(\"\\nCross-Validation Results (Gnostics - Mean Absolute Error):\")\nfor fold, mse in enumerate(cv_results, 1):\n    print(f\"Fold {fold}: {mse:.4f}\")\n\n# cross validation with root mean absolute error\ncv_rmse = CrossValidator(model=mg_model, X=X, y=y, k=5, random_seed=42)\ncv_results_rmse = cv_rmse.evaluate(root_mean_squared_error)\nprint(\"\\nCross-Validation Results (Root Mean Squared Error):\")\nfor fold, rmse in enumerate(cv_results_rmse, 1):\n    print(f\"Fold {fold}: {rmse:.4f}\")\n\n# cross validation with mean squared error\ncv_mae = CrossValidator(model=mg_model, X=X, y=y, k=5, random_seed=42)\ncv_results_mae = cv_mae.evaluate(mean_squared_error)\nprint(\"\\nCross-Validation Results (Mean Squared Error):\")\nfor fold, mae in enumerate(cv_results_mae, 1):\n    print(f\"Fold {fold}: {mae:.4f}\")\n\n# cross validation with normal mse\ncv_normal = CrossValidator(model=mg_model, X=X, y=y, k=5, random_seed=42)\ncv_results_normal = cv_normal.evaluate(normal_mse)\nprint(\"\\nCross-Validation Results (Regular MSE):\")\nfor fold, mse in enumerate(cv_results_normal, 1):\n    print(f\"Fold {fold}: {mse:.4f}\")\n</code></pre> <p>Note:</p> <ul> <li>The <code>mean_squared_error</code> function from Machine Gnostics computes MSE using the gnostic mean, which is more robust to outliers and non-Gaussian data than the traditional mean. Explore more gnostic metrics here</li> <li>Use gnostic metrics for deeper, more reliable diagnostics in challenging data scenarios.</li> </ul>"},{"location":"tutorials/ml_plr_1/#tips","title":"Tips","text":"<ul> <li>Use <code>PolynomialRegressor</code> for robust polynomial regression, especially when data may contain outliers or non-Gaussian noise.</li> <li>Adjust the <code>degree</code> parameter for higher-order polynomial fits.</li> <li>Use the <code>loss</code> parameter to select different gnostic loss functions for event-level robustness.</li> <li>Enable <code>record_history=True</code> to analyze training dynamics and convergence.</li> <li>For more advanced usage and parameter tuning, see the API Reference.</li> </ul> <p>Next: Explore more tutorials and real-world examples in the Examples section!</p>"},{"location":"tutorials/overview/","title":"Welcome to the Machine Gnostics Tutorials","text":"<p>These tutorials are designed to help you master the Machine Gnostics framework for robust, interpretable, and assumption-free data analysis and machine learning. Whether you are a beginner or an advanced user, you will find step-by-step guides, practical code examples, and conceptual explanations to accelerate your learning.</p>"},{"location":"tutorials/overview/#getting-started","title":"Getting Started","text":"<p>Before you begin, please ensure that Machine Gnostics is installed correctly in your environment.</p> <ul> <li>See the Installation Guide for setup instructions.</li> </ul> <p>Note</p> <p>Machine Gnostics is based on Mathematical Gnostics theorems. The procedures and inference of results may differ from standard statistical methods, offering new perspectives and robust diagnostics.</p>"},{"location":"tutorials/overview/#what-youll-learn","title":"What You'll Learn","text":""},{"location":"tutorials/overview/#data-analysis","title":"Data Analysis","text":"<ul> <li>Machine Gnostics Measures: Understand the core measures and how they differ from traditional statistics.</li> <li>Gnostics Distribution Functions: Learn to fit and interpret GDFs for your data.</li> <li>Gnostics Tests Perform gnostic test on given data samples.</li> <li>Cluster Analysis: Discover how to identify clusters and estimate their bounds (basic and advanced workflows).</li> <li>Interval Analysis: Estimate robust intervals (tolerance, typical, and cluster) for scientific and engineering data (basic and advanced workflows).</li> <li>Uncertainty Analysis: Apply Gnostic methods to real-world uncertainty quantification.</li> </ul>"},{"location":"tutorials/overview/#supervised-machine-learning","title":"Supervised Machine Learning","text":"<ul> <li>Linear Regression: Fit and interpret robust linear models.</li> <li>Polynomial Regression: Model nonlinear relationships with resilience to outliers.</li> <li>Logistic Regression: Perform robust binary classification with flexible probability outputs.</li> <li>MLflow Integration: Track experiments and manage model deployment.</li> </ul>"},{"location":"tutorials/overview/#example-notebooks","title":"Example Notebooks","text":"<p>Explore hands-on Jupyter notebooks for each topic in the Examples section.</p> <p>Ready to begin?</p> <p>Start with the first tutorial or jump to the topic that interests you most. Each tutorial is self-contained and includes code, explanations, and practical tips.</p>"}]}