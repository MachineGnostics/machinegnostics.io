{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Gnostics","text":"<p>Welcome to <code>Machine Gnostics</code>, an innovative Python library designed to implement the principles of Mathematical Gnostics for robust data analysis, modeling, and inference. Unlike traditional statistical approaches that depend heavily on probabilistic assumptions, Machine Gnostics harnesses deterministic algebraic and geometric structures. This unique foundation enables the library to deliver exceptional resilience against outliers, noise, and corrupted data, making it a powerful tool for challenging real-world scenarios.</p> <p> </p> <p>Machine Gnostics</p> <p>Laws of Nature, Encoded\u2014For Everyone!</p> <pre><code>graph LR\n    Entropy[\"Entropy\"]\n    Curvature[\"Space Curvature\"]\n    Bounds[\"[0, \u221e] Bounds\"]\n    Laws[\"Laws of Nature\"]\n    MG[\"Machine Gnostics\"]\n    AI[\"**AI** built with 'Laws of Nature'\"]\n    Universe[\"Universe\"]\n\n    Universe --&gt; Entropy\n    Universe --&gt; Curvature\n    Universe --&gt; Bounds\n\n    Entropy --&gt; Laws\n    Curvature --&gt; Laws\n    Bounds --&gt; Laws\n\n    Laws --&gt; MG\n    Laws -.-&gt; AI\n    MG --&gt; AI\n\n    %% Help center Laws visually\n    Entropy -.-&gt; MG\n    Curvature -.-&gt; MG\n    Bounds -.-&gt; MG\n\n    style Universe stroke-width:2px\n    style Laws stroke-width:3px\n    style MG stroke-width:2px\n    style AI stroke-width:2px</code></pre> <p>Machine Gnostics is an open-source initiative that seeks to redefine the mathematical underpinnings of machine learning. While most conventional ML libraries are grounded in probabilistic and statistical frameworks, Machine Gnostics explores alternative paradigms\u2014drawing from deterministic algebra, information theory, and geometric methods. This approach opens new avenues for building robust, interpretable, and reliable analysis tools that can withstand the limitations of traditional models.</p> <p>Machine Gnostics</p> <p>As a pioneering project, Machine Gnostics invites users to adopt a fresh perspective and develop a new understanding of machine learning. The library is currently in its infancy, and as such, some features may require refinement and fixes. We are actively working to expand its capabilities, with new models and methods planned for the near future. Community support and collaboration are essential to realizing Machine Gnostics\u2019 full potential. Together, let\u2019s build a new AI grounded in a rational and resilient paradigm.</p> <p>Machine Gnostics challenges the limitations of traditional, probabilistic models. Instead of relying on assumptions and large data samples, it encodes the very laws of nature\u2014geometry, physics, entropy\u2014into algorithms that extract truth from data, even when samples are small, noisy, or corrupted.</p>"},{"location":"#data-science-rooted-in-nature","title":"Data Science Rooted in Nature","text":"<p>Machine Gnostics challenges the limitations of traditional, probabilistic models. Instead of relying on assumptions and large data samples, it encodes the very laws of nature\u2014geometry, physics, entropy\u2014into algorithms that extract truth from data, even when samples are small, noisy, or corrupted.</p> <p>\u201cLet data speak for themselves.\u201d Machine Gnostics empowers you to uncover the real structure of your data, free from statistical dogma.</p>"},{"location":"#why-machine-gnostics","title":"Why Machine Gnostics?","text":"<ul> <li>Beyond Statistics: Move past fragile, assumption-heavy models. MG is built for the real world\u2014messy, complex, and unpredictable.</li> <li>Nature-Inspired Algorithms: Deterministic, axiomatic, and robust\u2014rooted in geometry, physics, and information theory.</li> <li>Resilient to Outliers &amp; Noise: Analyze small, corrupted, or outlier-ridden datasets with confidence.</li> <li>Universal &amp; Open:   Free, open-source, and adaptable for science, engineering, and industry.</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li>Advanced Gnostic Data Analysis:   Unlock sophisticated exploratory data analysis (EDA) with algorithms that reveal hidden structures, relationships, and patterns in your data. Designed for data scientists, analysts, and researchers, Machine Gnostics provides tools that go far beyond traditional statistics\u2014enabling deeper, more meaningful insights for both small and complex datasets.</li> <li>Industry-Ready Machine Learning:   Enjoy seamless integration with standard machine learning workflows. Machine Gnostics models support familiar <code>fit</code> and <code>predict</code> methods, making them easy to adopt in any pipeline. With built-in MLflow integration, you can track, version, and deploy models effortlessly\u2014bridging the gap between research and real-world industry applications.</li> <li>Next-Generation Deep Learning (MAGNET):   Prepare for the future with MAGNET (Machine Gnostics Networks), our upcoming deep learning framework. Rooted in the gnostic theorem and the laws of nature, MAGNET will offer a new paradigm for building robust, interpretable neural networks. Stay tuned as we develop this groundbreaking extension to the Machine Gnostics ecosystem.</li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<p>Machine Gnostics encodes the \u201cgnostic cycle\u201d of observation and feedback, letting you model uncertainty as a consequence of real, measurable conditions\u2014not just randomness. See the Concepts page for a deep dive into the science and philosophy behind MG.</p>"},{"location":"#real-world-impact","title":"Real-World Impact","text":"<ul> <li>Testimonials:Hear from scientists and engineers who have solved unsolvable problems with MG.See Testimonials &amp; History.</li> <li>Case Studies:   Explore real applications in thermodynamics, environmental science, and more.   See Tutorials.</li> </ul>"},{"location":"#get-involved","title":"Get Involved","text":"<p>Machine Gnostics is open source and community-driven.</p> <ul> <li>Contribute: Join us on GitHub.</li> <li>Contact: Connect with the community\u2014see Contact.</li> </ul>"},{"location":"#learn-more","title":"Learn More","text":"<ul> <li> <p> Mathematical Gnostics Core concepts</p> </li> <li> <p> Gnostic Fundamental principles</p> </li> <li> <p> Machine Gnostics Library architecture</p> </li> <li> <p> Gnostic Distribution Functions</p> </li> <li> <p> Glossary Terms &amp; definitions</p> </li> <li> <p> References Academic papers</p> </li> <li> <p> Tutorials Get started</p> </li> <li> <p> OSS License: GNU v3.0</p> </li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>Welcome to the Machine Gnostics Community!</p> <p>We\u2019re excited to connect with fellow data enthusiasts, researchers, and learners. Whether you have a question, want to collaborate, or just want to say hello, we\u2019d love to hear from you!</p>"},{"location":"contact/#why-connect-with-us","title":"Why Connect with Us?","text":"<ul> <li>Join our Community: Get access to exclusive tutorials, early updates, and community discussions.</li> <li>Collaborate: Interested in contributing or partnering? Let\u2019s work together on innovative projects.</li> <li>Stay Updated: Subscribe to our newsletter for the latest research, features, and events.</li> </ul>"},{"location":"contact/#how-you-can-contribute","title":"How You Can Contribute","text":"<p>Share your feedback and suggestions, report bugs or request features on GitHub, write tutorials or share your use-cases, or help answer questions in our Discord or LinkedIn groups. Every contribution helps us grow!</p>"},{"location":"contact/#community-spotlight","title":"Community Spotlight","text":"<p>Biweekly, we publish informative articles that break down Machine Gnostics concepts and share them with the data science and research community. We are also open to writing collaborations and new ideas\u2014if you\u2019d like to contribute or suggest a topic, let us know!</p>"},{"location":"contact/#have-a-question-or-idea","title":"Have a Question or Idea?","text":"<p>No question is too small. We're here to help and collaborate \u2014 please reach out using one of the options below:</p>"},{"location":"contact/#social-platforms","title":"Social Platforms","text":"<p>Contact Information</p> <p>Primary Contact:</p> <p>Dr. Nirmal Parmar \ud83d\udce7 info.machinegnostics@gmail.com</p> <p>Community &amp; Social: Discord \u00a0|\u00a0  LinkedIn \u00a0|\u00a0  GitHub \u00a0|\u00a0  Instagram</p>"},{"location":"contact/#subscribe-for-newsletters-latest-updates-and-tutorials","title":"Subscribe for Newsletters, Latest Updates, and Tutorials","text":"<p>Subscribe</p> Your Email: Leave this field empty:          SUBSCRIBE"},{"location":"contact/#lets-connect","title":"Let\u2019s Connect!","text":"<ul> <li>Join our Discord for real-time discussions.</li> <li>Follow us on LinkedIn for professional updates.</li> <li>Star us on GitHub to support the project.</li> <li>Follow us on Instagram for quick updates.</li> </ul> <p>How to Get Support</p> <ul> <li>For issues or feature requests, please open a ticket on our GitHub repository.</li> <li>For consultation, learning sessions, or the latest updates, follow us on LinkedIn or join our GitHub community.</li> <li>For direct inquiries, email us at info.machinegnostics@gmail.com.</li> </ul> <p>We appreciate your interest in Machine Gnostics and look forward to collaborating with researchers, developers, and practitioners passionate about robust and interpretable machine learning.</p> <p>Let's build the future of data science together!</p>"},{"location":"installation/","title":"Installation Guide","text":"<p>Machine Gnostics is distributed as a standard Python package and is designed for easy installation and integration into your data science workflow. The library has been tested on macOS with Python 3.11 and is fully compatible with standard data science libraries.</p>"},{"location":"installation/#1-create-a-python-virtual-environment","title":"1. Create a Python Virtual Environment","text":"<p>It is best practice to use a virtual environment to manage your project dependencies and avoid conflicts with other Python packages.</p> macOS &amp; LinuxWindows <pre><code># Create a new virtual environment named '.mg-env'     \npython3 -m venv .mg-env\n\n# Activate the environment     \nsource .mg-env/bin/activate    \n</code></pre> <pre><code># Create a new virtual environment named '.mg-env'     \npython -m venv .mg-env     \n\n# Activate the environment     \n.mg-env\\Scripts\\activate     \n</code></pre>"},{"location":"installation/#2-install-machine-gnostics","title":"2. Install Machine Gnostics","text":"<p>Install the Machine Gnostics library using pip:</p> macOS &amp; LinuxWindows <pre><code>pip install machinegnostics     \n</code></pre> <pre><code>pip install machinegnostics     \n</code></pre> <p>This command will install Machine Gnostics and automatically resolve its dependencies.</p>"},{"location":"installation/#3-verify-installation","title":"3. Verify Installation","text":"<p>You can verify that Machine Gnostics and its dependencies are installed correctly by importing them in a Python session:</p> <pre><code># check import\nimport machinegnostics\nprint(\"imported successfully!\")\n</code></pre> <p>You can also check the installation with pip:</p> macOS &amp; LinuxWindows <pre><code>pip show machinegnostics     \n</code></pre> <pre><code>pip show machinegnostics     \n</code></pre>"},{"location":"installation/#4-quick-usage-example","title":"4. Quick Usage Example","text":"<p>Machine Gnostics is designed to be as simple to use as other machine learning libraries. You can call its functions and classes directly after installation.</p> <p>Gnostic Distribution Function</p> <pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF\n\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\negdf = EGDF()\negdf.fit(data)\negdf.plot()\nprint(egdf.params)\n</code></pre> <p>Polynomial Regression</p> <pre><code>import numpy as np\nfrom machinegnostics.models import PolynomialRegressor\n\n# Example data\nX = np.array([0., 0.4, 0.8, 1.2, 1.6, 2. ])\ny = np.array([17.89408548, 69.61586934, -7.19890572, 9.37670866, -10.55673099, 16.57855348])\n\n# Create and fit a robust polynomial regression model\nmodel = PolynomialRegressor(degree=2)\nmodel.fit(X, y)\n\nmodel_lr = LinearRegressor()\nmodel_lr.fit(X, y)\n\n# Make predictions\ny_pred = model.predict(X)\ny_pred_lr = model_lr.predict(X)\n\nprint(\"Predictions:\", y_pred)\n\n# coefficients\nprint(\"Coefficients:\", model.coefficients)\n\n# x vs y, y_pred plot\nimport matplotlib.pyplot as plt\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X, y_pred, color='red', label='Polynomial Prediction')\nplt.plot(X, y_pred_lr, color='green', label='Linear Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Polynomial and Linear Regression')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p>Please find step by step tutorial here.</p>"},{"location":"installation/#5-platform-and-environment","title":"5. Platform and Environment","text":"<ul> <li>Operating System: Tested on macOS and Windows 11</li> <li>Python Version: 3.11 recommended</li> <li>Dependencies: Compatible with NumPy, pandas, SciPy, and other standard data science libraries</li> </ul>"},{"location":"installation/#6-troubleshooting","title":"6. Troubleshooting","text":"<ul> <li>Activate Your Environment:   Always activate your virtual environment before installing or running Machine Gnostics.</li> </ul> WindowsmacOS/Linux <pre><code>.mg-env\\Scripts\\activate     \n# or for conda     \nconda activate myenv     \n</code></pre> <pre><code>source .mg-env/bin/activate     \n# or for conda     \nconda activate myenv     \n</code></pre> <ul> <li>Check Your Python Version:   Ensure you are using Python 3.8 or newer.</li> </ul> WindowsmacOS/Linux <pre><code>python --version     \n</code></pre> <pre><code>python3 --version     \n</code></pre> <ul> <li>Upgrade pip:   An outdated pip can cause installation errors. Upgrade pip before installing:</li> </ul> WindowsmacOS/Linux <pre><code>pip install --upgrade pip     \n</code></pre> <pre><code>pip install --upgrade pip     \n</code></pre> <ul> <li>Install from a Clean Environment:If you encounter conflicts, try creating a fresh virtual environment and reinstalling.</li> <li>Check Your Internet Connection:Download errors often result from network issues. Make sure you are connected.</li> <li>Permission Issues:If you see permission errors, avoid using <code>sudo pip install</code>. Instead, use a virtual environment.</li> <li> <p>Still Stuck?</p> <p>Double-check the installation instructions.</p> <p>Contact us or open an issue on GitHub.</p> </li> </ul> <p>Machine Gnostics is designed for simplicity and reliability, making robust machine learning accessible for all Python users.</p>"},{"location":"da/cluster_analysis/","title":"ClusterAnalysis: End-to-End Clustering-Based Bound Estimation (Machine Gnostics)","text":"<p>The <code>ClusterAnalysis</code> class provides a robust, automated workflow for estimating main cluster bounds in a dataset using Gnostic Distribution Functions (GDFs) and advanced clustering analysis. It is designed for interpretable, reproducible interval estimation in scientific, engineering, and data science applications.</p>"},{"location":"da/cluster_analysis/#overview","title":"Overview","text":"<p>ClusterAnalysis orchestrates the entire process of fitting a GDF (ELDF/EGDF), assessing data homogeneity, performing cluster boundary detection, and returning interpretable lower and upper cluster bounds (LCB, UCB) for the main data cluster.</p> <ul> <li>Automated Pipeline: Integrates GDF fitting, homogeneity testing, and cluster analysis.</li> <li>Flexible: Supports both local (ELDF) and global (EGDF) GDFs.</li> <li>Robust: Handles weighted data, bounded/unbounded domains, and advanced parameterization.</li> <li>Diagnostics: Detailed error/warning logging and reproducible parameter tracking.</li> <li>Memory-Efficient: Optional flushing of intermediate results.</li> <li>Visualization: Built-in plotting for GDF and cluster analysis results.</li> </ul>"},{"location":"da/cluster_analysis/#key-features","title":"Key Features","text":"<ul> <li>End-to-end cluster-based bound estimation</li> <li>Integrates GDF fitting, homogeneity testing, and clustering</li> <li>Supports local and global GDFs</li> <li>Handles weighted, bounded, and unbounded data</li> <li>Detailed error and warning logging</li> <li>Memory-efficient operation via flushing</li> <li>Visualization of GDF and cluster analysis results</li> </ul>"},{"location":"da/cluster_analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>verbose</code> bool False Print detailed logs and progress information <code>catch</code> bool True Store intermediate results and diagnostics <code>derivative_threshold</code> float 0.01 Threshold for derivative-based cluster boundary detection <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower probable bound (optional) <code>UB</code> float or None None Upper probable bound (optional) <code>S</code> float or 'auto' 'auto' Scale parameter for GDF ('auto' for automatic estimation) <code>varS</code> bool False Use variable scale parameter during optimization <code>z0_optimize</code> bool True Optimize location parameter Z0 during fitting <code>tolerance</code> float 1e-5 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 1000 Number of points for GDF evaluation <code>homogeneous</code> bool True Assume data homogeneity <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'L-BFGS-B' Optimization method (scipy.optimize) <code>max_data_size</code> int 1000 Max data size for smooth GDF generation <code>flush</code> bool False Flush intermediate results after fitting to save memory"},{"location":"da/cluster_analysis/#attributes","title":"Attributes","text":"<ul> <li>LCB: <code>float or None</code>   Lower Cluster Bound (main cluster lower edge)</li> <li>UCB: <code>float or None</code>   Upper Cluster Bound (main cluster upper edge)</li> <li>params: <code>dict</code>   All parameters, intermediate results, errors, and warnings</li> <li>_fitted: <code>bool</code>   Indicates whether analysis has been completed</li> </ul>"},{"location":"da/cluster_analysis/#methods","title":"Methods","text":""},{"location":"da/cluster_analysis/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Runs the full cluster analysis pipeline on the input data.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array for interval analysis</li> <li>plot: <code>bool</code> (optional)   If True, generates plots for the fitted GDF and cluster analysis</li> </ul> <p>Returns: <code>tuple</code> \u2014 <code>(LCB, UCB)</code> as the main cluster bounds</p>"},{"location":"da/cluster_analysis/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary with the estimated bounds and key results.</p> <p>Returns: <code>dict</code> \u2014 <code>{ 'LCB': float, 'UCB': float }</code></p>"},{"location":"da/cluster_analysis/#plot","title":"<code>plot()</code>","text":"<p>Visualizes the fitted GDF and cluster analysis results (if not flushed).</p> <p>Returns: None (displays plot)</p>"},{"location":"da/cluster_analysis/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import ClusterAnalysis\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize ClusterAnalysis\nca = ClusterAnalysis(verbose=True)\n\n# Fit and get cluster bounds\nLCB, UCB = ca.fit(data)\nprint(f\"Main cluster bounds: LCB={LCB:.3f}, UCB={UCB:.3f}\")\n\n# Visualize results\nca.plot()\n\n# Access results dictionary\nresults = ca.results()\nprint(results)\n</code></pre>"},{"location":"da/cluster_analysis/#notes","title":"Notes","text":"<ul> <li>Designed for robust, interpretable cluster-based bound estimation</li> <li>Works best with local GDFs (ELDF); global GDFs (EGDF) are supported</li> <li>If <code>homogeneous=True</code> but data is heterogeneous, a warning is issued</li> <li>All intermediate parameters, errors, and warnings are tracked in <code>params</code></li> <li>For large datasets or memory-constrained environments, set <code>flush=True</code> to save memory (disables plotting)</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/da_models/","title":"Models - Data Analysis (Machine Gnostics)","text":""},{"location":"da/da_models/#welcome-to-machine-gnostics-data-analysis-models","title":"Welcome to Machine Gnostics Data Analysis Models","text":"<p>Machine Gnostics provides a unified framework for robust, assumption-free data analysis using advanced statistical and gnostic theory principles. The \"Models\" section is your entry point to understanding the core analytical tools available in this library.</p>"},{"location":"da/da_models/#what-are-machine-gnostics-models","title":"What Are Machine Gnostics Models?","text":"<p>Machine Gnostics models are specialized classes and algorithms designed to analyze, interpret, and diagnose data distributions. They go beyond traditional statistics by focusing on universal properties, diagnostic features, and gnostic error measures, making them suitable for a wide range of scientific, engineering, and machine learning applications.</p>"},{"location":"da/da_models/#key-data-analysis-model-categories","title":"Key Data Analysis Model Categories","text":""},{"location":"da/da_models/#gnostic-distribution-distributions-gdf","title":"Gnostic Distribution Distributions (GDF)","text":"<ul> <li>EGDF (Estimating Global Distribution Function)</li> <li>ELDF (Estimating Local Distribution Function)</li> <li>QGDF (Quantifying Global Distribution Function)</li> <li>QLDF (Quantifying Local Distribution Function)</li> </ul> <p>These models provide flexible, non-parametric representations of data distributions, supporting both empirical and quantile-based analysis. For more information, see GDF documentation.</p>"},{"location":"da/da_models/#gnostic-data-tests","title":"Gnostic Data Tests","text":"<ul> <li> <p>Homogeneity Test</p> <p>Functions for assessing data uniformity, analyzing distribution consistency and structural regularity.</p> </li> <li> <p>Scedasticity Test</p> <p>Functions for assessing data variance, volatility, and dispersion characteristics across the dataset.</p> </li> <li> <p>Membership Test</p> <p>Algorithms for quantifying membership relevance and diagnostic scores for individual data points.</p> </li> </ul>"},{"location":"da/da_models/#data-analysis","title":"Data Analysis","text":"<ul> <li> <p>Cluster Analysis</p> <p>Tool for identifying natural groupings and structure in data.</p> </li> <li> <p>Interval Analysis</p> <p>Tool for estimating confidence intervals, bounds, and diagnostic regions with robust handling of outliers.</p> </li> <li> <p>Data Cluster</p> <p>Advanced boundary detection for identifying main clusters from probability density functions.</p> </li> <li> <p>Data Interval</p> <p>Robust interval estimation engine for adaptive scanning and extracting meaningful data ranges.</p> </li> </ul> <p>Each page provides a detailed overview, key features, parameters, example usage, and references.</p>"},{"location":"da/da_models/#why-use-machine-gnostics-models","title":"Why Use Machine Gnostics Models?","text":"<ul> <li>Assumption-Free: No reliance on normality, linearity, or parametric forms.</li> <li>Universal: Applicable to any data type or domain.</li> <li>Diagnostic: Built-in error estimation, entropy measures, and robust statistics.</li> <li>Extensible: Easily integrates with existing Python data science workflows.</li> </ul>"},{"location":"da/da_models/#next-steps","title":"Next Steps","text":"<ul> <li>Browse individual model pages for in-depth documentation and code examples.</li> <li>Try out example notebooks in the examples folder for hands-on learning.</li> <li>Integrate models into your own analysis pipeline for robust, diagnostic data science.</li> </ul>"},{"location":"da/data_cluster/","title":"DataCluster: Advanced Cluster Boundary Detection (Machine Gnostics)","text":"<p>The <code>DataCluster</code> class identifies main cluster boundaries (LCB and UCB) from probability density functions (PDF) of fitted Gnostic Distribution Functions (GDFs), including ELDF, EGDF, QLDF, and QGDF.</p>"},{"location":"da/data_cluster/#overview","title":"Overview","text":"<p>DataCluster performs advanced cluster analysis on fitted GDF objects. It uses a unified derivative-based method on normalized PDF data to precisely locate the boundaries of the main data cluster.</p> <p>Key Features:</p> <ul> <li>Unified Algorithm: Apply consistent boundary detection across all GDF types (ELDF, EGDF, QLDF, QGDF).</li> <li>PDF Normalization: Analyzes min-max normalized PDF values [0,1] for robust thresholding.</li> <li>Derivative Analysis: Locates boundaries where the combined signal (PDF + 1st Derivative) drops below a threshold.</li> <li>Robust Fallback: Automatically falls back to data bounds if boundary detection fails.</li> <li>Diagnostic Plotting: Visualizes the PDF, derivative signals, and detected boundaries.</li> </ul>"},{"location":"da/data_cluster/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> ELDF/EGDF/QLDF/QGDF - Fitted GDF object (must have <code>pdf_points</code> via <code>catch=True</code>). <code>verbose</code> bool False Enable detailed progress reporting and diagnostic output. <code>catch</code> bool True Enable error catching and graceful degradation. <code>derivative_threshold</code> float 0.01 Threshold for boundary detection (lower = wider cluster, higher = narrower)."},{"location":"da/data_cluster/#attributes","title":"Attributes","text":"<ul> <li>LCB: <code>float</code> or <code>None</code>   Cluster Lower Boundary (left edge of the main cluster).</li> <li>UCB: <code>float</code> or <code>None</code>   Cluster Upper Boundary (right edge of the main cluster).</li> <li>z0: <code>float</code> or <code>None</code>   Characteristic point of the distribution (from GDF).</li> <li>S_opt: <code>float</code> or <code>None</code>   Optimal scale parameter (from GDF).</li> <li>pdf_normalized: <code>ndarray</code> or <code>None</code>   Min-max normalized PDF values [0,1] used for analysis.</li> <li>params: <code>dict</code>   Complete analysis results including boundaries, methods used, and diagnostics.</li> </ul>"},{"location":"da/data_cluster/#methods","title":"Methods","text":""},{"location":"da/data_cluster/#fitplotfalse","title":"<code>fit(plot=False)</code>","text":"<p>Perform cluster boundary detection analysis.</p> <ul> <li>plot: <code>bool</code> (default=False). If True, generates a plot of the results.</li> </ul> <p>Returns: <code>Tuple[float, float]</code> \u2014 The detected (LCB, UCB) values. Returns <code>(None, None)</code> if analysis fails.</p>"},{"location":"da/data_cluster/#results","title":"<code>results()</code>","text":"<p>Return comprehensive analysis results dictionary.</p> <p>Returns: <code>Dict</code> \u2014 Dictionary containing:</p> <ul> <li>Cluster Bound results: <code>LCB</code>, <code>UCB</code>, <code>cluster_width</code>, <code>clustering_successful</code></li> <li>GDF Info: <code>gdf_type</code>, <code>Z0</code>, <code>S_opt</code></li> <li>Diagnostics: <code>method_used</code>, <code>normalization_method</code>, <code>errors</code>, <code>warnings</code></li> </ul>"},{"location":"da/data_cluster/#plotfigsize12-8","title":"<code>plot(figsize=(12, 8))</code>","text":"<p>Visualize PDF, boundaries, and derivative analysis.</p> <ul> <li>figsize: <code>tuple</code> (default=(12, 8)).</li> </ul> <p>Returns: None. (Displays a two-panel plot: Top=Original PDF, Bottom=Derivative Analysis).</p>"},{"location":"da/data_cluster/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import QLDF, DataCluster\n\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# 1. Fit GDF (e.g., ELDF) - Ensure catch=True for PDF points\n# create an ELDF object, outlier robust\neldf = ELDF()\n\n# fit the data\neldf.fit(data=data)\n\n# 2. Perform Cluster Analysis\ncluster = DataCluster(gdf=eldf, verbose=True)\nCLB, CUB = cluster.fit(plot=True)\n\nif CLB is not None:\n    print(f\"Main Cluster: [{CLB:.3f}, {CUB:.3f}]\")\n\n# 3. Access Detailed Results\nresults = cluster.results()\nprint(f\"Cluster Width: {results['cluster_width']}\")\n\n# 4. Custom Visualization\ncluster.plot(figsize=(15, 10))\n</code></pre>"},{"location":"da/data_cluster/#notes","title":"Notes","text":"<ul> <li>Unified Method: Clustering now uses a single consistent method (normalized derivative threshold) for all GDF types.</li> <li>Normalization: All PDFs are normalized to [0, 1] to ensure the <code>derivative_threshold</code> behaves consistently regardless of the original data scale.</li> <li>Requirements: The input <code>gdf</code> must be fitted with <code>catch=True</code> so that <code>pdf_points</code> are available for analysis.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-10-10</p>"},{"location":"da/data_interval/","title":"DataIntervals: Robust Interval Estimation Engine (Machine Gnostics)","text":"<p>The <code>DataIntervals</code> class provides robust, adaptive, and diagnostic interval estimation for Gnostics Distribution Function (GDF) classes such as ELDF, EGDF, QLDF, and QGDF. It is designed to estimate meaningful data intervals (such as tolerance and typical intervals) based on the behavior of the GDF's central parameter (Z0) as the data is extended, while enforcing ordering constraints and providing detailed diagnostics.</p>"},{"location":"da/data_interval/#overview","title":"Overview","text":"<p>DataIntervals is designed for advanced interval analysis in scientific, engineering, and reliability applications. It adaptively scans the data domain, extending the data with candidate values and tracking the variation of the central parameter Z0.</p> <p>Key Features:</p> <ul> <li>Adaptive Search: Efficiently scans the data domain with a dense search near the central value (Z0) and sparser search near the boundaries.</li> <li>Robustness: Supports gnostic filtering (clustering) to enhance robustness against outliers.</li> <li>Diagnostics: Provides warnings and errors for suboptimal settings and ordering violations.</li> <li>Ordering Constraint: Ensures that the estimated intervals satisfy the natural ordering: ZL &lt; Z0L &lt; Z0 &lt; Z0U &lt; ZU.</li> <li>Visualization: Plots Z0 variation, estimated intervals, and data coverage.</li> </ul>"},{"location":"da/data_interval/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> ELDF/EGDF/QLDF/QGDF - Fitted GDF (Gnostics Distribution Function) object. <code>n_points</code> int 10 Number of search points for interval estimation. <code>dense_zone_fraction</code> float 0.4 Fraction of the search domain near Z0 to search densely. <code>dense_points_fraction</code> float 0.7 Fraction of points allocated to the dense zone. <code>convergence_window</code> int 15 Number of points in the moving window for convergence. <code>convergence_threshold</code> float 1e-6 Threshold for standard deviation of Z0 in convergence window. <code>min_search_points</code> int 30 Minimum number of search points before checking convergence. <code>boundary_margin_factor</code> float 0.001 Margin factor to avoid searching exactly at the boundaries. <code>extrema_search_tolerance</code> float 1e-6 Tolerance for detecting extrema in Z0 variation. <code>gnostic_filter</code> bool False If True, apply gnostic clustering to filter outlier Z0 values. <code>catch</code> bool True If True, catch and store warnings/errors internally. <code>verbose</code> bool False If True, print detailed progress and diagnostics. <code>flush</code> bool False If True, flush memory after fitting to save resources."},{"location":"da/data_interval/#attributes","title":"Attributes","text":"<ul> <li>ZL: <code>float</code>   Lower bound of the typical data interval.</li> <li>Z0L: <code>float</code>   Lower bound of the tolerance interval (Z0-based).</li> <li>Z0: <code>float</code>   Central value (Z0) of the original GDF.</li> <li>Z0U: <code>float</code>   Upper bound of the tolerance interval (Z0-based).</li> <li>ZU: <code>float</code>   Upper bound of the typical data interval.</li> <li>tolerance_interval: <code>float</code>   Width of the tolerance interval (Z0U - Z0L).</li> <li>typical_data_interval: <code>float</code>   Width of the typical data interval (ZU - ZL).</li> <li>ordering_valid: <code>bool</code>   Whether the ordering constraint (ZL &lt; Z0L &lt; Z0 &lt; Z0U &lt; ZU) is satisfied.</li> <li>params: <code>dict</code>   Dictionary of parameters, warnings, errors, and results.</li> <li>search_results: <code>dict</code>   Raw search results for datum values and corresponding Z0s.</li> </ul>"},{"location":"da/data_interval/#methods","title":"Methods","text":""},{"location":"da/data_interval/#fitplotfalse","title":"<code>fit(plot=False)</code>","text":"<p>Run the interval estimation process. Optionally plot results.</p> <ul> <li>plot: <code>bool</code> (default=False). If True, automatically plot analysis results.</li> </ul> <p>Returns: None</p>"},{"location":"da/data_interval/#results","title":"<code>results()</code>","text":"<p>Return a dictionary of estimated interval results and bounds.</p> <p>Returns: <code>Dict</code> \u2014 A dictionary containing keys for LB, LSB, DLB, LCB, LSD, ZL, Z0L, Z0, Z0U, ZU, USD, UCB, DUB, USB, UB.</p>"},{"location":"da/data_interval/#plot_intervalsfigsize12-8","title":"<code>plot_intervals(figsize=(12, 8))</code>","text":"<p>Plot the Z0 variation and estimated intervals.</p> <ul> <li>figsize: <code>tuple</code> (default=(12, 8)).</li> </ul> <p>Returns: None</p>"},{"location":"da/data_interval/#plotfigsize12-8","title":"<code>plot(figsize=(12, 8))</code>","text":"<p>Plot the GDF, PDF, and intervals on the data domain.</p> <ul> <li>figsize: <code>tuple</code> (default=(12, 8)).</li> </ul> <p>Returns: None</p>"},{"location":"da/data_interval/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import ELDF, DataIntervals\n\ndata = np.array([-13.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# 1. Fit ELDF first\neld = ELDF()\neld.fit(data)\n\n# 2. Run Interval Analysis\n# create a DataIntervals object\ndata_intervals = DataIntervals(gdf=eldf)\n\n# fit the data\ndata_intervals.fit()\n\n# plot\ndata_intervals.plot()\n\n# print intervals\ndata_intervals.results()\n</code></pre>"},{"location":"da/data_interval/#notes","title":"Notes","text":"<ul> <li>GDF Types: For best results, use with ELDF or QLDF.</li> <li>Optimization: increasing <code>n_points</code> improves accuracy but increases computation time.</li> <li>Usage: The class is designed for research and diagnostic use; adjust parameters for your data and application.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-10-10</p>"},{"location":"da/egdf/","title":"EGDF: Estimating Global Distribution Function (Machine Gnostics)","text":"<p>The <code>EGDF</code> class provides robust, assumption-free global distribution estimation for real-world data using the Machine Gnostics framework. Unlike traditional parametric models, EGDF adapts directly to your data, making it ideal for noisy, uncertain, or heterogeneous datasets.</p>"},{"location":"da/egdf/#overview","title":"Overview","text":"<p>EGDF is designed for robust probability and density estimation, especially when data may contain outliers, inner noise, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Robust: Handles outliers, inner noise, and contaminated data.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Parameter Estimation: Scale and bounds inferred from data.</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for EGDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/egdf/#key-features","title":"Key Features","text":"<ul> <li>Fits a global distribution function to your data</li> <li>Robust to outliers and inner noise</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of EGDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> </ul>"},{"location":"da/egdf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter (auto-estimated or fixed value) <code>z0_optimize</code> bool True Optimize location parameter during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 500 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'Powell' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth EGDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/egdf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/egdf/#methods","title":"Methods","text":""},{"location":"da/egdf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the EGDF to your data, estimating all relevant parameters and generating the global distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/egdf/#plotplot_smoothtrue-plotboth-boundstrue-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=True, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted EGDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>   Plot smooth interpolated curve.</li> <li>plot: <code>str</code>   'gdf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>   Show bound lines.</li> <li>extra_df: <code>bool</code>   Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/egdf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/egdf/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize EGDF\negdf = EGDF()\n\n# Fit the model\negdf.fit(data)\n\n# Plot the results\negdf.plot()\n\n# Access fitted parameters\nresults = egdf.results()\nprint(\"Global scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/egdf/#notes","title":"Notes","text":"<ul> <li>EGDF is robust to outliers and suitable for non-Gaussian, contaminated, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., reliability, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/eldf/","title":"ELDF: Estimating Local Distribution Function (Machine Gnostics)","text":"<p>The <code>ELDF</code> class provides robust, assumption-free local distribution estimation for real-world data using the Machine Gnostics framework. ELDF is designed for detailed local analysis, peak detection, and modal characterization, making it ideal for noisy, uncertain, or heterogeneous datasets.</p>"},{"location":"da/eldf/#overview","title":"Overview","text":"<p>ELDF is optimized for local probability and density estimation, especially when data may contain outliers, inner noise, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Robust: Handles outliers, inner noise, and contaminated data.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Parameter Estimation: Scale and bounds inferred from data.</li> <li>Advanced Z0 Estimation: Finds the gnostic mean (location of maximum PDF).</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for ELDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/eldf/#key-features","title":"Key Features","text":"<ul> <li>Fits a local distribution function to your data</li> <li>Robust to outliers and inner noise</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of ELDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> <li>Variable scale parameter option for heteroscedasticity</li> </ul>"},{"location":"da/eldf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter (auto-estimated or fixed value) <code>varS</code> bool False Use variable scale parameter during optimization <code>minimum_varS</code> float 0.1 Minimum scale parameter value if varS is True <code>z0_optimize</code> bool True Optimize location parameter Z0 during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 1000 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'Powell' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth ELDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/eldf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, varS, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/eldf/#methods","title":"Methods","text":""},{"location":"da/eldf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the ELDF to your data, estimating all relevant parameters and generating the local distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/eldf/#plotplot_smoothtrue-plotboth-boundstrue-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=True, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted ELDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>   Plot smooth interpolated curve.</li> <li>plot: <code>str</code>   'eldf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>   Show bound lines.</li> <li>extra_df: <code>bool</code>   Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/eldf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/eldf/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import ELDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize ELDF\neldf = ELDF()\n\n# Fit the model\neldf.fit(data)\n\n# Plot the results\neldf.plot()\n\n# Access fitted parameters\nresults = eldf.results()\nprint(\"Local scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/eldf/#notes","title":"Notes","text":"<ul> <li>ELDF is robust to outliers and suitable for non-Gaussian, contaminated, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., clustering, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of local distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/homogeneity/","title":"DataHomogeneity: Homogeneity Analysis for EGDF (Machine Gnostics)","text":"<p>Analyze data homogeneity for EGDF objects using probability density function analysis. This class provides comprehensive homogeneity analysis for Estimating Global Distribution Functions (EGDF) by examining the shape and characteristics of their probability density functions (PDF).</p>"},{"location":"da/homogeneity/#overview","title":"Overview","text":"<p>The homogeneity criterion is based on the mathematical properties and expected PDF behavior of EGDF according to Mathematical Gnostic theory principles.</p> <p>Gnostic Theory Foundation: The EGDF is uniquely determined by the data sample and finds the optimal scale parameter automatically. Unlike local distribution functions, EGDF has limited flexibility and provides a unique representation for each homogeneous data sample. The key principle is that homogeneous data should produce a distribution with a single density maximum, while non-homogeneous data will exhibit multiple maxima or negative density values.</p> <p>Homogeneity Criteria:</p> <ul> <li>EGDF (Estimating Global Distribution Function): Data is considered homogeneous if:<ol> <li>PDF has exactly one global maximum (single peak)</li> <li>PDF contains no negative values</li> </ol> </li> </ul> <p>EGDF Characteristics:</p> <ul> <li>Uniqueness: EGDF finds the best scale parameter automatically, providing a unique model</li> <li>Robustness: EGDF is robust with respect to outliers</li> <li>Homogeneity Testing: Particularly suitable for reliable data homogeneity testing</li> <li>Global Nature: Uses normalized weights resulting in limited flexibility controlled by optimal scale</li> <li>Data-Driven: Primary parameters are the data themselves, following gnostic \"let data speak\" principle</li> </ul> <p>Non-Homogeneity Detection:</p> <p>EGDF can sensitively detect two main causes of non-homogeneity: 1. Outliers: Individual data points significantly different from others, creating local maxima 2. Clusters: Separate groups in the data, resulting in multiple density peaks</p> <p>Analysis Pipeline:</p> <ol> <li>Validation: Ensures input is EGDF only (rejects QGDF/ELDF/QLDF)</li> <li>PDF Extraction: Retrieves PDF points from fitted EGDF object</li> <li>Smoothing: Applies Gaussian filtering for noise reduction</li> <li>Maxima Detection: Identifies peaks in the smoothed PDF</li> <li>Homogeneity Assessment: Evaluates based on peak count and PDF negativity</li> <li>Result Storage: Comprehensive parameter collection and storage</li> </ol>"},{"location":"da/homogeneity/#key-features","title":"Key Features","text":"<ul> <li>Automatic EGDF validation</li> <li>Robust peak detection with configurable smoothing</li> <li>Comprehensive error and warning tracking</li> <li>Memory management with optional data flushing</li> <li>Detailed visualization of analysis results</li> <li>Integration with existing GDF parameter systems</li> </ul>"},{"location":"da/homogeneity/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> EGDF - Fitted Estimating Global Distribution Function object. <code>verbose</code> bool False Controls output verbosity during analysis. <code>catch</code> bool True Enables comprehensive result storage in params dictionary. <code>flush</code> bool False Controls memory management of large arrays after analysis. <code>smoothing_sigma</code> float 1.0 Gaussian smoothing parameter for PDF preprocessing. <code>min_height_ratio</code> float 0.01 Minimum relative height threshold for peak detection. <code>min_distance</code> int (Optional) None Minimum separation between detected peaks in array indices."},{"location":"da/homogeneity/#attributes","title":"Attributes","text":"<ul> <li>is_homogeneous: <code>bool</code> or <code>None</code>   Primary analysis result. None before fit(), True/False after analysis.</li> <li>picks: <code>List[Dict]</code>   Detected maxima with detailed information.</li> <li>z0: <code>float</code> or <code>None</code>   Global optimum value from EGDF object or detected from PDF.</li> <li>global_extremum_idx: <code>int</code> or <code>None</code>   Array index of the global maximum.</li> <li>fitted: <code>bool</code>   Read-only property indicating if analysis has been completed.</li> </ul>"},{"location":"da/homogeneity/#methods","title":"Methods","text":""},{"location":"da/homogeneity/#fitplotfalse","title":"<code>fit(plot=False)</code>","text":"<p>Perform comprehensive homogeneity analysis on the EGDF object. It analyzes the probability density function (PDF) of the fitted EGDF object to determine if the underlying data exhibits homogeneous characteristics.</p> <ul> <li>plot: <code>bool</code> (optional, default=False)   If True, generates plots for visual inspection of the analysis results.</li> </ul> <p>Returns: <code>bool</code> \u2014 True if data is homogeneous, False otherwise.</p>"},{"location":"da/homogeneity/#results","title":"<code>results()</code>","text":"<p>Retrieve comprehensive homogeneity analysis results and metadata.</p> <p>Returns: <code>Dict[str, Any]</code> \u2014 Comprehensive results dictionary including core findings, maxima information, PDF characteristics, analysis configuration, and diagnostics.</p>"},{"location":"da/homogeneity/#plotfigsize12-8-titlenone","title":"<code>plot(figsize=(12, 8), title=None)</code>","text":"<p>Create a comprehensive visualization of the homogeneity analysis results, displaying the PDF, detected maxima, and status.</p> <ul> <li>figsize: <code>tuple</code> (default=(12, 8))   Figure dimensions in inches as (width, height).</li> <li>title: <code>str</code> (optional)   Custom plot title.</li> </ul> <p>Returns: None</p>"},{"location":"da/homogeneity/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF, DataHomogeneity\n\n# Homogeneous data\ndata = np.array([ -3, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\negdf = EGDF(data=data, catch=True)\negdf.fit()\n\n# Homogeneity analysis\nhomogeneity = DataHomogeneity(egdf, verbose=True)\nis_homogeneous = homogeneity.fit(plot=True)\nprint(f\"Data is homogeneous: {is_homogeneous}\")\n\n# Access results\nresults = homogeneity.results()\nprint(f\"Number of maxima detected: {len(results['picks'])}\")\n</code></pre>"},{"location":"da/homogeneity/#notes","title":"Notes","text":"<p>Mathematical Background:</p> <p>The gnostic homogeneity analysis is based on the principle that homogeneous data should produce a unimodal PDF with specific characteristics for EGDF:</p> <ul> <li>EGDF Uniqueness: Each data sample has exactly one optimal EGDF representation</li> <li>Scale Optimization: EGDF automatically finds the best scale parameter S_opt</li> <li>Density Properties: Homogeneous data produces single maximum, non-negative density</li> <li>Numerical Sensitivity: Analysis must be numerical, not based on visual inspection</li> </ul> <p>Why Only EGDF:</p> <p>Homogeneity testing is only applicable to EGDF because:</p> <ul> <li>EGDF provides unique representation for each data sample</li> <li>Automatic scale parameter optimization enables reliable homogeneity testing</li> <li>Global nature with normalized weights makes it suitable for detecting data structure</li> <li>Robustness against outliers while maintaining sensitivity to detect them</li> </ul> <p>Parameter Tuning Guidelines:</p> <ul> <li>smoothing_sigma: Start with 1.0, increase for noisy data to improve numerical stability</li> <li>min_height_ratio: Start with 0.01, increase to reduce false positives from noise</li> <li>min_distance: Usually auto-calculated, manually set for specific data characteristics</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-10-10</p>"},{"location":"da/interval_analysis/","title":"IntervalAnalysis: Marginal Interval Analysis (Machine Gnostics)","text":"<p>The <code>IntervalAnalysis</code> class provides robust, adaptive, and diagnostic interval estimation for Gnostic Distribution Functions (GDFs) such as ELDF, EGDF, QLDF, and QGDF. It estimates meaningful data intervals (tolerance, typical intervals) based on the behavior of the GDF's central parameter (Z0) as the data is extended, enforcing ordering constraints and providing detailed diagnostics.</p>"},{"location":"da/interval_analysis/#overview","title":"Overview","text":"<p>IntervalAnalysis orchestrates the complete process of fitting GDFs, checking homogeneity, and computing robust data intervals using the DataIntervals engine. It is designed for reliability, diagnostics, and adaptive interval estimation in scientific and engineering data analysis.</p> <p>Gnostic vs. Statistical Interval Analysis:Gnostic interval analysis does not rely on probabilistic or statistical assumptions. Instead, it uses algebraic and geometric properties of the data and distribution functions, providing deterministic, reproducible, and interpretable intervals even for small, noisy, or non-Gaussian datasets. This is fundamentally different from classical statistical interval estimation, which depends on distributional assumptions and sampling theory.</p> <ul> <li>Assumption-Free: No parametric or probabilistic assumptions.</li> <li>Robust: Handles outliers, heterogeneity, and bounded/unbounded domains.</li> <li>Adaptive: Intervals adapt to data structure and central parameter behavior.</li> <li>Diagnostics: Tracks warnings, errors, and intermediate results.</li> <li>Visualization: Built-in plotting for distributions and intervals.</li> <li>Memory-Efficient: Optional flushing of intermediate arrays.</li> </ul>"},{"location":"da/interval_analysis/#key-features","title":"Key Features","text":"<ul> <li>End-to-end interval estimation for GDFs</li> <li>Automatic homogeneity testing and diagnostics</li> <li>Adaptive tolerance and typical interval computation</li> <li>Handles weighted, bounded, and unbounded data</li> <li>Detailed error and warning logging</li> <li>Visualization of fitted distributions and intervals</li> <li>Deterministic and reproducible results</li> </ul>"},{"location":"da/interval_analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter for distribution <code>z0_optimize</code> bool True Optimize central parameter Z0 during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 10 Number of points for distribution evaluation <code>n_points_gdf</code> int 1000 Number of points for smooth GDF generation <code>homogeneous</code> bool True Assume data homogeneity (enables homogeneity testing) <code>catch</code> bool True Store warnings/errors and intermediate results <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'Powell' Optimization method (scipy.optimize) <code>verbose</code> bool False Print detailed progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth GDF generation <code>flush</code> bool True Flush intermediate arrays after fitting <code>dense_zone_fraction</code> float 0.4 Fraction of domain near Z0 for dense interval search <code>dense_points_fraction</code> float 0.7 Fraction of search points in dense zone <code>convergence_window</code> int 15 Window size for convergence detection <code>convergence_threshold</code> float 1e-6 Threshold for Z0 convergence <code>min_search_points</code> int 30 Minimum search points before checking convergence <code>boundary_margin_factor</code> float 0.001 Margin factor to avoid searching at boundaries <code>extrema_search_tolerance</code> float 1e-6 Tolerance for detecting extrema in Z0 variation <code>gnostic_filter</code> bool False Apply gnostic clustering to filter outlier Z0 values <code>cluster_bounds</code> bool True Estimate cluster bounds using DataCluster <code>membership_bounds</code> bool True Estimate membership bounds using DataMembership"},{"location":"da/interval_analysis/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Stores all warnings, errors, and diagnostic information from the analysis.</li> </ul>"},{"location":"da/interval_analysis/#methods","title":"Methods","text":""},{"location":"da/interval_analysis/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Runs the complete interval analysis workflow on the input data.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>1D numpy array of input data for interval analysis</li> <li>plot: <code>bool</code> (optional)   If True, automatically generates diagnostic plots after fitting</li> </ul> <p>Returns: <code>dict</code> \u2014 Estimated interval bounds and diagnostics</p>"},{"location":"da/interval_analysis/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of estimated interval results and bounds. Also called 'Data Certification'</p> <p>Returns: <code>dict</code> \u2014 Contains keys such as <code>'LB', 'LSB', 'DLB', 'LCB', 'LSD', 'ZL', 'Z0L', 'Z0', 'Z0U', 'ZU', 'USD', 'UCB', 'DUB', 'USB', 'UB'</code></p> <pre><code>-**LB**: Lower Bound\nThe practical lower limit for the interval (may be set by user or inferred).\n\n-**LSB**: Lower Sample (Membership) Bound\nThe lowest value for which data is homogeneous.\n\n-**DLB**: Data Lower Bound\nThe absolute minimum value present in the data.\n\n-**LCB**: Lower Cluster Bound\nThe lower edge of the main data cluster.\n\n-**LSD**: Lower Standard Deviation Bound\nThe lowest value as per gnostic standard deviation.\n\n-**ZL**: Z0 Lower Interval\nThe lower bound of the typical interval.\n\n-**Z0L**: Z0 Lower Bound\nThe lower bound of the tolerance interval.\n\n-**Z0**: Central Value (Gnostic Mean)\nThe central parameter of the distribution (gnostic mean).\n\n-**Z0U**: Z0 Upper Bound\nThe upper bound of the tolerance interval.\n\n-**ZU**: Z0 Upper Interval\nThe upper bound of the typical interval.\n\n-**USD**: Upper Support/Domain Bound\nThe highest value in the support or domain of the fitted distribution.\n\n-**UCB**: Upper Cluster Bound\nThe upper edge of the main data cluster.\n\n-**DUB**: Data Upper Bound\nThe absolute maximum value present in the data.\n\n-**USB**: Upper Sample (Membership) Bound\nThe highest value for which data is homogeneous (membership analysis).\n\n-**UB**: Upper Bound\nThe practical upper limit for the interval (may be set by user or inferred).\n</code></pre>"},{"location":"da/interval_analysis/#plotgdftrue-intervalstrue","title":"<code>plot(GDF=True, intervals=True)</code>","text":"<p>Visualizes the fitted GDFs and the estimated intervals.</p> <ul> <li>GDF: <code>bool</code> (default: True)Plot the fitted ELDF (local distribution function)</li> <li>intervals: <code>bool</code> (default: True)   Plot the estimated intervals and Z0 variation</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/interval_analysis/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import IntervalAnalysis\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize IntervalAnalysis\nia = IntervalAnalysis(verbose=True)\n\n# Fit and get interval results\nia.fit(data, plot=True)\nprint(ia.results())\n\n# Visualize results\nia.plot(GDF=True, intervals=True)\n</code></pre>"},{"location":"da/interval_analysis/#notes","title":"Notes","text":"<ul> <li>Gnostic interval analysis is fundamentally different from statistical interval analysis: it does not rely on probability or sampling theory, but on algebraic and geometric properties of the data and distribution functions.</li> <li>Homogeneity of the data is checked automatically; warnings are issued if violated.</li> <li>Suitable for scientific, engineering, and reliability applications.</li> <li>All warnings and errors are stored in the <code>params</code> attribute for later inspection.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"da/membership/","title":"DataMembership: Gnostic Membership Test (Machine Gnostics)","text":"<p>The <code>DataMembership</code> class provides a robust method to test whether a value can be considered a member of a homogeneous data sample, using the EGDF (Estimating Global Distribution Function) framework. It determines the bounds within which new data points can be added to a sample without disrupting its homogeneity.</p>"},{"location":"da/membership/#overview","title":"Overview","text":"<p>Membership Test: \"Is a value Z\u03be a potential member of the given sample Z?\" In other words: \"Will the homogeneous sample Z remain homogeneous after extension by Z\u03be?\"</p> <p>Logic Process:</p> <ol> <li>Homogeneity Check: First, check if the sample Z is homogeneous using <code>DataHomogeneity</code>.</li> <li>Extension Test: If Z is homogeneous, extend the sample with a candidate value Z\u03be and check if the extended sample remains homogeneous.</li> <li>Bound Search: Determine the Lower Sample Bound (LSB) and Upper Sample Bound (USB).<ul> <li>LSB search range: [LB, DLB]</li> <li>USB search range: [DUB, UB]</li> </ul> </li> <li>Result: Find the minimum and maximum values of Z\u03be that preserve homogeneity.</li> </ol>"},{"location":"da/membership/#key-features","title":"Key Features","text":"<ul> <li>Homogeneity Preservation: Membership is defined by the preservation of sample homogeneity.</li> <li>Adaptive Bound Search: Iteratively finds the exact boundaries where homogeneity is lost.</li> <li>Diagnostic Visualization: Plots EGDF, PDF, and the determined membership bounds.</li> </ul>"},{"location":"da/membership/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> EGDF - Fitted EGDF object (must be fitted and contain data). <code>verbose</code> bool False If True, detailed logs are printed during execution. <code>catch</code> bool True If True, errors and warnings are stored in <code>params</code>. <code>tolerance</code> float 1e-3 Tolerance level for numerical calculations. <code>max_iterations</code> int 100 Maximum number of iterations for bound search. <code>initial_step_factor</code> float 0.001 Initial step size factor for adaptive bound search."},{"location":"da/membership/#attributes","title":"Attributes","text":"<ul> <li>LSB: <code>float</code> or <code>None</code>   The calculated Lower Sample Bound.</li> <li>USB: <code>float</code> or <code>None</code>   The calculated Upper Sample Bound.</li> <li>is_homogeneous: <code>bool</code>   Indicates whether the original data sample is homogeneous.</li> <li>params: <code>dict</code>   Stores results, errors, warnings, and search parameters.</li> <li>fitted: <code>bool</code>   Indicates whether the membership analysis has been completed.</li> </ul>"},{"location":"da/membership/#methods","title":"Methods","text":""},{"location":"da/membership/#fit","title":"<code>fit()</code>","text":"<p>Performs the membership analysis to determine the Lower Sample Bound (LSB) and Upper Sample Bound (USB).</p> <p>Returns: <code>Tuple[Optional[float], Optional[float]]</code> \u2014 The calculated LSB and USB values.</p>"},{"location":"da/membership/#plot","title":"<code>plot()</code>","text":"<p>Generates a plot of the EGDF and PDF with membership bounds.</p> <p>Parameters:</p> <ul> <li><code>plot_smooth</code> (bool, default=True): If True, plots smoothed curves.</li> <li><code>plot</code> (str, default='both'): 'gdf', 'pdf', or 'both'.</li> <li><code>bounds</code> (bool, default=True): If True, includes data bounds.</li> <li><code>figsize</code> (tuple, default=(12, 8)): Figure size.</li> </ul> <p>Returns: None</p>"},{"location":"da/membership/#results","title":"<code>results()</code>","text":"<p>Returns the analysis results stored in the <code>params</code> attribute.</p> <p>Returns: <code>Dict[str, Any]</code> \u2014 Analysis results including LSB, USB, homogeneity status, and parameters.</p>"},{"location":"da/membership/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import EGDF, DataMembership\n\n# 1. Prepare homogeneous data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# 2. Fit EGDF\negdf = EGDF(data=data, catch=True)\negdf.fit()\n\n# 3. Perform Membership Analysis\nmembership = DataMembership(gdf=egdf, verbose=True)\nlsb, usb = membership.fit()\n\nprint(f\"Lower Bound: {lsb}\")\nprint(f\"Upper Bound: {usb}\")\n\n# 4. Visualize Results\nmembership.plot()\n\n# 5. Get detailed results\nresults = membership.results()\nprint(results)\n</code></pre>"},{"location":"da/membership/#notes","title":"Notes","text":"<ul> <li>Requirement: This analysis only works with EGDF objects.</li> <li>Prerequisite: The initial sample must be homogeneous. If <code>DataHomogeneity</code> finds it non-homogeneous, the analysis proceeds no further.</li> <li>Output: LSB and USB represent the safe range for extending the dataset while maintaining its structural distribution properties.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-10-10</p>"},{"location":"da/qgdf/","title":"QGDF: Quantifying Global Distribution Function (Machine Gnostics)","text":"<p>The <code>QGDF</code> class provides robust, assumption-free global quantification of data distributions using the Machine Gnostics framework. QGDF is designed for inlier-resistant, sample-wide analysis, making it ideal for heterogeneous, clustered, or uncertain datasets where dense regions may dominate.</p>"},{"location":"da/qgdf/#overview","title":"Overview","text":"<p>QGDF is optimized for global quantification and density estimation, especially when data may contain dense clusters, inliers, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Inlier-Resistant: Robust to dense clusters and inliers.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Parameter Estimation: Scale and bounds inferred from data.</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for QGDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/qgdf/#key-features","title":"Key Features","text":"<ul> <li>Fits a global quantifying distribution function to your data</li> <li>Robust to inliers and dense clusters</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of QGDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> </ul>"},{"location":"da/qgdf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter (auto-estimated or fixed value) <code>z0_optimize</code> bool True Optimize location parameter during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 500 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'Powell' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth QGDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/qgdf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/qgdf/#methods","title":"Methods","text":""},{"location":"da/qgdf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the QGDF to your data, estimating all relevant parameters and generating the global quantifying distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/qgdf/#plotplot_smoothtrue-plotboth-boundstrue-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=True, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted QGDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>Plot smooth interpolated curve.</li> <li>plot: <code>str</code>'qgdf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>Show bound lines.</li> <li>extra_df: <code>bool</code>Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/qgdf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/qgdf/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import QGDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize QGDF\nqgdf = QGDF()\n\n# Fit the model\nqgdf.fit(data)\n\n# Plot the results\nqgdf.plot()\n\n# Access fitted parameters\nresults = qgdf.results()\nprint(\"Global scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/qgdf/#notes","title":"Notes","text":"<ul> <li>QGDF is robust to inliers and suitable for non-Gaussian, clustered, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., reliability, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"da/qldf/","title":"QLDF: Quantifying Local Distribution Function (Machine Gnostics)","text":"<p>The <code>QLDF</code> class provides robust, assumption-free local quantification of data distributions using the Machine Gnostics framework. QLDF is designed for inlier-resistant, detailed local analysis, making it ideal for heterogeneous, clustered, or uncertain datasets where dense regions may dominate.</p>"},{"location":"da/qldf/#overview","title":"Overview","text":"<p>QLDF is optimized for local quantification and density estimation, especially when data may contain dense clusters, inliers, or unknown distributions. It leverages gnostic algebra and error geometry to deliver resilient, interpretable results without requiring prior statistical assumptions.</p> <ul> <li>Assumption-Free: No parametric forms or distributional assumptions.</li> <li>Inlier-Resistant: Robust to dense clusters and inliers.</li> <li>Flexible: Supports additive and multiplicative data forms.</li> <li>Weighted Data: Incorporates sample weights for advanced analysis.</li> <li>Automatic Z0 Identification: Finds local minima in probability density.</li> <li>Advanced Interpolation: Precise estimation of critical points.</li> <li>Memory-Efficient: Optimized for large datasets.</li> <li>Visualization: Built-in plotting for QLDF and PDF.</li> <li>Customizable: Multiple solver options, bounds, and precision settings.</li> </ul>"},{"location":"da/qldf/#key-features","title":"Key Features","text":"<ul> <li>Fits a local quantifying distribution function to your data</li> <li>Robust to inliers and dense clusters</li> <li>Supports weighted and unweighted samples</li> <li>Automatic or manual bounds and scale selection</li> <li>Additive ('a') and multiplicative ('m') data forms</li> <li>Advanced optimization with customizable tolerance and solver</li> <li>Visualization of QLDF, PDF, and bounds</li> <li>Memory-efficient for large datasets</li> <li>Detailed results and diagnostics</li> <li>Variable scale parameter option for heteroscedasticity</li> </ul>"},{"location":"da/qldf/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>DLB</code> float or None None Data Lower Bound (absolute minimum, optional) <code>DUB</code> float or None None Data Upper Bound (absolute maximum, optional) <code>LB</code> float or None None Lower Probable Bound (practical lower limit, optional) <code>UB</code> float or None None Upper Probable Bound (practical upper limit, optional) <code>S</code> float or 'auto' 'auto' Scale parameter (auto-estimated or fixed value) <code>varS</code> bool False Use variable scale parameter during optimization <code>minimum_varS</code> float 0.1 Minimum scale parameter value if varS is True <code>z0_optimize</code> bool True Optimize location parameter Z0 during fitting <code>tolerance</code> float 1e-9 Convergence tolerance for optimization <code>data_form</code> str 'a' Data form: 'a' (additive), 'm' (multiplicative) <code>n_points</code> int 500 Number of points for distribution curve <code>homogeneous</code> bool True Assume data homogeneity <code>catch</code> bool True Store intermediate results (memory usage) <code>weights</code> np.ndarray or None None Prior weights for data points <code>wedf</code> bool False Use Weighted Empirical Distribution Function <code>opt_method</code> str 'Powell' Optimization method (scipy.optimize) <code>verbose</code> bool False Print progress and diagnostics <code>max_data_size</code> int 1000 Max data size for smooth QLDF generation <code>flush</code> bool True Flush large arrays (memory management)"},{"location":"da/qldf/#attributes","title":"Attributes","text":"<ul> <li>params: <code>dict</code>   Fitted parameters and results after fitting.</li> <li>DLB, DUB, LB, UB, S, varS, z0_optimize, tolerance, data_form, n_points, homogeneous, catch, weights, wedf, opt_method, verbose, max_data_size, flush:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"da/qldf/#methods","title":"Methods","text":""},{"location":"da/qldf/#fitdata-plotfalse","title":"<code>fit(data, plot=False)</code>","text":"<p>Fits the QLDF to your data, estimating all relevant parameters and generating the local quantifying distribution function.</p> <ul> <li>data: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Input data array.</li> <li>plot: <code>bool</code> (optional)   If True, automatically plots the fitted distribution.</li> </ul> <p>Returns: None (results stored in <code>params</code>)</p>"},{"location":"da/qldf/#plotplot_smoothtrue-plotboth-boundstrue-extra_dftrue-figsize128","title":"<code>plot(plot_smooth=True, plot='both', bounds=True, extra_df=True, figsize=(12,8))</code>","text":"<p>Visualizes the fitted QLDF and related plots.</p> <ul> <li>plot_smooth: <code>bool</code>   Plot smooth interpolated curve.</li> <li>plot: <code>str</code>   'qldf', 'pdf', or 'both'.</li> <li>bounds: <code>bool</code>   Show bound lines.</li> <li>extra_df: <code>bool</code>   Include additional distribution functions.</li> <li>figsize: <code>tuple</code>   Figure size.</li> </ul> <p>Returns: None (displays plot)</p>"},{"location":"da/qldf/#results","title":"<code>results()</code>","text":"<p>Returns a dictionary of all fitted parameters and results.</p> <p>Returns: <code>dict</code> (fitted parameters, bounds, scale, diagnostics, etc.)</p>"},{"location":"da/qldf/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import QLDF\n\n# Example data\ndata = np.array([ -13.5, 0, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])\n\n# Initialize QLDF\nqldf = QLDF()\n\n# Fit the model\nqldf.fit(data)\n\n# Plot the results\nqldf.plot()\n\n# Access fitted parameters\nresults = qldf.results()\nprint(\"Local scale parameter:\", results['S_opt'])\nprint(\"Distribution bounds:\", results['LB'], results['UB'])\n</code></pre>"},{"location":"da/qldf/#notes","title":"Notes","text":"<ul> <li>QLDF is robust to inliers and suitable for non-Gaussian, clustered, or uncertain data.</li> <li>Supports both additive and multiplicative data forms.</li> <li>Use weights for advanced analysis (e.g., clustering, risk).</li> <li>For large datasets, set <code>catch=False</code> to save memory.</li> <li>Visualization options allow in-depth analysis of local distribution structure.</li> <li>For more information, see GDF documentation and Machine Gnostics.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"da/scedasticity/","title":"DataScedasticity: Gnostic Homoscedasticity and Heteroscedasticity Test (Machine Gnostics)","text":"<p>The <code>DataScedasticity</code> class provides a gnostic approach to testing for homoscedasticity and heteroscedasticity in data using local gnostic distribution functions (ELDF, QLDF). Unlike classical statistical tests that check residuals, this method analyzes the variability of the gnostic scale parameter.</p>"},{"location":"da/scedasticity/#overview","title":"Overview","text":"<p>This class determines whether a dataset is homoscedastic (constant scale) or heteroscedastic (varying scale) using a fitted Gnostic (Local) Distribution Function (GDF).</p> <p>The Gnostic Approach: If the estimated Scale parameter (<code>S</code>) is variable across the domain (<code>varS=True</code>), the data is considered heteroscedastic. If the Scale parameter is constant, the data is homoscedastic.</p> <p>Key differences vs. standard tests: - Uses GDF's scale parameter (<code>S_var</code>) instead of residual-based heuristics. - Works with local/global gnostic scale (<code>S_local</code>, <code>S_opt</code>) for inference. - Integrates directly with Machine Gnostics models (<code>ELDF</code>, <code>QLDF</code>).</p>"},{"location":"da/scedasticity/#key-features","title":"Key Features","text":"<ul> <li>Direct GDF Integration: Works seamlessly with <code>ELDF</code> and <code>QLDF</code>.</li> <li>Scale-Based Inference: Uses the fundamental gnostic scale parameter.</li> <li>Robust Classification: Simple decision boundary based on scale variability.</li> <li>Detailed Diagnostics: Provides access to local and global scale parameters.</li> </ul>"},{"location":"da/scedasticity/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>gdf</code> <code>Union[ELDF, QLDF]</code> <code>ELDF</code> Fitted GDF instance (must be <code>ELDF</code> or <code>QLDF</code>) configured with <code>varS=True</code>. <code>catch</code> <code>bool</code> <code>True</code> Whether to store analysis results in <code>params</code>. <code>verbose</code> <code>bool</code> <code>False</code> Enables debug-level logging when <code>True</code>."},{"location":"da/scedasticity/#attributes","title":"Attributes","text":"<ul> <li>gdf: <code>Union[ELDF, QLDF]</code>   The underlying GDF instance.</li> <li>params: <code>dict</code>   Results container populated after <code>fit()</code>.</li> <li>fitted: <code>bool</code>   Boolean indicating whether scedasticity classification has been run.</li> </ul>"},{"location":"da/scedasticity/#methods","title":"Methods","text":""},{"location":"da/scedasticity/#fit","title":"<code>fit()</code>","text":"<p>Classify scedasticity based on the variability of the GDF's scale parameter.</p> <p>Returns: <code>bool</code> \u2014 <code>True</code> if data is homoscedastic (constant scale), <code>False</code> if heteroscedastic.</p>"},{"location":"da/scedasticity/#results","title":"<code>results()</code>","text":"<p>Return a dictionary summarizing the scedasticity analysis.</p> <p>Returns: <code>dict</code> \u2014 Contains keys:</p> <ul> <li><code>'scedasticity'</code>: <code>'homoscedastic'</code> or <code>'heteroscedastic'</code></li> <li><code>'scale_parameter'</code>: The <code>S_var</code> array from the GDF</li> <li><code>'S_local'</code>: Local gnostic scale values</li> <li><code>'S_global'</code>: Global/optimal gnostic scale (<code>S_opt</code>)</li> </ul>"},{"location":"da/scedasticity/#example-usage","title":"Example Usage","text":"PythonOutput <pre><code>import numpy as np\nfrom machinegnostics.magcal import ELDF, DataScedasticity\n\n# Example data (stack-loss data)\ny = np.array([ 7,  8,  8,  8,  9, 11, 12, 13, 14, 14, 15, 15, 15, 18, 18, 19, 20, 28, 37, 37, 42])\n\n# Step 1: Fit ELDF with variable scale (essential for scedasticity check)\neldf = ELDF(varS=True, verbose=False)\neldf.fit(data=y, plot=True)\n\n# Step 2: Run scedasticity test\nsc = DataScedasticity(gdf=eldf, verbose=True)\nis_homo = sc.fit()\n\n# Step 3: Check results\nprint(f\"Is data homoscedastic? {is_homo}\")\ninfo = sc.results()\n</code></pre>"},{"location":"da/scedasticity/#notes","title":"Notes","text":"<ul> <li>The supplied <code>gdf</code> must be an instance of <code>ELDF</code> or <code>QLDF</code>, fitted with <code>varS=True</code>.</li> <li>Validation is performed during initialization to ensure the GDF is compatible.</li> <li><code>fit()</code> sets the internal state; call <code>results()</code> afterwards to obtain a structured dictionary of outputs.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-10-10</p>"},{"location":"datasets/animal_data/","title":"make_animals_check_data: Gnostic Animals Dataset","text":"<p>The <code>make_animals_check_data</code> function retrieves the 'Animals' dataset (Rousseeuw &amp; Leroy, 1987), a classic small and challenging dataset for robust regression. It contains body weight and brain weight for 28 animals.</p>"},{"location":"datasets/animal_data/#overview","title":"Overview","text":"<p>This dataset is famous in robust statistics because it contains severe outliers that can skew traditional regression models. The goal is typically to model the relationship between body weight and brain weight.</p> <ul> <li>Features (\\(X\\)): Body weight in kilograms (kg).</li> <li>Target (\\(y\\)): Brain weight in grams (g).</li> <li>Challenge: The dataset spans several orders of magnitude (from a Mouse to a Brachiosaurus). It includes 3 dinosaurs (Diplodocus, Brachiosaurus, Triceratops) which act as \"bad leverage\" points\u2014they have massive body weights but relatively small brains compared to the mammalian trend.</li> <li>Suggested Usage: Regression analysis, often with log-transformation: \\(\\log(\\text{Brain}) \\sim \\log(\\text{Body})\\).</li> </ul>"},{"location":"datasets/animal_data/#returns","title":"Returns","text":"Return Type Description <code>X</code> numpy.ndarray Body weight in kg. Shape <code>(28, 1)</code>. <code>y</code> numpy.ndarray Brain weight in g. Shape <code>(28,)</code>. <code>names</code> list of str The common name of the animal for each data point (e.g., \"Mouse\")."},{"location":"datasets/animal_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_animals_check_data\nimport numpy as np\n\n# Load the dataset\nX, y, names = make_animals_check_data()\n</code></pre>"},{"location":"datasets/animal_data/#dataset-content","title":"Dataset Content","text":"<p>The dataset includes the following animals: Mountain beaver, Cow, Grey wolf, Goat, Guinea pig, Diplodocus, Asian elephant, Rhesus monkey, Kangaroo, Golden hamster, Mouse, Rabbit, Sheep, Jaguar, Chimpanzee, Rat, Brachiosaurus, Mole, Pig, Echidna, Triceratops, Pigmy marmoset, African elephant, Human, Potar monkey, Cat, Giraffe, Gorilla.</p> <p>Source: Rousseeuw, P. J., &amp; Leroy, A. M. (1987). Robust Regression and Outlier Detection. John Wiley &amp; Sons.</p> <p>Author: Nirmal Parmar</p>"},{"location":"datasets/anscombe_data/","title":"make_anscombe_check_data: Anscombe's Quartet Dataset","text":"<p>The <code>make_anscombe_check_data</code> function retrieves one of the four datasets from Anscombe's Quartet. These datasets are famous for being statistically identical (same mean, variance, correlation, and regression line) yet visually distinct, highlighting the importance of graphing data and using robust methods.</p>"},{"location":"datasets/anscombe_data/#overview","title":"Overview","text":"<p>Anscombe's quartet comprises four datasets that demonstrate the pitfalls of relying solely on simple descriptive statistics. They are ideal for benchmarking robust regression and gnostic models against standard least-squares approaches.</p> <ul> <li>Dataset 1: A simple linear relationship with Gaussian noise. This is the \"standard\" case where simple linear regression works well.</li> <li>Dataset 2: A clear non-linear (polynomial) relationship. Linear regression fails to capture the curve.</li> <li>Dataset 3: A tight linear relationship with a single severe outlier. The outlier pulls the standard regression line significantly.</li> <li>Dataset 4: A vertical line (constant X) with one influential outlier point. This point solely determines the correlation and regression slope.</li> </ul> <p>All four datasets share nearly identical:</p> <ul> <li>Mean of X: 9.0</li> <li>Mean of y: 7.50</li> <li>Sample Variance of X: 11.0</li> <li>Sample Variance of y: 4.12</li> <li>Correlation between X and y: 0.816</li> <li>Linear Regression Line: \\(y = 3.00 + 0.500x\\)</li> </ul>"},{"location":"datasets/anscombe_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>dataset_id</code> int The identifier of the dataset to retrieve (1, 2, 3, or 4). <code>1</code>"},{"location":"datasets/anscombe_data/#returns","title":"Returns","text":"Return Type Description <code>X</code> numpy.ndarray The input feature array of shape <code>(11,)</code>. <code>y</code> numpy.ndarray The target array of shape <code>(11,)</code>."},{"location":"datasets/anscombe_data/#raises","title":"Raises","text":"<ul> <li>ValueError     If <code>dataset_id</code> is not 1, 2, 3, or 4.</li> </ul>"},{"location":"datasets/anscombe_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_anscombe_check_data\nimport numpy as np\n\n# Load Dataset 3 (Linear with outlier)\nX, y = make_anscombe_check_data(dataset_id=3)\n\nprint(f\"Dataset 3 - X mean: {np.mean(X):.2f}, y mean: {np.mean(y):.2f}\")\n# Output: X mean: 9.00, y mean: 7.50\n</code></pre> <p>Source: Anscombe, F. J. (1973). \"Graphs in Statistical Analysis\". American Statistician. 27 (1): 17\u201321.</p> <p>Author: Nirmal Parmar</p>"},{"location":"datasets/cls_data/","title":"make_classification_check_data: Synthetic Classification Dataset","text":"<p>The <code>make_classification_check_data</code> function generates a synthetic dataset for validating classification models within the Machine Gnostics framework. This function creates simple blob-based clusters using Gaussian distributions, serving as a reliable \"hello world\" test for classification algorithms.</p>"},{"location":"datasets/cls_data/#overview","title":"Overview","text":"<p>This utility simplifies the creation of classification datasets by generating clusters of points centered around randomly positioned centroids.</p> <ul> <li>Method: Gaussian blobs with configurable separability.</li> <li>Purpose: Unit testing models, verifying pipeline integrity, and demonstrating basic classification capabilities.</li> <li>Customization: Easily adjust the number of samples, features, classes, and task difficulty (separability).</li> </ul>"},{"location":"datasets/cls_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>n_samples</code> int Total number of data points to generate. <code>30</code> <code>n_features</code> int Number of input features (dimensions) per sample. <code>2</code> <code>n_classes</code> int Number of distinct classes (labels). <code>2</code> <code>separability</code> float Distance multiplier for class centers. Higher values = easier separation. <code>2.0</code> <code>seed</code> int Random seed for reproducibility. <code>42</code>"},{"location":"datasets/cls_data/#returns","title":"Returns","text":"Return Type Description <code>X</code> numpy.ndarray Input feature array of shape <code>(n_samples, n_features)</code>. <code>y</code> numpy.ndarray Target label array of shape <code>(n_samples,)</code>."},{"location":"datasets/cls_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_classification_check_data\nimport numpy as np\n\n# Generate a 3-class dataset with 50 samples\nX, y = make_classification_check_data(n_samples=50, n_classes=3)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"Unique classes: {np.unique(y)}\")\n# Output:\n# X shape: (50, 2)\n# Unique classes: [0 1 2]\n</code></pre> <p>Author: Nirmal Parmar</p>"},{"location":"datasets/forbes_data/","title":"make_forbes_check_data: Forbes Dataset (1857)","text":"<p>The <code>make_forbes_check_data</code> function retrieves Forbes' historic dataset on boiling points measured in the Alps. This dataset is a standard benchmark in robust statistics, often used to demonstrate the impact of outliers on regression models.</p>"},{"location":"datasets/forbes_data/#overview","title":"Overview","text":"<p>Collected by James D. Forbes in the 19th century, this dataset contains 17 observations of the boiling point of water and barometric pressure at various altitudes in the Alps.</p> <ul> <li>Features (\\(X\\)): Boiling Point in degrees Fahrenheit (\\(^\\circ\\)F).</li> <li>Target (\\(y\\)): Barometric Pressure in inches of mercury (inHg).</li> <li>Significance: It is classically used to demonstrate robust regression because Observation 12 is widely considered an outlier (likely due to a transcription or measurement error) that skews standard least-squares regression.</li> <li>Typical Usage: Modeling the relationship: \\(100 \\times \\log_{10}(\\text{Pressure}) \\sim \\text{BoilingPoint}\\).</li> </ul>"},{"location":"datasets/forbes_data/#returns","title":"Returns","text":"Return Type Description <code>X</code> numpy.ndarray Boiling Point (\\(^\\circ\\)F). Shape <code>(17, 1)</code>. <code>y</code> numpy.ndarray Pressure (inHg). Shape <code>(17,)</code>."},{"location":"datasets/forbes_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_forbes_check_data\nimport numpy as np\n\n# Load the dataset\nX, y = make_forbes_check_data()\n\nprint(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n# Output: Dataset shape: X=(17, 1), y=(17,)\n\n# Inspect the known outlier (Index 11, Observation 12)\nprint(f\"Observation 12: Boiling Point={X[11,0]}, Pressure={y[11]}\")\n</code></pre> <p>Source: Forbes, J. D. (1857). \"Further experiments on the boiling point of water\". Transactions of the Royal Society of Edinburgh. 21 (2): 235\u2013243.</p> <p>Author: Nirmal Parmar</p>"},{"location":"datasets/linnerud_data/","title":"make_linnerud_check_data: Linnerud Multi-output Regression Dataset","text":"<p>The <code>make_linnerud_check_data</code> function loads or generates a Linnerud-like multi-output regression dataset. This dataset consists of 3 exercise data variables and 3 physiological variables collected from 20 middle-aged men in a fitness club. It is a classic dataset for multi-output regression tasks.</p>"},{"location":"datasets/linnerud_data/#overview","title":"Overview","text":"<p>This utility provides access to the Linnerud dataset:</p> <ul> <li>Features (\\(X\\)): Three exercise variables: <code>Chins</code>, <code>Situps</code>, and <code>Jumps</code>.</li> <li>Targets (\\(Y\\)): Three physiological variables: <code>Weight</code>, <code>Waist</code>, and <code>Pulse</code>.</li> <li>Purpose: Ideal for testing models that predict multiple targets simultaneously (multi-output regression).</li> <li>Source: Wraps Scikit-learn's <code>load_linnerud</code> if available; otherwise falls back to a similarly shaped synthetic generator.</li> </ul>"},{"location":"datasets/linnerud_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>return_names</code> bool If <code>True</code>, returns the list of column names for \\(X\\) and \\(Y\\). <code>True</code>"},{"location":"datasets/linnerud_data/#returns","title":"Returns","text":"Return Type Description <code>X</code> numpy.ndarray Exercise features of shape <code>(20, 3)</code>. Columns correspond to <code>Chins</code>, <code>Situps</code>, <code>Jumps</code>. <code>Y</code> numpy.ndarray Physiological targets of shape <code>(20, 3)</code>. Columns correspond to <code>Weight</code>, <code>Waist</code>, <code>Pulse</code>. <code>X_names</code> list[str] Names of the feature columns <code>['Chins', 'Situps', 'Jumps']</code>. Returned only if <code>return_names=True</code>. <code>Y_names</code> list[str] Names of the target columns <code>['Weight', 'Waist', 'Pulse']</code>. Returned only if <code>return_names=True</code>."},{"location":"datasets/linnerud_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_linnerud_check_data\n\n# Load data with column names\nX, Y, X_names, Y_names = make_linnerud_check_data(return_names=True)\n\nprint(f\"Features: {X_names}\")\n# Output: ['Chins', 'Situps', 'Jumps']\n\nprint(f\"Targets: {Y_names}\")\n# Output: ['Weight', 'Waist', 'Pulse']\n\nprint(f\"X Shape: {X.shape}, Y Shape: {Y.shape}\")\n# Output: X Shape: (20, 3), Y Shape: (20, 3)\n</code></pre>"},{"location":"datasets/longley_data/","title":"make_longley_check_data: Longley Economic Dataset","text":"<p>The <code>make_longley_check_data</code> function generates a Longley-like economic dataset. Ideally suited for testing numerical accuracy and stability in regression models, this dataset is famous for its high multicollinearity among predictors.</p>"},{"location":"datasets/longley_data/#overview","title":"Overview","text":"<p>This utility creates a synthetic version of the Longley economic dataset:</p> <ul> <li>Structure: 16 observations with 7 economic variables.</li> <li>Characteristics: High collinearity between predictors like <code>GNP</code>, <code>Population</code>, and <code>Year</code>.</li> <li>Purpose: Validating the numerical stability of regression algorithms (e.g., least squares) under conditions of ill-conditioning.</li> <li>Reproducibility: Uses a fixed seed (default 42).</li> </ul>"},{"location":"datasets/longley_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>seed</code> int Random seed for reproducibility. <code>42</code>"},{"location":"datasets/longley_data/#returns","title":"Returns","text":"Return Type Description <code>data</code> numpy.ndarray Array of shape <code>(16, 7)</code> containing economic indicators. <code>column_names</code> list[str] List of column names in order: <code>['GNP.deflator', 'GNP', 'Unemployed', 'Armed.Forces', 'Population', 'Year', 'Employed']</code>."},{"location":"datasets/longley_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_longley_check_data\n\n# Generate Longley data\ndata, cols = make_longley_check_data()\n\nprint(f\"Shape: {data.shape}\")\n# Output: (16, 7)\n\nprint(f\"Columns: {cols}\")\n# Output: ['GNP.deflator', 'GNP', 'Unemployed', 'Armed.Forces', 'Population', 'Year', 'Employed']\n</code></pre>"},{"location":"datasets/moon_data/","title":"make_moons_check_data: Synthetic Two Moons Dataset","text":"<p>The <code>make_moons_check_data</code> function generates a synthetic \"two moons\" dataset. This classic dataset consists of two interleaving half-circles and is widely used for visualizing and benchmarking clustering and classification algorithms, particularly for testing non-linear decision boundaries.</p>"},{"location":"datasets/moon_data/#overview","title":"Overview","text":"<p>The dataset forms two crescent shapes that are not linearly separable. </p> <ul> <li>Class 0 (Upper Moon): A half-circle arching upwards.</li> <li>Class 1 (Lower Moon): A half-circle arching downwards, shifted and interlocked with the upper moon.</li> <li>Purpose: Ideal for testing kernel methods, neural networks, or advanced clustering algorithms (like <code>GnosticLocalClustering</code>) that can handle non-convex shapes.</li> </ul>"},{"location":"datasets/moon_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>n_samples</code> int Total number of data points to generate. <code>30</code> <code>noise</code> float or None Standard deviation of Gaussian noise added to data. None = No noise. <code>None</code> <code>seed</code> int Random seed for reproducibility. <code>42</code>"},{"location":"datasets/moon_data/#returns","title":"Returns","text":"Return Type Description <code>X</code> numpy.ndarray Input feature array of shape <code>(n_samples, 2)</code>. <code>y</code> numpy.ndarray Target label array of shape <code>(n_samples,)</code>."},{"location":"datasets/moon_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_moons_check_data\nimport numpy as np\n\n# Generate noisy moon data\nX, y = make_moons_check_data(n_samples=100, noise=0.1)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"Unique classes: {np.unique(y)}\")\n# Output:\n# X shape: (100, 2)\n# Unique classes: [0 1]\n</code></pre> <p>Author: Nirmal Parmar</p>"},{"location":"datasets/mtcars_data/","title":"make_mtcars_check_data: MtCars Dataset","text":"<p>The <code>make_mtcars_check_data</code> function generates a synthetic version of the famous Motor Trend Car Road Tests (MtCars) dataset. This dataset is widely used for teaching and testing linear regression concepts, particularly for predicting fuel efficiency (<code>mpg</code>) based on vehicle characteristics.</p>"},{"location":"datasets/mtcars_data/#overview","title":"Overview","text":"<p>This utility provides a reproducible, MtCars-shaped dataset:</p> <ul> <li>Structure: 32 observations (cars) with 11 variables.</li> <li>Relationships: Realistic correlations, such as <code>mpg</code> decreasing as <code>hp</code> (horsepower) and <code>wt</code> (weight) increase.</li> <li>Purpose: Excellent for testing regression models, feature selection, and data visualization techniques.</li> <li>Reproducibility: Uses a fixed seed (default 42).</li> </ul>"},{"location":"datasets/mtcars_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>seed</code> int Random seed for reproducibility. <code>42</code>"},{"location":"datasets/mtcars_data/#returns","title":"Returns","text":"Return Type Description <code>data</code> numpy.ndarray Array of shape <code>(32, 11)</code> containing vehicle specifications. <code>column_names</code> list[str] List of column names: <code>['mpg', 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb']</code>. <code>car_names</code> list[str] List of placeholder car names (e.g., <code>'Car 1'</code>, <code>'Car 2'</code>) matching the rows."},{"location":"datasets/mtcars_data/#columns-description","title":"Columns Description","text":"Column Description <code>mpg</code> Miles/(US) gallon <code>cyl</code> Number of cylinders <code>disp</code> Displacement (cu.in.) <code>hp</code> Gross horsepower <code>drat</code> Rear axle ratio <code>wt</code> Weight (1000 lbs) <code>qsec</code> 1/4 mile time <code>vs</code> Engine (0 = V-shaped, 1 = straight) <code>am</code> Transmission (0 = automatic, 1 = manual) <code>gear</code> Number of forward gears <code>carb</code> Number of carburetors"},{"location":"datasets/mtcars_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_mtcars_check_data\n\n# Generate MtCars data\ndata, cols, names = make_mtcars_check_data()\n\nprint(f\"Shape: {data.shape}\")\n# Output: (32, 11)\n\nprint(f\"Key Features: {cols[3]} (HP), {cols[5]} (Weight)\")\n# Output: Key Features: hp (HP), wt (Weight)\n\n# Predict MPG (column 0)\nX = data[:, [3, 5]] # hp, wt\ny = data[:, 0]      # mpg\n</code></pre>"},{"location":"datasets/puromycin_data/","title":"make_puromycin_check_data: Puromycin Reaction Rate Dataset","text":"<p>The <code>make_puromycin_check_data</code> function generates a synthetic version of the Puromycin biochemical reaction dataset. This dataset is a classic benchmark for non-linear regression modeling, specifically for fitting Michaelis\u2013Menten kinetics to reaction rates in treated versus untreated cells.</p>"},{"location":"datasets/puromycin_data/#overview","title":"Overview","text":"<p>This utility provides a reproducible, Puromycin-like dataset:</p> <ul> <li>Structure: 23 observations with 3 variables.</li> <li>Groups: Data distinguishes between treated (<code>state=1</code>) and untreated (<code>state=0</code>) samples.</li> <li>Relationships: Follows Michaelis\u2013Menten kinetics (\\(Velocity = \\frac{V_{max} \\cdot Concentration}{K_m + Concentration}\\)) with distinct parameters for each group.</li> <li>Purpose: Ideal for testing non-linear regression models and analyzing group differences in curve fitting.</li> <li>Reproducibility: Uses a fixed seed (default 42).</li> </ul>"},{"location":"datasets/puromycin_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>seed</code> int Random seed for reproducibility. <code>42</code>"},{"location":"datasets/puromycin_data/#returns","title":"Returns","text":"Return Type Description <code>data</code> numpy.ndarray Array of shape <code>(23, 3)</code>. <code>column_names</code> list[str] List of column names: <code>['conc', 'rate', 'state']</code>."},{"location":"datasets/puromycin_data/#columns-description","title":"Columns Description","text":"Column Description <code>conc</code> Substrate concentration (ppm) <code>rate</code> Initial reaction velocity (counts/min/min) <code>state</code> Indicator of treatment: <code>0</code> = Untreated, <code>1</code> = Treated with Puromycin"},{"location":"datasets/puromycin_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_puromycin_check_data\n\n# Generate Puromycin data\ndata, cols = make_puromycin_check_data()\n\n# Separate groups\nuntreated = data[data[:, 2] == 0]\ntreated = data[data[:, 2] == 1]\n\nprint(f\"Untreated samples: {len(untreated)}\")\n# Output: Untreated samples: 12 (approx)\n\nprint(f\"Treated samples: {len(treated)}\")\n# Output: Treated samples: 11 (approx)\n</code></pre>"},{"location":"datasets/reg_data/","title":"make_regression_check_data: Synthetic Regression Dataset","text":"<p>The <code>make_regression_check_data</code> function generates synthetic regression datasets. It supports linear, polynomial, and sinusoidal relationships with additive Gaussian noise and optional outliers. This serves as a versatile testbed for validating regression models within the Machine Gnostics framework.</p>"},{"location":"datasets/reg_data/#overview","title":"Overview","text":"<p>This utility creates regression datasets for testing:</p> <ul> <li>Functions: Linear/Polynomial (\\(y = \\sum a_i x^i + c\\)), Sinusoidal (\\(y = A \\sin(x) + c\\)), or Cosine.</li> <li>Outliers: Option to contaminate a percentage of data with large shifts (<code>outlier_ratio</code>), ideal for testing robust regression capabilities.</li> <li>Purpose: Unit testing models, verifying pipeline integrity, and demonstrating regression capabilities.</li> <li>Reproducibility: Uses a fixed seed (default 42).</li> </ul>"},{"location":"datasets/reg_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>n_samples</code> int The number of data points to generate. <code>20</code> <code>slope</code> float True coefficient (linear/poly) or amplitude (sin/cos). <code>3.5</code> <code>intercept</code> float The true intercept (bias) of the underlying relationship. <code>10.0</code> <code>noise_level</code> float Standard deviation of Gaussian noise added to target. <code>2.0</code> <code>degree</code> int Degree of the polynomial. Used if <code>function_type='poly'</code>. <code>1</code> <code>function_type</code> str Component function type: <code>'poly'</code>, <code>'sin'</code>, <code>'cos'</code>. <code>'poly'</code> <code>outlier_ratio</code> float Proportion of samples to contaminate with outliers (0.0 to 1.0). <code>0.0</code> <code>seed</code> int Random seed for reproducibility. <code>42</code>"},{"location":"datasets/reg_data/#returns","title":"Returns","text":"Return Type Description <code>X</code> numpy.ndarray Input feature array of shape <code>(n_samples,)</code>. Values uniform in [0, 10]. <code>y</code> numpy.ndarray Target array of shape <code>(n_samples,)</code>."},{"location":"datasets/reg_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_regression_check_data\nimport numpy as np\n\n# Example 1: Standard Linear Regression\nX, y = make_regression_check_data(n_samples=50)\n\n# Example 2: Sinusoidal data with outliers\nX_sin, y_sin = make_regression_check_data(\n    n_samples=100, \n    function_type='sin', \n    outlier_ratio=0.1\n)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n</code></pre> <p>Author: Nirmal Parmar</p>"},{"location":"datasets/stackloss_data/","title":"make_stackloss_check_data: Stack Loss Dataset","text":"<p>The <code>make_stackloss_check_data</code> function retrieves the classic Stack Loss dataset (Brownlee, 1965). This dataset describes the operation of a plant for the oxidation of ammonia to nitric acid and is a standard benchmark for robust regression due to the presence of well-known outliers.</p>"},{"location":"datasets/stackloss_data/#overview","title":"Overview","text":"<p>The dataset consists of 21 operational days of a plant converting ammonia to nitric acid. The goal is typically to predict the <code>Stack Loss</code> (the amount of ammonia escaping unabsorbed) based on three operational variables.</p> <ul> <li>Significance: Ideally suited for demonstrating robust regression methods because it contains several acknowledged outliers (observations 1, 2, 3, and 21) that can distort standard least-squares models.</li> <li>Size: 21 samples, 4 variables.</li> </ul>"},{"location":"datasets/stackloss_data/#data-dictionary","title":"Data Dictionary","text":"<p>The dataset is returned as a single matrix with the following columns:</p> <ol> <li>Air Flow (Feature): Rate of operation of the plant.</li> <li>Water Temp. (Feature): Cooling water inlet temperature.</li> <li>Acid Conc. (Feature): Acid concentration (in per 1000 minus 500).</li> <li>Stack.Loss (Target): Amount of ammonia escaping the absorption column.</li> </ol>"},{"location":"datasets/stackloss_data/#returns","title":"Returns","text":"Return Type Description <code>data</code> numpy.ndarray The complete data array of shape <code>(21, 4)</code>. <code>column_names</code> list of str The list of columns: <code>['Air Flow', 'Water Temp.', 'Acid Conc.', 'Stack.Loss']</code>"},{"location":"datasets/stackloss_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_stackloss_check_data\nimport numpy as np\n\n# Load the dataset\ndata, names = make_stackloss_check_data()\n\nprint(f\"Data shape: {data.shape}\")\nprint(f\"Columns: {names}\")\n\n# Separate Features (X) and Target (y)\nX = data[:, :3]\ny = data[:, 3]\n</code></pre> <p>Source: Brownlee, K. A. (1965). Statistical Theory and Methodology in Science and Engineering. New York: John Wiley &amp; Sons.</p> <p>Author: Nirmal Parmar</p>"},{"location":"datasets/starwars_data/","title":"make_starwars_check_data: Star Wars Characters Dataset","text":"<p>The <code>make_starwars_check_data</code> function generates a synthetic Star Wars-like dataset containing demographics for 87 characters. Inspired by the <code>dplyr</code> dataset in R, this utility is perfect for practicing categorical analysis, grouping operations, and basic data exploration.</p>"},{"location":"datasets/starwars_data/#overview","title":"Overview","text":"<p>This utility creates a character dataset similar to the original R dataset:</p> <ul> <li>Structure: 87 observations (characters).</li> <li>Variables: Height, Mass, Species, and Character Names (synthetic).</li> <li>Characteristics: <ul> <li>Species distribution is skewed towards 'Human' (approx. 55%).</li> <li>Physical traits like height and mass are statistically distinct between species (e.g., Wookiees are taller/heavier, Hutts are very heavy).</li> </ul> </li> <li>Purpose: Ideal for data manipulation tasks (filtering, grouping), joining tables, and categorical visualization.</li> <li>Reproducibility: Uses a fixed seed (default 42).</li> </ul>"},{"location":"datasets/starwars_data/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>n</code> int Number of characters to generate. <code>87</code> <code>seed</code> int Random seed for reproducibility. <code>42</code>"},{"location":"datasets/starwars_data/#returns","title":"Returns","text":"Return Type Description <code>height_cm</code> numpy.ndarray Character heights in cm. Shape <code>(n,)</code>. <code>mass_kg</code> numpy.ndarray Character masses in kg. Shape <code>(n,)</code>. <code>species</code> list[str] Species label for each entry (e.g., <code>'Human'</code>, <code>'Wookiee'</code>, <code>'Droid'</code>). <code>names</code> list[str] List of placeholder character names (e.g., <code>'Character 1'</code>)."},{"location":"datasets/starwars_data/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.data import make_starwars_check_data\nimport pandas as pd\n\n# Generate character data\nh, m, s, names = make_starwars_check_data()\n\n# Create a DataFrame for easy viewing\ndf = pd.DataFrame({\n    'Name': names,\n    'Species': s,\n    'Height': h,\n    'Mass': m\n})\n\nprint(df.head())\n# Output (approx):\n#           Name Species      Height       Mass\n# 0  Character 1   Human  176.452312  81.231231\n# 1  Character 2  Droid   168.123123  84.512341\n# ...\n\n# Find the average mass of Humans\nhuman_mass = df[df['Species'] == 'Human']['Mass'].mean()\nprint(f\"Avg Human Mass: {human_mass:.2f} kg\")\n</code></pre>"},{"location":"magnet/magnet/","title":"Magnet: Machine Gnostic Neural Network","text":"<p>Magnet is a next-generation neural network architecture inspired by Mathematical Gnostics (MG). Unlike traditional neural networks that rely on probabilistic backpropagation, Magnet is built on a deterministic, finite, and algebraic foundation\u2014offering new possibilities for robust, interpretable learning.</p>"},{"location":"magnet/magnet/#what-makes-magnet-unique","title":"What Makes Magnet Unique?","text":"<ul> <li>Deterministic Learning: All computations are finite, reproducible, and free from randomness.</li> <li>Event-Level Modeling: Uncertainty and error are handled at the level of individual data events, not just populations.</li> <li>Algebraic Inference: Magnet leverages gnostic algebra and error geometry for transparent, explainable results.</li> <li>Resilient Architecture: Designed to withstand outliers, corrupted data, and distributional shifts.</li> </ul>"},{"location":"magnet/magnet/#roadmap-collaboration","title":"Roadmap &amp; Collaboration","text":"<p>Magnet is currently under active development. Coming soon: - Detailed documentation and architecture diagrams - Implementation guides and code examples - Benchmarks and comparison studies</p> <p>NOTE</p> <p>We welcome collaboration and new ideas! If you\u2019re interested in contributing, sharing feedback, or exploring partnerships, please reach out\u2014your insights can help shape the future of Machine Gnostic neural networks.</p> <p>Stay tuned for updates as we bring the next generation of neural networks to Machine Gnostics!</p> <p>Suggestions for future additions:</p> <ul> <li>Add a high-level diagram or conceptual illustration of Magnet\u2019s architecture.</li> <li>Include a \u201cVision\u201d or \u201cGoals\u201d section describing what Magnet aims to solve compared to existing neural networks.</li> <li>Provide a link or contact for collaboration (email, GitHub, etc.).</li> <li>List planned features or modules (e.g., layers, activation functions, training methods).</li> <li>Share any preliminary results or benchmarks if available.</li> </ul>"},{"location":"metrics/accuracy/","title":"accuracy_score: Classification Accuracy Metric","text":"<p>The <code>accuracy_score</code> function computes the accuracy of classification models by comparing predicted labels to true labels. It is a fundamental metric for evaluating the performance of classifiers in binary and multiclass settings.</p>"},{"location":"metrics/accuracy/#overview","title":"Overview","text":"<p>Accuracy is defined as the proportion of correct predictions among the total number of cases examined. It is a simple yet powerful metric for assessing how well a model is performing, especially when the classes are balanced.</p>"},{"location":"metrics/accuracy/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like or pandas Series Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series Estimated target values as returned by a classifier. Shape: (n_samples,) <code>verbose</code> bool Print detailed progress, warnings, and results <ul> <li>Both <code>y_true</code> and <code>y_pred</code> can be numpy arrays, lists, or pandas Series.</li> <li>If a pandas DataFrame is passed, a <code>ValueError</code> is raised (select a column instead).</li> </ul>"},{"location":"metrics/accuracy/#returns","title":"Returns","text":"<ul> <li>accuracy: <code>float</code>   The accuracy score as a float in the range [0, 1].</li> </ul>"},{"location":"metrics/accuracy/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> </ul>"},{"location":"metrics/accuracy/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import accuracy_score\n\n# Example 1: Using lists\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(accuracy_score(y_true, y_pred))  # Output: 0.8\n\n# Example 2: Using pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(accuracy_score(df['true'], df['pred']))\n</code></pre>"},{"location":"metrics/accuracy/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>The accuracy metric is most informative when the dataset is balanced. For imbalanced datasets, consider additional metrics such as precision, recall, or F1 score.</li> </ul>"},{"location":"metrics/classification_report/","title":"classification_report: Classification Metrics Summary","text":"<p>The <code>classification_report</code> function generates a comprehensive summary of key classification metrics\u2014precision, recall, F1 score, and support\u2014for each class in your dataset. It supports both string and dictionary output formats, making it suitable for both human-readable reports and programmatic analysis.</p>"},{"location":"metrics/classification_report/#overview","title":"Overview","text":"<p>This function provides a detailed breakdown of classifier performance for each class, including:</p> <ul> <li>Precision: Proportion of positive identifications that were actually correct.</li> <li>Recall: Proportion of actual positives that were correctly identified.</li> <li>F1 Score: Harmonic mean of precision and recall.</li> <li>Support: Number of true instances for each class.</li> </ul> <p>It also computes weighted averages across all classes.</p>"},{"location":"metrics/classification_report/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>labels</code> array-like or None None List of labels to include in the report. If None, uses sorted unique labels from y_true and y_pred. <code>target_names</code> list of str or None None Optional display names matching the labels (same order). <code>digits</code> int 2 Number of digits for formatting output. <code>output_dict</code> bool False If True, return output as a dict. If False, return as a formatted string. <code>zero_division</code> {0, 1, 'warn'} 0 Value to return when there is a zero division (no predicted samples for a class). <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/classification_report/#returns","title":"Returns","text":"<ul> <li>report: <code>str</code> or <code>dict</code>   Text summary or dictionary of the precision, recall, F1 score, and support for each class.</li> </ul>"},{"location":"metrics/classification_report/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import classification_report\n\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\n\n# String report\nprint(classification_report(y_true, y_pred))\n\n# Dictionary report\nreport_dict = classification_report(y_true, y_pred, output_dict=True)\nprint(report_dict)\n</code></pre>"},{"location":"metrics/classification_report/#output-example","title":"Output Example","text":"<p>String Output:</p> <pre><code>Class             Precision    Recall   F1-score    Support\n==========================================================\n0                    1.00      0.50      0.67          2\n1                    0.00      0.00      0.00          1\n2                    1.00      1.00      1.00          2\n==========================================================\nAvg/Total            0.80      0.60      0.67          5\n</code></pre> <p>Dictionary Output:</p> <pre><code>{\n  '0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.67, 'support': 2},\n  '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n  '2': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2},\n  'avg/total': {'precision': 0.8, 'recall': 0.6, 'f1-score': 0.67, 'support': 5}\n}\n</code></pre>"},{"location":"metrics/classification_report/#notes","title":"Notes","text":"<ul> <li>The function uses <code>precision_score</code>, <code>recall_score</code>, and <code>f1_score</code> from the Machine Gnostics metrics module for consistency.</li> <li>If <code>target_names</code> is provided, its length must match the number of labels.</li> <li>For imbalanced datasets, the weighted average provides a more informative summary than the unweighted mean.</li> <li>The <code>zero_division</code> parameter controls the behavior when a class has no predicted samples.</li> </ul>"},{"location":"metrics/confusion_matrix/","title":"confusion_matrix: Confusion Matrix Metric","text":"<p>The <code>confusion_matrix</code> function computes the confusion matrix for classification tasks. This metric provides a summary of prediction results, showing how many samples were correctly or incorrectly classified for each class.</p>"},{"location":"metrics/confusion_matrix/#overview","title":"Overview","text":"<p>A confusion matrix is a table that is often used to describe the performance of a classification model. - Each row represents the actual class. - Each column represents the predicted class. - Entry (i, j) is the number of samples with true label i and predicted label j.</p> <p>This metric helps you understand the types of errors your classifier is making and is essential for evaluating classification accuracy, precision, recall, and other related metrics.</p>"},{"location":"metrics/confusion_matrix/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>y_true</code> array-like or pandas Series Ground truth (correct) target values. Shape: (n_samples,) Required <code>y_pred</code> array-like or pandas Series Estimated targets as returned by a classifier. Shape: (n_samples,) Required <code>labels</code> array-like List of labels to index the matrix. If None, uses all labels in sorted order. <code>None</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/confusion_matrix/#returns","title":"Returns","text":"<ul> <li>ndarray   Confusion matrix of shape (n_classes, n_classes). Entry (i, j) is the number of samples with true label i and predicted label j.</li> </ul>"},{"location":"metrics/confusion_matrix/#raises","title":"Raises","text":"<ul> <li>ValueError   If input arrays are not the same shape, are empty, are not 1D, or contain NaN/Inf.</li> </ul>"},{"location":"metrics/confusion_matrix/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import confusion_matrix\n\n# Example 1: Basic usage\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\n# Output:\n# array([[2, 0, 0],\n#        [0, 0, 1],\n#        [1, 0, 2]])\n\n# Example 2: With custom labels\ncm_custom = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\nprint(cm_custom)\n</code></pre>"},{"location":"metrics/confusion_matrix/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, numpy arrays, or pandas Series.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must be 1D, have the same shape, and must not be empty or contain NaN/Inf.</li> <li>If <code>labels</code> is not provided, all unique labels in <code>y_true</code> and <code>y_pred</code> are used in sorted order.</li> <li>The confusion matrix is essential for computing other metrics such as accuracy, precision, recall, and F1-score.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/divI/","title":"divI: Divergence Information (DivI) Metric","text":"<p>The <code>divI</code> function computes the Divergence Information (DivI) metric, a robust measure for evaluating the divergence between observed data and model predictions. This metric is based on gnostic characteristics and is particularly useful for assessing the quality of model fits, especially in the presence of noise or outliers.</p>"},{"location":"metrics/divI/#overview","title":"Overview","text":"<p>Divergence Information (DivI) quantifies how much the information content of the predicted values diverges from that of the true values. Unlike classical divergence measures, DivI leverages gnostic algebra, making it robust to irregularities and non-Gaussian data.</p> <p>Mathematically, DivI is defined as:</p> \\[ \\text{DivI} = \\frac{1}{N} \\sum_{i=1}^N \\frac{I(y_i)}{I(\\hat{y}_i)} \\] <p>where:</p> <ul> <li>\\(I(y_i)\\) is the E-information of the observed value \\(y_i\\),</li> <li>\\(I(\\hat{y}_i)\\) is the E-information of the fitted value \\(\\hat{y}_i\\),</li> <li>\\(N\\) is the number of data points.</li> </ul> <p>DivI compares the information content of the dependent variable and its fit. The better the fit, the closer DivI is to 1. If the fit is highly uncertain or poor, DivI decreases.</p>"},{"location":"metrics/divI/#interpretation","title":"Interpretation","text":"<ul> <li>Higher DivI: Indicates that the fitted values retain more of the information content of the observed data, suggesting a better model fit.</li> <li>Lower DivI: Indicates greater divergence between the distributions of the observed and fitted values, suggesting a poorer fit or higher uncertainty in the model.</li> </ul> <p>DivI is particularly useful in robust model evaluation, as it is less sensitive to outliers and non-normal data distributions.</p>"},{"location":"metrics/divI/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/divI/#returns","title":"Returns","text":"<ul> <li>float   The computed Divergence Information (DivI) value.</li> </ul>"},{"location":"metrics/divI/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/divI/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import divI\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = divI(y, y_fit)\nprint(result)\n</code></pre>"},{"location":"metrics/divI/#notes","title":"Notes","text":"<ul> <li>DivI is calculated using gnostic characteristics, providing a robust way to measure divergence between distributions.</li> <li>The metric is especially useful for model evaluation in real-world scenarios where data may be noisy or contain outliers.</li> <li>In the context of model evaluation, DivI is often used alongside other criteria such as Robust R-squared (RobR2) and the Geometric Mean of Multiplicative Fitting Errors (GMMFE) to provide a comprehensive assessment of model performance.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/entropy/","title":"entropy: Gnostic Entropy Metric","text":"<p>The <code>entropy</code> function computes the Gnostic entropy of a data distribution or the entropy of the difference between two data samples. This metric evaluates uncertainty or disorder within the framework of Mathematical Gnostics, providing robust, assumption-free measurements.</p>"},{"location":"metrics/entropy/#overview","title":"Overview","text":"<p>Gnostic entropy is a measure of the uncertainty associated with a dataset. It leverages the estimating (EGDF) or quantifying (QGDF) global distribution functions to determine the level of disorder:</p> <ul> <li>Single Dataset: Calculates the entropy of the distribution of <code>data</code>.</li> <li>Two Datasets: Calculates the entropy of the residuals (<code>data_compare - data</code>), useful for evaluating model errors.</li> </ul> <p>The calculation depends on the selected geometry:</p> <ul> <li>Case <code>'i'</code> (Estimation): <code>Entropy = 1 - mean(fi)</code>. Represents standard uncertainty, typically in [0, 1].</li> <li>Case <code>'j'</code> (Quantification): <code>Entropy = mean(fj) - 1</code>. Used for quantifying outliers or extreme deviations.</li> </ul>"},{"location":"metrics/entropy/#parameters","title":"Parameters","text":"Parameter Type Description <code>data</code> array-like Reference data values (e.g., Ground Truth) or single dataset. Must be 1D. <code>data_compare</code> array-like, optional Data to compare (e.g., Predicted). Comparison is <code>data_compare - data</code>. <code>S</code> float or 'auto' Scale parameter. If float, suggested [0.01, 2]. Default: <code>'auto'</code>. <code>case</code> str <code>'i'</code> for estimating (EGDF), <code>'j'</code> for quantifying (QGDF). Default: <code>'i'</code>. <code>z0_optimize</code> bool Whether to optimize the location parameter z0. Default: <code>False</code>. <code>data_form</code> str <code>'a'</code> for additive (diff), <code>'m'</code> for multiplicative. Default: <code>'a'</code>. <code>tolerance</code> float Convergence tolerance for optimization. Default: <code>1e-6</code>. <code>verbose</code> bool If True, enables detailed logging. Default: <code>False</code>."},{"location":"metrics/entropy/#returns","title":"Returns","text":"<ul> <li>float   The calculated Gnostic entropy value.</li> </ul>"},{"location":"metrics/entropy/#raises","title":"Raises","text":"<ul> <li>TypeError   If inputs are not array-like or have incorrect types.</li> <li>ValueError   If inputs have mismatched shapes, are empty, contain NaN/Inf, or if invalid options are provided.</li> </ul>"},{"location":"metrics/entropy/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import entropy\n\n# Example 1: Entropy of a single dataset\ndata = np.random.normal(0, 1, 100)\nent = entropy(data, case='i')\nprint(f\"Entropy (single): {ent}\")\n\n# Example 2: Entropy of residuals (Model Evaluation)\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])\nent_diff = entropy(data=y_true, data_compare=y_pred, case='i')\nprint(f\"Entropy (residuals): {ent_diff}\")\n\n# Example 3: detecting outliers with case 'j'\ny_outliers = np.array([1, 2, 3, 100])\nent_out = entropy(y_outliers, case='j')\nprint(f\"Entropy (quantifying): {ent_out}\")\n</code></pre>"},{"location":"metrics/entropy/#notes","title":"Notes","text":"<ul> <li>For standard uncertainty estimation, use case 'i'. The values are typically normalized between 0 (certainty) and 1 (max uncertainty).</li> <li>For analyzing tails and outliers, use case 'j'.</li> <li>If <code>S='auto'</code>, the scale parameter is estimated automatically based on data homogeneity.</li> </ul> <p>Author: Nirmal Parmar Date: 2026-02-02</p>"},{"location":"metrics/evalmet/","title":"evalMet: Composite Evaluation Metric","text":"<p>The <code>evalMet</code> function computes the Evaluation Metric (EvalMet), a composite score that combines three robust criteria\u2014Robust R-squared (RobR2), Geometric Mean of Model Fit Error (GMMFE), and Divergence Information (DivI)\u2014to provide a comprehensive assessment of model performance.</p>"},{"location":"metrics/evalmet/#overview","title":"Overview","text":"<p>EvalMet is designed to quantify the overall quality of a model fit by integrating three complementary metrics:</p> <ul> <li>RobR2: Measures the proportion of variance explained by the model, robust to outliers.</li> <li>GMMFE: Captures the average multiplicative fitting error on a logarithmic scale.</li> <li>DivI: Quantifies the divergence in information content between the observed data and the model fit.</li> </ul> <p>The combined metric is calculated as:</p> \\[ \\text{EvalMet} = \\frac{\\text{RobR2}}{\\text{GMMFE} \\cdot \\text{DivI}} \\] <p>A higher EvalMet value indicates a better model fit, balancing explained variance, error magnitude, and information divergence.</p>"},{"location":"metrics/evalmet/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y</code> np.ndarray \u2014 Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray \u2014 Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>w</code> np.ndarray None Optional weights for data points. 1D array, same shape as <code>y</code>. If not provided, equal weights are used. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/evalmet/#returns","title":"Returns","text":"<ul> <li>float   The computed Evaluation Metric (EvalMet) value.</li> </ul>"},{"location":"metrics/evalmet/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>w</code> is provided and does not have the same shape as <code>y</code>.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/evalmet/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import evalMet\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = evalMet(y, y_fit)\nprint(result)\n</code></pre>"},{"location":"metrics/evalmet/#notes","title":"Notes","text":"<ul> <li>EvalMet is most informative when used to compare multiple models or methods on the same dataset.</li> <li>The metric is robust to outliers and non-Gaussian data due to its use of gnostic algebra.</li> <li>EvalMet is especially useful in benchmarking and model selection scenarios, as it integrates multiple aspects of fit quality into a single score.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/f1_score/","title":"f1_score: Classification F1 Score Metric","text":"<p>The <code>f1_score</code> function computes the F1 score for classification models, supporting both binary and multiclass settings. The F1 score is the harmonic mean of precision and recall, providing a balanced measure that is especially useful when classes are imbalanced.</p>"},{"location":"metrics/f1_score/#overview","title":"Overview","text":"<p>The F1 score combines precision and recall into a single metric by taking their harmonic mean.</p> <p>This metric is particularly important when you want to balance the trade-off between precision and recall, such as in information retrieval, medical diagnosis, and fraud detection.</p>"},{"location":"metrics/f1_score/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred. <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/f1_score/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the F1 score for each class as an array.</li> </ul>"},{"location":"metrics/f1_score/#returns","title":"Returns","text":"<ul> <li>f1: <code>float</code> or <code>array of floats</code>   F1 score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of F1 values for each class.</li> </ul>"},{"location":"metrics/f1_score/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"metrics/f1_score/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import f1_score\n\n# Example 1: Macro-averaged F1 for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(f1_score(y_true, y_pred, average='macro'))\n\n# Example 2: Binary F1 with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(f1_score(df['true'], df['pred'], average='binary'))\n</code></pre>"},{"location":"metrics/f1_score/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"metrics/g_auto_corelation/","title":"auto_correlation: Gnostic Auto-Correlation Metric","text":"<p>The <code>auto_correlation</code> function computes the Gnostic auto-correlation coefficient for a data sample. This metric measures the similarity between a data sample and a lagged version of itself, using robust gnostic theory principles for reliable diagnostics\u2014even in the presence of noise or outliers.</p>"},{"location":"metrics/g_auto_corelation/#overview","title":"Overview","text":"<p>Auto-correlation quantifies how much a data sample resembles itself when shifted by a specified lag. Unlike classical auto-correlation, the Gnostic version uses irrelevance measures from gnostic theory, providing robust, assumption-free estimates that reflect the true structure of your data.</p>"},{"location":"metrics/g_auto_corelation/#parameters","title":"Parameters","text":"Parameter Type Description <code>data</code> np.ndarray Data sample (1D numpy array, no NaN/Inf). <code>lag</code> int Lag value (non-negative, less than length of data). Default:<code>0</code>. <code>case</code> str Geometry type:<code>'i'</code> for estimation (EGDF), <code>'j'</code> for quantifying (QGDF). Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default:<code>False</code>."},{"location":"metrics/g_auto_corelation/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic auto-correlation coefficient for the given lag.</li> </ul>"},{"location":"metrics/g_auto_corelation/#raises","title":"Raises","text":"<ul> <li>ValueError   If input is not a 1D numpy array, is empty, contains NaN/Inf, or if lag/case is invalid.</li> </ul>"},{"location":"metrics/g_auto_corelation/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import auto_correlation\n\n# Example 1: Compute auto-correlation for a simple dataset\ndata = np.array([1, 2, 3, 4, 5])\nlag = 1\nauto_corr = auto_correlation(data, lag=lag, case='i', verbose=False)\nprint(f\"Auto-Correlation (lag={lag}, case='i'): {auto_corr}\")\n\n# Example 2: Using quantifying geometry\nauto_corr_j = auto_correlation(data, lag=2, case='j', verbose=True)\nprint(f\"Auto-Correlation (lag=2, case='j'): {auto_corr_j}\")\n</code></pre>"},{"location":"metrics/g_auto_corelation/#notes","title":"Notes","text":"<ul> <li>The metric is robust to data uncertainty, noise, and outliers.</li> <li>Input data must be preprocessed and cleaned for optimal results.</li> <li>If data homogeneity is not met, the function automatically adjusts scale parameters for better reliability.</li> <li>The Gnostic auto-correlation uses irrelevance measures rather than classical means, providing deeper insight into temporal relationships in your data.</li> <li>Supports both estimation and quantification geometries for flexible analysis.</li> </ul>"},{"location":"metrics/g_auto_corelation/#gnostic-vs-classical-auto-correlation","title":"Gnostic vs. Classical Auto-Correlation","text":"<p>Note: Unlike classical auto-correlation metrics that rely on statistical means, the Gnostic auto-correlation uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true temporal relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/g_auto_covariance/","title":"auto_covariance: Gnostic Auto-Covariance Metric","text":"<p>The <code>auto_covariance</code> function computes the Gnostic auto-covariance coefficient for a data sample. This metric measures the relationship between a data sample and a lagged version of itself, using robust gnostic theory principles for reliable diagnostics\u2014even in the presence of noise or outliers.</p>"},{"location":"metrics/g_auto_covariance/#overview","title":"Overview","text":"<p>Auto-covariance quantifies how much a data sample co-varies with itself when shifted by a specified lag. Unlike classical auto-covariance, the Gnostic version uses irrelevance measures from gnostic theory, providing robust, assumption-free estimates that reflect the true structure of your data.</p>"},{"location":"metrics/g_auto_covariance/#parameters","title":"Parameters","text":"Parameter Type Description <code>data</code> np.ndarray Data sample (1D numpy array, no NaN/Inf). Represents a time series or sequential data. <code>lag</code> int Lag value (non-negative, less than length of data). Default:<code>0</code>. <code>case</code> str Geometry type:<code>'i'</code> for estimation (EGDF), <code>'j'</code> for quantifying (QGDF). Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default:<code>False</code>."},{"location":"metrics/g_auto_covariance/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic auto-covariance coefficient for the given lag. If the computed value is less than 1e-6, it is set to 0.0.</li> </ul>"},{"location":"metrics/g_auto_covariance/#raises","title":"Raises","text":"<ul> <li>ValueError   If input is not a 1D numpy array, is empty, contains NaN/Inf, or if lag/case is invalid.</li> </ul>"},{"location":"metrics/g_auto_covariance/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import auto_covariance\n\n# Example 1: Compute auto-covariance for a simple dataset\ndata = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\nlag = 1\nauto_covar = auto_covariance(data, lag=lag, case='i')\nprint(f\"Auto-covariance with lag={lag}: {auto_covar}\")\n\n# Example 2: Compute auto-covariance with QGDF\nauto_covar_j = auto_covariance(data, lag=2, case='j')\nprint(f\"Auto-covariance with lag=2: {auto_covar_j}\")\n\n# Example 3: Handle invalid input\ndata_invalid = np.array([1.0, np.nan, 3.0, 4.0, 5.0])\ntry:\n    auto_covar = auto_covariance(data_invalid, lag=1, case='i')\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"metrics/g_auto_covariance/#notes","title":"Notes","text":"<ul> <li>The function uses Gnostic theory to compute irrelevance values for the data and its lagged version.</li> <li>Irrelevance values are clipped to avoid overflow, with a maximum value of 1e12.</li> <li>Homogeneity checks are performed on the data and its lagged version. If the data is not homogeneous, warnings are raised and scale parameters are adjusted.</li> <li>The metric is robust to data uncertainty, noise, and outliers.</li> <li>Input data must be preprocessed and cleaned for optimal results.</li> </ul>"},{"location":"metrics/g_auto_covariance/#gnostic-vs-classical-auto-covariance","title":"Gnostic vs. Classical Auto-Covariance","text":"<p>Note: Unlike classical auto-covariance metrics that rely on statistical means, the Gnostic auto-covariance uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true temporal relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_correlation/","title":"correlation: Gnostic Correlation Metric","text":"<p>The <code>correlation</code> function computes the Gnostic correlation coefficient between two data samples. This metric provides a robust, assumption-free measure of association, leveraging Mathematical Gnostics to deliver reliable results even in the presence of noise, outliers, or non-Gaussian data.</p>"},{"location":"metrics/g_correlation/#overview","title":"Overview","text":"<p>The Gnostic correlation metric measures the association between two arrays using either estimation geometry (EGDF) or quantifying geometry (QGDF):</p> <ul> <li>Case <code>'i'</code>: Uses estimation geometry (EGDF) for correlation.</li> <li>Case <code>'j'</code>: Uses quantifying geometry (QGDF) for correlation.</li> </ul> <p>Unlike classical correlation coefficients, the Gnostic approach is resilient to data uncertainty and is meaningful for both small and large datasets.</p>"},{"location":"metrics/g_correlation/#parameters","title":"Parameters","text":"Parameter Type Description <code>X</code> array-like Feature data sample (1D array or single column from 2D array, no NaN/Inf). <code>y</code> array-like Target data sample (1D array, no NaN/Inf). <code>case</code> str <code>'i'</code> for estimation geometry, <code>'j'</code> for quantifying geometry. Default: <code>'i'</code>. <code>S</code> str Gnostic scale parameter. If <code>'auto'</code>, determines best scale based on homogeneity. Default: <code>'auto'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default: <code>False</code>."},{"location":"metrics/g_correlation/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic correlation coefficient between the two data samples.</li> </ul>"},{"location":"metrics/g_correlation/#raises","title":"Raises","text":"<ul> <li>TypeError   If <code>X</code> or <code>y</code> are not array-like.</li> <li>ValueError   If input arrays are empty, contain NaN/Inf, are not 1D, have mismatched shapes, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"metrics/g_correlation/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import correlation\nimport numpy as np\n\n# Example 1: Simple 1D arrays\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([5, 4, 3, 2, 1])\ncorr = correlation(X, y, case='i')\nprint(corr)\n\n# Example 2: Multi-column X\nX = np.array([[1, 10], [2, 20], [3, 30], [4, 40], [5, 50]])\ny = np.array([5, 4, 3, 2, 1])\nfor i in range(X.shape[1]):\n    corr = correlation(X[:, i], y)\n    print(f\"Correlation for column {i}: {corr}\")\n</code></pre>"},{"location":"metrics/g_correlation/#notes","title":"Notes","text":"<ul> <li>Both <code>X</code> and <code>y</code> must be 1D arrays of the same length, with no missing or infinite values.</li> <li>For multi-column <code>X</code>, pass each column separately (e.g., <code>X[:, i]</code>).</li> <li>The metric is robust to outliers and provides meaningful estimates even for small or noisy datasets.</li> <li>If data homogeneity is not met, the function will adjust parameters and issue a warning for best results.</li> <li>For optimal results, ensure your data is preprocessed and cleaned.</li> </ul>"},{"location":"metrics/g_correlation/#gnostic-vs-classical-correlation","title":"Gnostic vs. Classical Correlation","text":"<p>Note: Unlike classical correlation metrics that rely on statistical means and linear relationships, the Gnostic correlation uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true data relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_cross_variance/","title":"cross_covariance: Gnostic Cross-Covariance Metric","text":"<p>The <code>cross_covariance</code> function computes the Gnostic cross-covariance between two data samples. This metric uses gnostic theory to provide robust, assumption-free estimates of relationships between datasets, even in the presence of noise or outliers.</p>"},{"location":"metrics/g_cross_variance/#overview","title":"Overview","text":"<p>Gnostic cross-covariance generalizes classical covariance by leveraging irrelevance measures from gnostic theory:</p> <ul> <li>Estimating irrelevances are aggregated as trigonometric sines (case <code>'i'</code>).</li> <li>Quantifying irrelevances are aggregated as hyperbolic sines (case <code>'j'</code>).</li> </ul> <p>Both approaches converge to linear error in cases of weak uncertainty, but provide robust diagnostics in challenging data scenarios. The metric is computed as the mean product of irrelevances between two data samples.</p>"},{"location":"metrics/g_cross_variance/#parameters","title":"Parameters","text":"Parameter Type Description <code>X</code> np.ndarray Feature data sample (1D numpy array, no NaN/Inf). For multi-column X, pass each column separately. <code>y</code> np.ndarray Target data sample (1D numpy array, no NaN/Inf). <code>case</code> str Geometry type: <code>'i'</code> for estimation (EGDF), <code>'j'</code> for quantifying (QGDF). Default: <code>'i'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default: <code>False</code>."},{"location":"metrics/g_cross_variance/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic cross-covariance between the two data samples.</li> </ul>"},{"location":"metrics/g_cross_variance/#raises","title":"Raises","text":"<ul> <li>ValueError   If input arrays are not the same length, are empty, contain NaN/Inf, are not 1D, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"metrics/g_cross_variance/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import cross_covariance\n\n# Example 1: Compute cross-covariance for two simple datasets\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([5, 4, 3, 2, 1])\ncovar = cross_covariance(X, y, case='i', verbose=False)\nprint(f\"Cross-Covariance (case='i'): {covar}\")\n\n# Example 2: For multi-column X\nX = np.array([[1, 10], [2, 20], [3, 30], [4, 40], [5, 50]])\ny = np.array([5, 4, 3, 2, 1])\nfor i in range(X.shape[1]):\n  covar = cross_covariance(X[:, i], y)\n  print(f\"Cross-Covariance for column {i}: {covar}\")\n\n# Example 3: Handle invalid input\nX_invalid = np.array([1, np.nan, 3, 4, 5])\ntry:\n  covar = cross_covariance(X_invalid, y, case='i')\nexcept ValueError as e:\n  print(f\"Error: {e}\")\n</code></pre>"},{"location":"metrics/g_cross_variance/#notes","title":"Notes","text":"<ul> <li><code>X</code> must be a 1D numpy array (single column). For multi-column X, pass each column separately (e.g., <code>X[:, i]</code>).</li> <li><code>y</code> must be a 1D numpy array.</li> <li>Both arrays must be of the same length, with no NaN or Inf values.</li> <li>The metric is robust to data uncertainty and provides meaningful estimates even in the presence of noise or outliers.</li> <li>Ensure that the input data is preprocessed and cleaned for optimal results.</li> <li>If data homogeneity is not met, the function automatically adjusts scale parameters for better reliability.</li> <li>The Gnostic cross-covariance uses irrelevance measures rather than classical means, providing deeper insight into relationships between datasets.</li> <li>Supports both estimation and quantification geometries for flexible analysis.</li> </ul>"},{"location":"metrics/g_cross_variance/#gnostic-vs-classical-covariance","title":"Gnostic vs. Classical Covariance","text":"<p>Note: Unlike classical covariance metrics that rely on statistical means, the Gnostic cross-covariance uses irrelevance measures derived from gnostic theory. This approach is assumption-free and designed to reveal true relationships, even in the presence of outliers or non-normal distributions.</p> <p>Author: Nirmal Parmar    Date: 2025-09-24</p>"},{"location":"metrics/g_mean/","title":"mean: Gnostic Mean Metric","text":"<p>The <code>mean</code> function computes the Gnostic mean (Local Estimate of Location) of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of central tendency, leveraging irrelevance and fidelity measures for deeper insight into data structure and uncertainty.</p>"},{"location":"metrics/g_mean/#overview","title":"Overview","text":"<p>Gnostic mean generalizes classical mean by using irrelevance and fidelity measures:</p> <ul> <li>Case <code>'i'</code>: Estimates mean using ELDF (Empirical Likelihood Distribution Function).</li> <li>Case <code>'j'</code>: Quantifies mean using QLDF (Quantile Likelihood Distribution Function).</li> </ul> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios.</p>"},{"location":"metrics/g_mean/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>S</code> float/str Scaling parameter (<code>float</code> or <code>'auto'</code>). Suggested: [0.01, 2]. <code>'auto'</code> <code>case</code> str <code>'i'</code> for estimating mean (ELDF), <code>'j'</code> for quantifying mean (QLDF). <code>'i'</code> <code>z0_optimize</code> bool Whether to optimize z0. <code>True</code> <code>data_form</code> str Data form: <code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for ELDF/QLDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_mean/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic mean of the data.</li> </ul>"},{"location":"metrics/g_mean/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf input is not a numpy array, or if <code>S</code> is not a float or <code>'auto'</code>.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code>/<code>data_form</code> is invalid.</li> </ul>"},{"location":"metrics/g_mean/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic mean (default case)\ndata = np.array([1, 2, 3, 4, 5])\nmean_value = mg.mean(data)\nprint(mean_value)\n\n# Example 2: Quantifying mean with QLDF\nmean_j = mg.mean(data, case='j')\nprint(mean_j)\n</code></pre>"},{"location":"metrics/g_mean/#notes","title":"Notes","text":"<ul> <li>The function uses ELDF or QLDF to compute irrelevance and fidelity values, which are then used to estimate the mean.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical mean.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> </ul>"},{"location":"metrics/g_mean/#gnostic-vs-classical-mean","title":"Gnostic vs. Classical Mean","text":"<p>Note: Unlike classical mean metrics that use statistical averages, the Gnostic mean is computed using irrelevance and fidelity measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Authors: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_median/","title":"median: Gnostic Median Metric","text":"<p>The <code>median</code> function computes the Gnostic median (Global Estimate of Location) of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of central tendency, leveraging irrelevance and fidelity measures for deeper insight into data structure and uncertainty.</p>"},{"location":"metrics/g_median/#overview","title":"Overview","text":"<p>Gnostic median generalizes classical median by using irrelevance and fidelity measures:</p> <ul> <li>Case <code>'i'</code>: Estimates median using EGDF (Empirical Gnostics Distribution Function).</li> <li>Case <code>'j'</code>: Quantifies median using QGDF (Quantile Gnostics Distribution Function).</li> </ul> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios.</p>"},{"location":"metrics/g_median/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>case</code> str <code>'i'</code> for estimating median (EGDF), <code>'j'</code> for quantifying median (QGDF). <code>'i'</code> <code>S</code> float/str Scaling parameter for EGDF/QGDF. Can be <code>float</code> or <code>'auto'</code>. <code>'auto'</code> <code>z0_optimize</code> bool Whether to optimize z0 in EGDF/QGDF. <code>True</code> <code>data_form</code> str Data form for EGDF/QGDF: <code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for EGDF/QGDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_median/#returns","title":"Returns","text":"<ul> <li>float   The Gnostic median of the data.</li> </ul>"},{"location":"metrics/g_median/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf input is not a numpy array, or if <code>S</code> is not a float or <code>'auto'</code>.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code>/<code>data_form</code> is invalid.</li> </ul>"},{"location":"metrics/g_median/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic median (default case)\ndata = np.array([1, 2, 3, 4, 5])\nmedian_value = mg.median(data)\nprint(median_value)\n\n# Example 2: Quantifying median with QGDF\nmedian_j = mg.median(data, case='j')\nprint(median_j)\n</code></pre>"},{"location":"metrics/g_median/#notes","title":"Notes","text":"<ul> <li>The function uses EGDF or QGDF to compute irrelevance and fidelity values, which are then used to estimate the median.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical median.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> </ul>"},{"location":"metrics/g_median/#gnostic-vs-classical-median","title":"Gnostic vs. Classical Median","text":"<p>Note: Unlike classical median metrics that use statistical order statistics, the Gnostic median is computed using irrelevance and fidelity measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/g_relevance/","title":"hc: Gnostic Characteristics (Hc) Metric","text":"<p>The <code>hc</code> function computes the Gnostic Characteristics (Hc) metric for a set of true and predicted values. This metric evaluates the relevance or irrelevance of predictions using gnostic theory, providing robust, assumption-free diagnostics for model performance.</p>"},{"location":"metrics/g_relevance/#overview","title":"Overview","text":"<p>The Hc metric measures the gnostic relevance or irrelevance between true and predicted values:</p> <ul> <li>Case <code>'i'</code>: Estimates gnostic relevance. Values close to one indicate less relevance. Range: [0, 1].</li> <li>Case <code>'j'</code>: Estimates gnostic irrelevance. Values close to 1 indicate less irrelevance. Range: [0, \u221e).</li> </ul> <p>Unlike classical metrics, Hc uses gnostic algebra to provide deeper insight into the relationship between predictions and actual outcomes, especially in the presence of outliers or non-normal data.</p>"},{"location":"metrics/g_relevance/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (list, tuple, or numpy array). <code>y_pred</code> array-like Predicted values (list, tuple, or numpy array). <code>case</code> str <code>'i'</code> for relevance, <code>'j'</code> for irrelevance. Default: <code>'i'</code>. <code>S</code> float/str Gnostic scale parameter. Default: <code>'auto'</code>. <code>verbose</code> bool If True, enables detailed logging for debugging. Default: <code>False</code>."},{"location":"metrics/g_relevance/#returns","title":"Returns","text":"<ul> <li>float   The calculated Hc value (mean of squared gnostic characteristics).</li> </ul>"},{"location":"metrics/g_relevance/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like.</li> <li>ValueError   If inputs are empty, contain NaN/Inf, are not 1D, have mismatched shapes, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"metrics/g_relevance/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import hc\n\n# Example 1: Using lists\ny_true = [1, 2, 3]\ny_pred = [1, 2, 3]\nhc_value = hc(y_true, y_pred, case='i')\nprint(hc_value)\n\n# Example 2: Using numpy arrays and irrelevance case\nimport numpy as np\ny_true = np.array([2, 4, 6])\ny_pred = np.array([1, 2, 3])\nhc_value = hc(y_true, y_pred, case='j', verbose=True)\nprint(hc_value)\n</code></pre>"},{"location":"metrics/g_relevance/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must be 1D, have the same shape, and must not be empty or contain NaN/Inf.</li> <li>For standard comparison, irrelevances are calculated with S=1.</li> <li>The Hc metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical metrics.</li> </ul>"},{"location":"metrics/g_relevance/#gnostic-vs-classical-metrics","title":"Gnostic vs. Classical Metrics","text":"<p>Note: Unlike traditional metrics that use statistical means, the Hc metric is computed using gnostic algebra and characteristics. This approach is assumption-free and designed to reveal the true diagnostic properties of your data and model predictions.</p> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_std/","title":"std: Gnostic Standard Deviation Metric","text":"<p>The <code>std</code> function computes the Gnostic standard deviation of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of data dispersion, leveraging irrelevance and fidelity measures for deeper insight into uncertainty and structure.</p>"},{"location":"metrics/g_std/#overview","title":"Overview","text":"<p>Gnostic standard deviation generalizes classical standard deviation by using irrelevance and fidelity measures:</p> <ul> <li>Case <code>'i'</code>: Estimates standard deviation using the estimating geometry.</li> <li>Case <code>'j'</code>: Quantifies standard deviation using the quantifying geometry.</li> </ul> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios. The function returns lower and upper bounds for the standard deviation.</p>"},{"location":"metrics/g_std/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>case</code> str <code>'i'</code> for estimating standard deviation, <code>'j'</code> for quantifying standard deviation. <code>'i'</code> <code>S</code> float/str Scaling parameter for EGDF. Can be <code>float</code> or <code>'auto'</code>. Suggested range: [0.01, 10]. <code>'auto'</code> <code>z0_optimize</code> bool Whether to optimize z0 in ELDF. <code>True</code> <code>data_form</code> str Data form for ELDF: <code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for ELDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_std/#returns","title":"Returns","text":"<ul> <li>tuple   Lower and upper bounds of the standard deviation range.</li> </ul>"},{"location":"metrics/g_std/#raises","title":"Raises","text":"<ul> <li>TypeError   If input is not a numpy array, or if <code>S</code> is not a float or <code>'auto'</code>.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code>/<code>data_form</code> is invalid.</li> </ul>"},{"location":"metrics/g_std/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic standard deviation (default case)\ndata = np.array([1, 2, 3, 4, 5])\nstd_vals = mg.std(data)\nprint(std_vals)  # Returns (lb, ub)\n\n# Example 2: Quantifying standard deviation\nstd_vals_j = mg.std(data, case='j')\nprint(std_vals_j)\n</code></pre>"},{"location":"metrics/g_std/#notes","title":"Notes","text":"<ul> <li>The function uses mean and variance from gnostic theory to compute lower and upper bounds for standard deviation.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical standard deviation.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> <li>If <code>S='auto'</code>, the function optimizes the scaling parameter using EGDF.</li> </ul>"},{"location":"metrics/g_std/#gnostic-vs-classical-standard-deviation","title":"Gnostic vs. Classical Standard Deviation","text":"<p>Note: Unlike classical standard deviation metrics that use statistical means and variances, the Gnostic standard deviation is computed using irrelevance and fidelity measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Authors: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/g_variance/","title":"variance: Gnostic Variance Metric","text":"<p>The <code>variance</code> function computes the Gnostic variance of a data sample. This metric uses gnostic theory to provide robust, assumption-free estimates of data variability, leveraging irrelevance measures for deeper insight into uncertainty and structure.</p>"},{"location":"metrics/g_variance/#overview","title":"Overview","text":"<p>Gnostic variance generalizes classical variance by using irrelevance measures:</p> <ul> <li>Case <code>'i'</code>: Estimates variance using ELDF (Empirical Likelihood Distribution Function).</li> <li>Case <code>'j'</code>: Quantifies variance using QLDF (Quantile Likelihood Distribution Function).</li> </ul> <p>Both approaches are robust to outliers and non-normal data, providing reliable diagnostics in challenging scenarios.</p>"},{"location":"metrics/g_variance/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> np.ndarray Input data array (1D, no NaN/Inf). Required <code>case</code> str <code>'i'</code> for estimating variance (ELDF), <code>'j'</code> for quantifying variance (QLDF). <code>'i'</code> <code>S</code> float/str Scaling parameter for ELDF. can be <code>float</code> or <code>'auto'</code>.  Suggested range: [0.01, 2]. <code>'auto'</code> <code>z0_optimize</code> bool Whether to optimize z0 in ELDF/QLDF. <code>True</code> <code>data_form</code> str Data form for ELDF/QLDF:<code>'a'</code> for additive, <code>'m'</code> for multiplicative. <code>'a'</code> <code>tolerance</code> float Tolerance for ELDF fitting. <code>1e-6</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/g_variance/#returns","title":"Returns","text":"<ul> <li>float   Gnostic variance of the data.</li> </ul>"},{"location":"metrics/g_variance/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf input is not a numpy array.</li> <li>ValueError   If input is not 1D, is empty, contains NaN/Inf, or if <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"metrics/g_variance/#example-usage","title":"Example Usage","text":"<pre><code>import machinegnostics as mg\nimport numpy as np\n\n# Example 1: Compute gnostic variance (default case)\ndata = np.array([1, 2, 3, 4, 5])\nvar = mg.variance(data)\nprint(var)\n# Output: 0.002685330177795109\n</code></pre>"},{"location":"metrics/g_variance/#notes","title":"Notes","text":"<ul> <li>The function uses ELDF or QLDF to compute irrelevance values, which are then squared and averaged.</li> <li>Input data must be 1D, cleaned, and free of NaN/Inf.</li> <li>The metric is robust to outliers and non-normal data, providing more reliable diagnostics than classical variance.</li> <li>Scaling (<code>S</code>), optimization (<code>z0_optimize</code>), and data form (<code>data_form</code>) parameters allow for flexible analysis.</li> </ul>"},{"location":"metrics/g_variance/#gnostic-vs-classical-variance","title":"Gnostic vs. Classical Variance","text":"<p>Note: Unlike classical variance metrics that use statistical means, the Gnostic variance is computed using irrelevance measures from gnostic theory. This approach is assumption-free and designed to reveal the true diagnostic properties of your data.</p> <p>Authors: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/gmmfe/","title":"gmmfe: Geometric Mean of Model Fit Error (GMMFE) Metric","text":"<p>The <code>gmmfe</code> function computes the Geometric Mean of Model Fit Error (GMMFE), a robust metric for evaluating the average relative error between observed data and model predictions on a logarithmic scale. GMMFE is especially useful for datasets with a wide range of values or when the data is multiplicative in nature.</p>"},{"location":"metrics/gmmfe/#overview","title":"Overview","text":"<p>GMMFE quantifies the average multiplicative error between the true and predicted values, making it less sensitive to outliers and scale differences than classical metrics. It is one of the three core criteria (alongside RobR2 and DivI) for evaluating model performance in the Machine Gnostics framework.</p> <p>Mathematically, GMMFE is defined as:</p> \\[ \\text{GMMFE} = \\exp\\left( \\frac{1}{N} \\sum_{i=1}^N \\left| \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) \\right| \\right) \\] <p>where:</p> <ul> <li>\\(y_i\\) is the observed value,</li> <li>\\(\\hat{y}_i\\) is the fitted (predicted) value,</li> <li>\\(N\\) is the number of data points.</li> </ul> <p>A lower GMMFE indicates a better fit, as it means the geometric mean of the relative errors is smaller.</p>"},{"location":"metrics/gmmfe/#interpretation","title":"Interpretation","text":"<ul> <li>Lower GMMFE: Indicates smaller average multiplicative errors and a better model fit.</li> <li>Higher GMMFE: Indicates larger average multiplicative errors and a poorer fit.</li> </ul> <p>GMMFE is particularly valuable when comparing models across datasets with different scales or when the error distribution is multiplicative.</p>"},{"location":"metrics/gmmfe/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/gmmfe/#returns","title":"Returns","text":"<ul> <li>float   The computed Geometric Mean of Model Fit Error (GMMFE) value.</li> </ul>"},{"location":"metrics/gmmfe/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/gmmfe/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import gmmfe\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = gmmfe(y, y_fit)\nprint(result)\n</code></pre>"},{"location":"metrics/gmmfe/#notes","title":"Notes","text":"<ul> <li>GMMFE is calculated using the weighted geometric mean of the relative errors.</li> <li>It is robust to outliers and scale differences, making it suitable for a wide range of regression problems.</li> <li>In the Machine Gnostics framework, GMMFE is used alongside RobR2 and DivI to provide a comprehensive evaluation of model performance.</li> <li>The overall evaluation metric can be computed as:</li> </ul> <p>$$   \\text{EvalMet} = \\frac{\\text{RobR2}}{\\text{GMMFE} \\cdot \\text{DivI}}   $$</p> <p>where a higher EvalMet indicates better model performance.</p> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"metrics/mae/","title":"mean_absolute_error: Mean Absolute Error (MAE) Metric","text":"<p>The <code>mean_absolute_error</code> function computes the mean absolute error (MAE) between true and predicted values. MAE is a fundamental regression metric that measures the average magnitude of errors in a set of predictions, without considering their direction.</p> <p>Unlike traditional error metrics that use the statistical mean, Machine Gnostics metrics are computed using the gnostic mean. The gnostic mean is a robust, assumption-free measure designed to provide deeper insight and reliability, especially in the presence of outliers or non-normal data. This approach ensures that error metrics reflect the true structure and diagnostic properties of your data, in line with the principles of Mathematical Gnostics.</p>"},{"location":"metrics/mae/#overview","title":"Overview","text":"<p>Mean Absolute Error is defined as the average of the absolute differences between actual and predicted values.</p> <p>MAE is widely used in regression analysis to quantify how close predictions are to the actual outcomes. Lower MAE values indicate better model performance.</p>"},{"location":"metrics/mae/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/mae/#returns","title":"Returns","text":"<ul> <li>float   The average absolute difference between actual and predicted values.</li> </ul>"},{"location":"metrics/mae/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"metrics/mae/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import mean_absolute_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_absolute_error(y_true, y_pred))\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(mean_absolute_error(y_true, y_pred))\n</code></pre>"},{"location":"metrics/mae/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>MAE is robust to outliers but does not penalize large errors as strongly as mean squared error (MSE).</li> </ul> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"metrics/mse/","title":"mean_squared_error: Mean Squared Error (MSE) Metric","text":"<p>The <code>mean_squared_error</code> function computes the mean squared error (MSE) between true and predicted values. MSE is a fundamental regression metric that measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value.</p> <p>Unlike traditional error metrics that use the statistical mean, Machine Gnostics metrics are computed using the gnostic mean. The gnostic mean is a robust, assumption-free measure designed to provide deeper insight and reliability, especially in the presence of outliers or non-normal data. This approach ensures that error metrics reflect the true structure and diagnostic properties of your data, in line with the principles of Mathematical Gnostics.</p>"},{"location":"metrics/mse/#overview","title":"Overview","text":"<p>Mean Squared Error is defined as the average of the squared differences between actual and predicted values.</p> <p>MSE is widely used in regression analysis to quantify the accuracy of predictions. Lower MSE values indicate better model performance, while higher values indicate larger errors.</p>"},{"location":"metrics/mse/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/mse/#returns","title":"Returns","text":"<ul> <li>float   The average of squared differences between actual and predicted values.</li> </ul>"},{"location":"metrics/mse/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"metrics/mse/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import mean_squared_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_squared_error(y_true, y_pred))\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(mean_squared_error(y_true, y_pred))\n</code></pre>"},{"location":"metrics/mse/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>MSE penalizes larger errors more than MAE (mean absolute error), making it sensitive to outliers.</li> </ul> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"metrics/precision/","title":"precision_score: Classification Precision Metric","text":"<p>The <code>precision_score</code> function computes the precision of classification models, supporting both binary and multiclass settings. Precision measures the proportion of positive identifications that were actually correct, making it a key metric for evaluating classifiers, especially when the cost of false positives is high.</p>"},{"location":"metrics/precision/#overview","title":"Overview","text":"<p>Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP).</p> <p>This metric is especially important in scenarios where false positives are more costly than false negatives (e.g., spam detection, medical diagnosis).</p>"},{"location":"metrics/precision/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred. <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/precision/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the precision for each class as an array.</li> </ul>"},{"location":"metrics/precision/#returns","title":"Returns","text":"<ul> <li>precision: <code>float</code> or <code>array of floats</code>   Precision score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of precision values for each class.</li> </ul>"},{"location":"metrics/precision/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"metrics/precision/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import precision_score\n\n# Example 1: Macro-averaged precision for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(precision_score(y_true, y_pred, average='macro'))\n\n# Example 2: Binary precision with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(precision_score(df['true'], df['pred'], average='binary')) \n</code></pre>"},{"location":"metrics/precision/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"metrics/r2/","title":"r2_score: Coefficient of Determination (R\u00b2) Metric","text":"<p>The <code>r2_score</code> function computes the coefficient of determination (R\u00b2) for regression tasks. This metric measures the proportion of variance in the target variable that is explained by the model.</p>"},{"location":"metrics/r2/#overview","title":"Overview","text":"<p>R\u00b2 is a standard metric for evaluating regression models: - R\u00b2 = 1 indicates perfect prediction. - R\u00b2 = 0 indicates the model does no better than the mean. - R\u00b2 &lt; 0 indicates the model performs worse than the mean.</p>"},{"location":"metrics/r2/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>y_true</code> array-like True values (targets). Required <code>y_pred</code> array-like Predicted values. Required <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/r2/#returns","title":"Returns","text":"<ul> <li>float   Proportion of variance explained (1 is perfect prediction).</li> </ul>"},{"location":"metrics/r2/#raises","title":"Raises","text":"<ul> <li>TypeError   If inputs are not array-like.</li> <li>ValueError   If shapes do not match or inputs are empty.</li> </ul>"},{"location":"metrics/r2/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import r2_score\n\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nscore = r2_score(y_true, y_pred)\nprint(score)\n</code></pre>"},{"location":"metrics/r2/#adjusted_r2_score-adjusted-r2-metric","title":"adjusted_r2_score: Adjusted R\u00b2 Metric","text":"<p>The <code>adjusted_r2_score</code> function computes the adjusted R\u00b2 score, which accounts for the number of predictors in the model. This metric penalizes the addition of unnecessary features.</p>"},{"location":"metrics/r2/#overview_1","title":"Overview","text":"<p>Adjusted R\u00b2 is useful for comparing models with different numbers of predictors: - Adjusted R\u00b2 increases only if the new predictor improves the model more than would be expected by chance.</p>"},{"location":"metrics/r2/#parameters_1","title":"Parameters","text":"Parameter Type Description Default <code>y_true</code> array-like True values (targets). Required <code>y_pred</code> array-like Predicted values. Required <code>n_features</code> int Number of features (independent variables) in the model. Required <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/r2/#returns_1","title":"Returns","text":"<ul> <li>float   Adjusted R\u00b2 accounting for number of predictors.</li> </ul>"},{"location":"metrics/r2/#raises_1","title":"Raises","text":"<ul> <li>TypeError   If inputs are not array-like or <code>n_features</code> is not a non-negative integer.</li> <li>ValueError   If shapes do not match, inputs are empty, or <code>n_features</code> is invalid.</li> </ul>"},{"location":"metrics/r2/#example-usage_1","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import adjusted_r2_score\n\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nn_features = 2\nscore_adj = adjusted_r2_score(y_true, y_pred, n_features)\nprint(score_adj)\n</code></pre>"},{"location":"metrics/r2/#notes","title":"Notes","text":"<ul> <li>Both functions support input as lists, numpy arrays, or pandas Series.</li> <li>Inputs must be 1D, have the same shape, and must not be empty or contain NaN/Inf.</li> <li>Adjusted R\u00b2 is undefined if the number of samples is less than or equal to the number of features plus one.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-09-24</p>"},{"location":"metrics/r2_score/","title":"robr2: Robust R-squared (RobR2) Metric","text":"<p>The <code>robr2</code> function computes the Robust R-squared (RobR2) value for evaluating the goodness of fit between observed data and model predictions. Unlike the classical R-squared metric, RobR2 is robust to outliers and incorporates sample weights, making it ideal for noisy or irregular datasets.</p>"},{"location":"metrics/r2_score/#overview","title":"Overview","text":"<p>Robust R-squared (RobR2) measures the proportion of variance in the observed data explained by the fitted data, while reducing sensitivity to outliers. This is achieved by using a weighted formulation, which allows for more reliable model evaluation in real-world scenarios where data may not be perfectly clean.</p>"},{"location":"metrics/r2_score/#formula","title":"Formula","text":"\\[ \\text{RobR2} = 1 - \\frac{\\sum_i w_i (e_i - \\bar{e})^2}{\\sum_i w_i (y_i - \\bar{y})^2} \\] <p>Where:</p> <ul> <li>\\(e_i = y_i - \\hat{y}_i\\) (residuals)</li> <li>\\(\\bar{e}\\) = weighted mean of residuals</li> <li>\\(\\bar{y}\\) = weighted mean of observed data</li> <li>\\(w_i\\) = weight for each data point</li> </ul> <p>If weights are not provided, equal weights are assumed.</p>"},{"location":"metrics/r2_score/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>w</code> np.ndarray or None Optional weights for data points. 1D array, same shape as <code>y</code>. If None, equal weights are used. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/r2_score/#returns","title":"Returns","text":"<ul> <li>float   The computed Robust R-squared (RobR2) value. Ranges from 0 (no explanatory power) to 1 (perfect fit).</li> </ul>"},{"location":"metrics/r2_score/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>w</code> is provided and does not have the same shape as <code>y</code>.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"metrics/r2_score/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import robr2\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nw = np.array([1.0, 1.0, 1.0, 1.0])\n\nresult = robr2(y, y_fit, w)\nprint(result)\n</code></pre>"},{"location":"metrics/r2_score/#comparison-with-classical-r-squared","title":"Comparison with Classical R-squared","text":"<ul> <li>Classical R-squared: Assumes equal weights and is sensitive to outliers.</li> <li>RobR2: Incorporates weights and is robust to outliers, making it more reliable for datasets with irregularities or noise.</li> </ul>"},{"location":"metrics/r2_score/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis, Chapter 19</li> </ul>"},{"location":"metrics/r2_score/#notes","title":"Notes","text":"<ul> <li>If weights are not provided, the metric defaults to equal weighting for all data points.</li> <li>RobR2 is particularly useful for robust regression and model evaluation in the presence of outliers.</li> </ul> <p>Author: Nirmal Parmar  Date: 2025-09-24</p>"},{"location":"metrics/recall/","title":"recall_score: Classification Recall Metric","text":"<p>The <code>recall_score</code> function computes the recall of classification models, supporting both binary and multiclass settings. Recall measures the proportion of actual positives that were correctly identified, making it a key metric for evaluating classifiers, especially when the cost of false negatives is high.</p>"},{"location":"metrics/recall/#overview","title":"Overview","text":"<p>Recall is defined as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).</p> <p>This metric is especially important in scenarios where false negatives are more costly than false positives (e.g., disease screening, fraud detection).</p>"},{"location":"metrics/recall/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred. <code>verbose</code> bool True Print detailed progress, warnings, and results"},{"location":"metrics/recall/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the recall for each class as an array.</li> </ul>"},{"location":"metrics/recall/#returns","title":"Returns","text":"<ul> <li>recall: <code>float</code> or <code>array of floats</code>   Recall score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of recall values for each class.</li> </ul>"},{"location":"metrics/recall/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"metrics/recall/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import recall_score\n\n# Example 1: Macro-averaged recall for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(recall_score(y_true, y_pred, average='macro'))\n\n# Example 2: Binary recall with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(recall_score(df['true'], df['pred'], average='binary'))\n</code></pre>"},{"location":"metrics/recall/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"metrics/rmse/","title":"root_mean_squared_error: Root Mean Squared Error (RMSE) Metric","text":"<p>The <code>root_mean_squared_error</code> function computes the Root Mean Squared Error (RMSE) between true and predicted values. RMSE is a widely used regression metric that measures the square root of the average of the squared differences between predicted and actual values.</p> <p>Unlike traditional error metrics that use the statistical mean, Machine Gnostics metrics are computed using the gnostic mean. The gnostic mean is a robust, assumption-free measure designed to provide deeper insight and reliability, especially in the presence of outliers or non-normal data. This approach ensures that error metrics reflect the true structure and diagnostic properties of your data, in line with the principles of Mathematical Gnostics.</p>"},{"location":"metrics/rmse/#overview","title":"Overview","text":"<p>Root Mean Squared Error is defined as the square root of the mean squared error.</p> <p>RMSE provides an interpretable measure of prediction error in the same units as the target variable. Lower RMSE values indicate better model performance.</p>"},{"location":"metrics/rmse/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values. <code>verbose</code> bool Print detailed progress, warnings, and results"},{"location":"metrics/rmse/#returns","title":"Returns","text":"<ul> <li>float   The square root of the average of squared errors between actual and predicted values.</li> </ul>"},{"location":"metrics/rmse/#raises","title":"Raises","text":"<ul> <li>TypeErrorIf <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"metrics/rmse/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import root_mean_squared_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(root_mean_squared_error(y_true, y_pred))\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(root_mean_squared_error(y_true, y_pred))\n</code></pre>"},{"location":"metrics/rmse/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>RMSE is sensitive to outliers due to the squaring of errors.</li> <li>RMSE is in the same units as the target variable, making it easy to interpret.</li> </ul> <p>Author: Nirmal Parmar      Date: 2025-09-24</p>"},{"location":"metrics/silhouette_score/","title":"silhouette_score: Silhouette Score Metric","text":"<p>The <code>silhouette_score</code> function computes the mean Silhouette Coefficient for a dataset given its cluster labels. It evaluates the quality of clustering by measuring how similar an object is to its own cluster (cohesion) compared to other clusters (separation).</p>"},{"location":"metrics/silhouette_score/#overview","title":"Overview","text":"<p>The Silhouette Coefficient is calculated for each sample using:</p> <ul> <li>a: The mean distance between a sample and all other points in the same cluster.</li> <li>b: The mean distance between a sample and all other points in the nearest cluster.</li> </ul> <p>The coefficient for a single sample is given by: [ s = \\frac{b - a}{\\max(a, b)} ]</p> <p>Expected values range from -1 to 1:</p> <ul> <li>+1: Indicates that the sample is far away from the neighboring clusters.</li> <li>0: Indicates that the sample is on or very close to the decision boundary between two neighboring clusters.</li> <li>-1: Indicates that those samples might have been assigned to the wrong cluster.</li> </ul>"},{"location":"metrics/silhouette_score/#parameters","title":"Parameters","text":"Parameter Type Description <code>X</code> array-like Feature array of shape (n_samples, n_features). <code>labels</code> array-like Cluster labels for each sample. Shape (n_samples,). <code>verbose</code> bool If True, enables detailed logging for debugging. Default: <code>False</code>."},{"location":"metrics/silhouette_score/#returns","title":"Returns","text":"<ul> <li>float   The mean Silhouette Coefficient for all samples.</li> </ul>"},{"location":"metrics/silhouette_score/#raises","title":"Raises","text":"<ul> <li>TypeError   If inputs <code>X</code> or <code>labels</code> are not array-like.</li> <li>ValueError   If dimensions are incorrect, data is empty, contains NaN/Inf, or if number of labels doesn't match samples.</li> </ul>"},{"location":"metrics/silhouette_score/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import silhouette_score\nimport numpy as np\n\n# Example: Clustering assessment\nX = np.array([[1, 2], [1, 4], [1, 0],\n              [10, 2], [10, 4], [10, 0]])\nlabels = np.array([0, 0, 0, 1, 1, 1])\n\nscore = silhouette_score(X, labels)\nprint(f\"Silhouette Score: {score}\")\n</code></pre>"},{"location":"metrics/silhouette_score/#notes","title":"Notes","text":"<ul> <li>Requires at least 2 distinct clusters to be calculated.</li> <li>If the number of unique labels is 1 or equal to the number of samples, the score is 0.0.</li> <li>Uses Euclidean distance for calculations.</li> <li>This metric is useful for selecting the optimal number of clusters (e.g., in K-Means).</li> </ul> <p>Author: Nirmal Parmar Date: 2026-02-02</p>"},{"location":"metrics/stationarity_test/","title":"stationarity_test: Gnostic Stationarity Test","text":"<p>The <code>stationarity_test</code> function checks for stationarity in a time series using the homogeneity of Residual Entropy. This method leverages the Machine Gnostics framework to provide a robust assessment of whether the statistical properties of a time series are constant over time.</p>"},{"location":"metrics/stationarity_test/#overview","title":"Overview","text":"<p>This function analyzes the stationarity of a time series by computing the Residual Entropy (RE) over a sliding window. It then determines if the sequence of RE values is homogeneous using the DataHomogeneity test.</p> <p>The process involves:</p> <ol> <li>Sliding a window of size <code>window_size</code> across the data.</li> <li>Fitting an Estimation Global Distribution Function (EGDF) to the data in each window.</li> <li>Extracting the Residual Entropy for each window.</li> <li>Testing the sequence of entropy values for gnostic homogeneity.</li> </ol> <p>If the Residual Entropy sequence is homogeneous, the time series is considered stationary.</p>"},{"location":"metrics/stationarity_test/#parameters","title":"Parameters","text":"Parameter Type Description Default <code>data</code> array-like Time series data to analyze (1D). Required <code>window_size</code> int Size of the sliding window for entropy calculation. <code>10</code> <code>S</code> str Scale parameter for EGDF fitting (<code>'auto'</code> or float). <code>'auto'</code> <code>data_form</code> str Form of input data: <code>'a'</code> (additive) or <code>'m'</code> (multiplicative). <code>'a'</code> <code>verbose</code> bool If True, enables detailed logging for debugging. <code>False</code>"},{"location":"metrics/stationarity_test/#returns","title":"Returns","text":"<ul> <li>bool <code>True</code> if the time series is stationary (Residual Entropy is homogeneous), <code>False</code> otherwise.</li> </ul>"},{"location":"metrics/stationarity_test/#raises","title":"Raises","text":"<ul> <li>TypeError   If <code>data</code> is not array-like.</li> <li>ValueError   If data is empty, or if <code>window_size</code> is invalid (must be less than data length).</li> </ul>"},{"location":"metrics/stationarity_test/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import stationarity_test\nimport numpy as np\n\n# Example 1: Stationary data (Gaussian noise)\ndata_stationary = np.random.normal(0, 1, 100)\nis_stat = stationarity_test(data_stationary, window_size=20)\nprint(f\"Is stationary: {is_stat}\")\n\n# Example 2: Non-stationary data (Trend)\nt = np.linspace(0, 10, 100)\ndata_trend = np.sin(t) + t  # Sine wave with linear trend\nis_stat_trend = stationarity_test(data_trend, window_size=20)\nprint(f\"Is stationary: {is_stat_trend}\")\n</code></pre>"},{"location":"metrics/stationarity_test/#notes","title":"Notes","text":"<ul> <li>Window Size: The <code>window_size</code> determines the locality of the entropy calculation. It must be smaller than the total length of the data but large enough to fit an EGDF (typically &gt; 3).</li> <li>Residual Entropy: This metric captures the uncertainty within the local window. If this uncertainty remains consistent (homogeneous) throughout the series, the process is deemed stationary.</li> <li>Robustness: Like other Gnostic metrics, this test is robust to outliers and does not rely on Gaussian assumptions.</li> </ul> <p>Author: Nirmal Parmar Date: 2026-02-02</p>"},{"location":"mg/architecture/","title":"Machine Gnostics Architecture","text":"<p>Overview</p> <p>This diagram presents the conceptual architecture of the Machine Gnostics paradigm. Unlike traditional machine learning rooted in statistical theory, this new approach is built on the foundation of Mathematical Gnostics (MG)\u2014a finite, deterministic, and physically inspired framework.</p> <p>High-level Architecture Diagram:</p> <pre><code>flowchart TD\n    DATA[\"INPUT\"]\n    USER[\"OUTPUT\"]\n    subgraph MG_SYS[\"Machine Gnostics Architecture\"]\n        IFACE1[\"Machine Gnostics Interface\"]\n        MGTheory[\"Mathematical Gnostics\"]\n        MAGCAL[\"MAGCAL\"]\n        Models[\"Models\"]\n        Metrics[\"Metrics\"]\n        Magnet[\"Magnet\"]\n        MLFlow[\"mlflow Integration\"]\n        IFACE2[\"Machine Gnostics Interface\"]\n    end\n    DATA --&gt; IFACE1\n    IFACE1 --&gt; MGTheory\n    MGTheory --&gt; MAGCAL\n    MAGCAL --&gt; Models\n    MAGCAL --&gt; Metrics\n    MAGCAL --&gt; Magnet\n    Models &lt;--&gt; Metrics\n    Metrics &lt;--&gt; Magnet\n    Models --&gt; MLFlow\n    Metrics --&gt; MLFlow\n    Magnet --&gt; MLFlow\n    MLFlow --&gt; IFACE2\n    IFACE2 --&gt; USER</code></pre> <p>Glossary:</p> MAGCAL Mathematical Gnostics Calculations and Data Analysis Models Models Machine Learning Models Magnet Machine Gnostics Neural Networks Metrics Machine Gnostics and Statistical Metrics"},{"location":"mg/architecture/#components","title":"Components","text":"<p>1. DATA</p> <p>The foundation of Machine Gnostics is DATA, interpreted differently from statistical frameworks:</p> <ul> <li>Each data point is a real event with individual importance and uncertainty.</li> <li>No reliance on large sample assumptions or population-level abstractions.</li> <li>Adheres to the principle: \u201cLet the data speak for themselves.\u201d</li> </ul> <p>2. Mathematical Gnostics</p> <p>This is the theoretical base of the system. It replaces the assumptions of probability with deterministic modeling:</p> <ul> <li>Uses Riemannian geometry, Einsteinian relativity, vector bi-algebra, and thermodynamics.</li> <li>Models uncertainty at the level of individual events, not populations.</li> <li>Establishes a finite theory for finite data, with robust treatment of variability.</li> </ul> <p>3. MAGCAL (Mathematical Gnostics Calculations)</p> <p>MAGCAL is the computational engine that enables gnostic inference:</p> <ul> <li>Performs deterministic, non-statistical calculations.</li> <li>Enables robust modeling using gnostic algebra and error geometry.</li> <li>Resilient to outliers, corrupted data, and distributional shifts.</li> </ul> <p>4. Models | Metrics | Magnet</p> <p>This layer maps to familiar components of ML pipelines but with MG-specific logic:</p> <ul> <li>Models: Developed on the principles of Mathematical Gnostics.</li> <li>Metrics: Evaluate using gnostic loss functions and event-level error propagation.</li> <li>Magnet: A novel neural architecture based on Mathematical Gnostics</li> </ul> <p>5. mlflow Integration</p> <p>Despite its theoretical novelty, Machine Gnostics fits smoothly into modern ML workflows:</p> <ul> <li>mlflow provides tracking, model registry, and reproducibility.</li> <li>Ensures that experiments and deployments align with standard ML practices.</li> </ul> <p>6. Machine Gnostics (Integration Layer)</p> <p>This layer unifies all components into a working system:</p> <ul> <li>MAGCAL is a Mathematical Gnostics based engine.</li> <li>Functions as a complete ML framework based on a deterministic, finite, and algebraic paradigm.</li> <li>Enables seamless data-to-model pipelines rooted in the principles of Mathematical Gnostics.</li> </ul>"},{"location":"mg/architecture/#summary","title":"Summary","text":"<p>Quick Understanding</p> Traditional ML (Statistics) Machine Gnostics Based on probability theory Based on deterministic finite theory Relies on large datasets Works directly with small datasets Uses averages and distributions Uses individual error and event modeling Rooted in Euclidean geometry Rooted in Riemannian geometry &amp; physics Vulnerable to outliers Robust to real-world irregularities"},{"location":"mg/architecture/#references","title":"References","text":"<p>Machine Gnostics is not just an alternative\u2014it is a new foundation for AI, capable of rational, robust, and interpretable data modeling.</p>"},{"location":"mg/concepts/","title":"Foundations of Mathematical Gnostics","text":"<p>Abstract</p> <p>Mathematical Gnostics (MG) offers a fundamentally different approach to data analysis and uncertainty compared to traditional mathematical statistics. Understanding these differences is crucial for users of the Machine Gnostics library, as it shapes the philosophy, algorithms, and practical outcomes of gnostic-based data analysis.</p>"},{"location":"mg/concepts/#key-differences-between-statistics-and-gnostics","title":"Key Differences Between Statistics and Gnostics","text":"1. Focus on the Individual Event2. Treatment of Data &amp; Uncertainty3. Math &amp; Physical Foundations4. Aggregation &amp; Analysis <ul> <li>Statistics: Traditional statistics investigates the regularities and properties of large collections of uncertain events, relying on the Law of Large Numbers and the Central Limit Theorem. The theory is built for infinite or very large sample sizes, and results for finite datasets are often extrapolated from these infinite models.</li> <li>Gnostics: MG concentrates on the uncertainty of a single event. It builds mathematical and physical models that directly address finite (even small) collections of uncertain events. This approach is more natural for real-world scenarios, where data is always finite.</li> </ul> <ul> <li>Statistics: Assumes the existence of a mean and standard deviation for an underlying probability distribution. Data is often treated as samples from an idealized random process, and analysis is based on population-level properties.</li> <li>Gnostics: Treats each data point as an image of a real, existing event governed by the laws of nature. MG respects the actual values of the data, following the principle: \u201cLet data speak for themselves.\u201d The weight or importance of each data item is determined by its own individual error, not by its class or family.</li> </ul> <ul> <li>Statistics: Relies primarily on Euclidean geometry and Newtonian mechanics, with mathematical theory of measure as its foundation.</li> <li>Gnostics: Utilizes Riemannian geometry and Einstein\u2019s relativistic mechanics, along with vector bi-algebra and thermodynamics. MG also introduces quantification theory as a foundational measurement theory.</li> </ul> <ul> <li>Statistics: Aggregates observed data additively, focusing on population-level summaries.</li> <li>Gnostics: Suggests that additive aggregation should be applied to the parameters of the Ideal Gnostic Cycle, not directly to the observed data.</li> </ul>"},{"location":"mg/concepts/#scientific-bases-a-comparative-diagram","title":"Scientific Bases: A Comparative Diagram","text":"The scientific foundations of statistics (left) and gnostics (right) span mathematics, physics, geometry, and measurement theory, but differ fundamentally in their approach and underlying principles. [Pavel Kovanic, Mathematical Gnostics (2023)]"},{"location":"mg/concepts/#approaches-to-data-uncertainty","title":"Approaches to Data Uncertainty","text":"Statistics builds its theory for infinite sample sizes and extrapolates results for finite datasets. Gnostics, in contrast, constructs its theory directly for finite (even single) events, providing a more natural fit for real-world data. [Pavel Kovanic, Mathematical Gnostics (2023)] <p>Paradigm Shift: From Statistics to Gnostics</p> <p>Mathematical gnostics represents a paradigm shift in how we approach data variability and analysis:</p> <ul> <li>Statistics is rooted in the behavior of large numbers and infinite limits, often requiring extrapolation to address finite datasets.</li> <li>Gnostics is designed for the finite world, modeling uncertainty at the level of individual events and small datasets.</li> </ul> <p>This shift requires a new way of thinking, much like moving from Newtonian to Einsteinian physics. While statistics is easily demonstrated with simple experiments (like coin tosses), the power of gnostics is revealed through its algorithms and their performance on real-world, finite data.</p>"},{"location":"mg/concepts/#principles-of-the-gnostic-paradigm","title":"Principles of the Gnostic Paradigm","text":"<ul> <li>Concentration on Individual Events:   MG focuses on the regularities and uncertainty of individual events, not just large populations.</li> <li>Respect for Data Values:   Data is taken as it is, with each value carrying its own information and uncertainty.</li> <li>Use of Advanced Geometry and Mechanics:   MG employs Riemannian geometry and relativistic mechanics, providing a richer mathematical framework for modeling uncertainty.</li> <li>Individual Error Weighting:   The importance of each data point is determined by its own error, not by group-level properties.</li> </ul>"},{"location":"mg/concepts/#why-adopt-the-gnostic-approach","title":"Why Adopt the Gnostic Approach?","text":"<p>Advantages</p> <ul> <li>Natural Fit for Finite Data: Real-world data is always finite. MG provides tools and theory that are directly applicable without relying on extrapolation from infinite models.</li> <li>Robustness: By focusing on individual data points and their uncertainties, MG offers greater resilience to outliers and corrupted data.</li> <li>Paradigm-Changing Power: MG overcomes many limitations of traditional statistics, especially in cases where statistical assumptions break down.</li> </ul>"},{"location":"mg/concepts/#further-reading","title":"Further Reading","text":"<p>Literature</p> <p>For a deeper dive into the foundations and applications of mathematical gnostics, see:</p> <ul> <li>Pavel Kovanic, Mathematical Gnostics (2023)</li> <li>Pavel Kovanic &amp; M.B. Humber, The Economics of Information: Mathematical Gnostics for Data Analysis (2015)</li> </ul> <p>Mathematical Gnostics is a new paradigm for data analysis\u2014one that respects the individuality of data, leverages advanced mathematics, and is designed for the finite, real world.</p>"},{"location":"mg/gdf/","title":"Gnostic Distribution Functions (GDF)","text":"<p>Abstract</p> <p>Gnostic Distribution Functions (GDF) are a new class of probability and density estimators designed for robust, flexible, and assumption-free data analysis. Unlike traditional statistical distributions, GDFs do not require any prior assumptions about the underlying data distribution. Instead, they allow the data to \"speak for themselves,\" making them especially powerful for small, noisy, or uncertain datasets.</p>"},{"location":"mg/gdf/#why-use-gdf","title":"Why Use GDF?","text":"<p>Key Benefits</p> <ul> <li>[x] No A Priori Assumptions: GDFs do not rely on predefined parametric forms or statistical models.</li> <li>[x] Robustness: They are inherently robust to outliers and inner noise, making them suitable for real-world, contaminated, or uncertain data.</li> <li>[x] Flexibility: GDFs adapt to both homogeneous and heterogeneous data samples, providing detailed insights into data structure.</li> <li>[x] Wide Applicability: Useful for probability estimation, density estimation, cluster analysis, and homogeneity testing.</li> </ul>"},{"location":"mg/gdf/#four-types-of-gnostic-distribution-functions","title":"Four Types of Gnostic Distribution Functions","text":"<p>GDFs are organized along two axes:</p> <ol> <li>Local vs. Global: Local functions use weighted irrelevance, while global functions use normalized weights.</li> <li>Estimating vs. Quantifying: Estimating functions use estimating irrelevance, while quantifying functions use quantifying irrelevance.</li> </ol> <p>This results in four types:</p> ELDFEGDFQLDFQGDF <p>Estimating Local Distribution Function</p> <ul> <li>Characteristic: Outlier-resistant, highly flexible.</li> <li>Best for: Cluster analysis, detailed structure exploration.</li> </ul> <p>Estimating Global Distribution Function</p> <ul> <li>Characteristic: Outlier-resistant, unique for each sample.</li> <li>Best for: Homogeneity testing, robust global summaries.</li> </ul> <p>Quantifying Local Distribution Function</p> <ul> <li>Characteristic: Inlier-resistant, highly flexible.</li> <li>Best for: Analysis where loose clustering or inlier noise is a factor.</li> </ul> <p>Quantifying Global Distribution Function</p> <ul> <li>Characteristic: Inlier-resistant, unique for each sample.</li> <li>Best for: Robust estimation when inliers (dense clusters) dominate.</li> </ul>"},{"location":"mg/gdf/#key-concepts","title":"Key Concepts","text":"Data-Driven GDFs are parameterized directly by your data, not by external assumptions. Robustness ELDF and EGDF are robust to outliers, while QLDF and QGDF are robust to inliers (dense clusters). Flexibility Local functions (ELDF, QLDF) are highly flexible and ideal for cluster analysis and exploring marginal data structures. Global functions (EGDF, QGDF) are best for homogeneity testing and robust probability estimation. Scale Parameter The flexibility of local functions is controlled by a scale parameter, allowing you to \"zoom in\" on data structure. <p>Practical Guidance</p> <ul> <li>Use local functions for exploratory, granular analysis and cluster detection.</li> <li>Use global functions for summary, sample-wide analysis and homogeneity testing.</li> <li>Choose estimating functions when robustness to outliers is needed.</li> <li>Choose quantifying functions when robustness to inliers is important.</li> </ul> <p>Summary</p> <p>GDFs provide robust, flexible tools for probability and density estimation, especially in challenging data scenarios. The four types allow you to tailor your analysis to the nature of your data and the goals of your study. By removing the constraints of traditional statistical models, GDFs open new possibilities for data-driven insights.</p> <p>For implementation details and examples, see the Tutorials.</p>"},{"location":"mg/mg_arguments/","title":"Glossary","text":"<p>Abstract</p> <p>This document provides definitions and explanations for the main arguments and variables used in Machine Gnostics data analytics, machine learning, and deep learning models. Understanding these concepts will help users grasp the unique characteristics of the Machine Gnostics library, which is based on the non-statistical paradigm of Mathematical Gnostics.</p>"},{"location":"mg/mg_arguments/#core-concepts","title":"Core Concepts","text":"Machine Gnostics A machine learning and deep learning library founded on Mathematical Gnostics, a non-statistical paradigm for data analysis. Mathematical Gnostics An alternative to traditional statistical methods, focusing on the quantification and estimation of uncertainty in data."},{"location":"mg/mg_arguments/#key-arguments-and-gnostic-characteristics","title":"Key Arguments and Gnostic Characteristics","text":""},{"location":"mg/mg_arguments/#1-gnostic-geometries","title":"1. Gnostic Geometries","text":"<p>These are the fundamental variables used to describe data in the gnostic framework. They are divided into two main spaces:</p> <ul> <li>Quantifying Space (Q-space, \\( j \\)): Describes the variability and irrelevance in the data.</li> <li>Estimating Space (E-space, \\( i \\)): Describes the estimation of variability and relevance.</li> </ul> <p>Comparative Table of Geometries:</p> Feature Quantifying (Q-space) Estimating (E-space) Variability \\( f_j \\) (Quantifying data variability) \\( f_i \\) (Estimating data variability) Relevance/Irrelevance \\( h_j \\) (Quantifying irrelevance/error) \\( h_i \\) (Estimating relevance) Probability \\( p_j \\) (Quantifying probability) \\( p_i \\) (Estimating probability) Information \\( I_j \\) (Quantifying information) \\( I_i \\) (Estimating information) Entropy \\( e_j \\) (Quantifying entropy) \\( e_i \\) (Estimating entropy) <p>All four key variables (\\( f_j, h_j, f_i, h_i \\)) are collectively called gnostic characteristics.</p>"},{"location":"mg/mg_arguments/#detailed-definitions","title":"Detailed Definitions","text":""},{"location":"mg/mg_arguments/#2-entropy-residuals","title":"2. Entropy &amp; Residuals","text":"<ul> <li> <p>\\( e_i \\): Estimating entropy   Entropy estimate for the data.</p> </li> <li> <p>\\( e_j \\): Quantifying entropy   Quantifies the entropy content.</p> </li> <li> <p>\\( re \\): Residual entropy   The remaining entropy after estimation, representing the difference between quantification and estimation entropy.</p> </li> </ul>"},{"location":"mg/mg_arguments/#3-loss-functions","title":"3. Loss Functions","text":"<ul> <li>\\( H_c \\) loss: Gnostic mean relevance loss   A loss function based on gnostic relevance, where \\( c \\) can be \\( i \\) or \\( j \\).</li> </ul> <p>Further Reading</p> <p>For more detailed mathematical background, see the foundational texts on References and the documentation of the Machine Gnostics library.</p>"},{"location":"mg/principles/","title":"Principles of Advanced Data Analysis in Machine Gnostics","text":"<p>Abstract</p> <p>Machine Gnostics is grounded in the philosophy of Mathematical Gnostics, which emphasizes extracting the maximum information from data while respecting its objectivity and inherent structure. These principles are especially relevant for modern machine learning and data science, where robust, data-driven insights are crucial.</p> <p>Below are the core principles of advanced data analysis as practiced in Machine Gnostics, adapted for practical use in machine learning and data science:</p>"},{"location":"mg/principles/#key-principles","title":"Key Principles","text":"1. Respect the Objectivity of Data <ul> <li>Avoid imposing unjustified models: Do not force data into a priori statistical models or distributions without evidence.</li> <li>Do not trim or discard outliers without justification: Outliers may contain valuable information about the system or process.</li> <li>Acknowledge non-homogeneity: Recognize and address the presence of outliers and sample non-homogeneity rather than ignoring them.</li> <li>Use proper aggregation: Aggregate data in ways that respect the underlying structure and axioms of gnostic theory.</li> <li>Respect data finiteness: Do not treat finite samples as if they were infinite populations.</li> </ul> 2. Make Use of All Available Data <ul> <li>Include censored and incomplete data: Do not ignore data just because it is partially observed.</li> <li>Weight outliers and inliers appropriately: Assign justified weights to suspected outliers and inliers (noise), rather than excluding them outright.</li> <li>Exclude data only with evidence: Remove data points only if their impact is negligible or their origin is invalid.</li> <li>Consider side effects: Be aware of and account for side effects caused by the processes generating the data.</li> </ul> 3. Let the Data Decide <ul> <li>Allow data to determine its own structure: Let the data reveal its group membership, homogeneity, bounds, and metric space.</li> <li>Data-driven uncertainty: Evaluate uncertainty using the data\u2019s own properties, not just statistical assumptions.</li> <li>Interdependence and distribution: Let the data inform you about its interdependence, distribution, and density functions.</li> <li>Separate uncertainty from variability: Distinguish between uncertainty and true variability in the data.</li> </ul> 4. Individualized Weighting <ul> <li>Assign weights at the data point level: Each data item should be weighted based on its own value, not just the sample it belongs to.</li> </ul> 5. Use Statistical Methods Judiciously <ul> <li>Justify statistical assumptions: Only use statistical methods when their assumptions are met by the data.</li> <li>Embrace non-statistical methods: When statistical assumptions fail, use robust, non-statistical approaches.</li> </ul> 6. Prefer Robust Methods <ul> <li>Robust estimation: Use robust estimation and identification methods over non-robust ones, especially in the presence of outliers or non-normal data.</li> <li>Choose the right robustness: Select the type of robustness (inner/outer) appropriate for your task.</li> </ul> 7. Prefer Distributions Over Point Estimates <ul> <li>Use distribution functions: Where possible, use full distributions rather than single-point estimates for data characteristics.</li> </ul> 8. Ensure Comparability <ul> <li>Compare like with like: Only compare objects or samples that behave according to the same model.</li> </ul> 9. Seek Explanations, Not Excuses <ul> <li>Don\u2019t blame randomness: Investigate and explain uncertainty using data and available information, rather than attributing everything to randomness.</li> </ul> 10. Apply Realistic and Theoretically Sound Criteria <ul> <li>Optimize using information/entropy: Use information-theoretic criteria for optimization and evaluation.</li> <li>Follow optimal data transformation paths: Respect theoretically proven optimal methods for data transformation and estimation.</li> </ul> 11. Maintain an Open and Critical Mindset <ul> <li>Avoid methodological conservatism: Be open to new methods and approaches.</li> <li>Challenge expectations: Do not insist on preconceived outcomes or reject unexpected results without further analysis.</li> <li>Prioritize thoughtful analysis: The best data treatment may require more effort and deeper thinking.</li> </ul>"},{"location":"mg/principles/#why-these-principles-matter","title":"Why These Principles Matter","text":"<p>Advantages for Machine Learning &amp; Data Science</p> <ul> <li>[x] Robustness: Machine Gnostics methods are designed to be robust to outliers, noise, and non-standard data distributions, making them ideal for real-world data.</li> <li>[x] Data-Driven: The approach lets the data guide the analysis, reducing bias from unjustified assumptions.</li> <li>[x] Comprehensive Use of Data: No data is wasted\u2014every point is considered for its potential information value.</li> <li>[x] Transparency: By letting the data decide, results are more interpretable and trustworthy.</li> </ul> <p>Summary for New Users</p> <ul> <li>Don\u2019t force your data into ill-fitting models.</li> <li>Use all your data, including outliers and incomplete points, with justified weighting.</li> <li>Let the data reveal its own structure, uncertainty, and relationships.</li> <li>Prefer robust, information-theoretic methods when possible.</li> <li>Be open-minded and critical\u2014let the data, not your expectations, drive your analysis.</li> </ul> <p>Machine Gnostics provides a principled, robust, and data-centric foundation for advanced data analysis in machine learning and data science.</p>"},{"location":"mg/principles/#references","title":"References","text":""},{"location":"models/ml_models/","title":"Models - Machine Learning (Machine Gnostics)","text":""},{"location":"models/ml_models/#welcome-to-machine-gnostics-machine-learning-models","title":"Welcome to Machine Gnostics Machine Learning Models","text":"<p>Machine Gnostics provides a growing suite of machine learning models for transparent, robust, and diagnostic predictive analytics. Our goal is to deliver interpretable, assumption-free machine learning solutions that combine classic algorithms with gnostic diagnostics. Whether you are working on classification, regression, clustering, or other tasks, Machine Gnostics models help you understand both predictions and underlying data structure.</p> <p>NOTE</p> <p>We are actively developing additional machine learning models in all categories\u2014including supervised, unsupervised, and more. Stay tuned for updates as new tools and documentation become available.</p> <p>We are open to collaboration and new ideas. If you\u2019re interested in contributing, sharing feedback, or exploring partnerships, feel free to connect with us\u2014your insights and creativity are always welcome!</p>"},{"location":"models/ml_models/#key-machine-learning-model-categories","title":"Key Machine Learning Model Categories","text":"<ul> <li> <p>Classification Models</p> <ul> <li>Logistic Regression</li> <li>Multi Class Classification</li> </ul> </li> <li> <p>Regression Models</p> <ul> <li>Linear Regression</li> <li>Polynomial Regression</li> </ul> </li> <li> <p>Clustering Models</p> <ul> <li>KMeans Clustering</li> <li>Estimating Local Clustering</li> </ul> </li> <li> <p>Forecasting Models</p> <ul> <li>Auto Regressor</li> <li>ARIMA</li> <li>SARIMA</li> </ul> </li> <li> <p>Tree Models Enhanced Scikit-learn</p> <ul> <li>Decision Tree Regressor</li> <li>Decision Tree Classifier</li> <li>Random Forest Regressor</li> <li>Random Forest Classifier</li> </ul> </li> <li> <p>Tree Models Enhanced XGBoost</p> <ul> <li>Gnostic Boosting Regressor</li> <li>Gnostic Boosting Classifier</li> </ul> </li> <li> <p>Model Util</p> <ul> <li>Cross Validation</li> <li>Train Test Splits</li> <li>Gnostic Metrics</li> </ul> </li> <li> <p>Deployment</p> <ul> <li>Mlflow Integration</li> </ul> </li> </ul>"},{"location":"models/ml_models/#why-use-machine-gnostics-machine-learning-models","title":"Why Use Machine Gnostics Machine Learning Models?","text":"<ul> <li>Transparent: Built-in diagnostics and error analysis for every model.</li> <li>Assumption-Free: No strict requirements on data distribution or linearity.</li> <li>Robust: Handles outliers, non-normality, and real-world data challenges.</li> <li>Extensible: Integrates seamlessly with Python data science and ML workflows.</li> <li>Expanding: New models and features are continuously being added.</li> </ul>"},{"location":"models/ml_models/#getting-started","title":"Getting Started","text":"<p>Explore the documentation for each model to learn about their features, usage patterns, and example workflows.  </p> <p>Each page provides a detailed overview, key features, parameters, example usage, and references.</p>"},{"location":"models/ml_models/#next-steps","title":"Next Steps","text":"<ul> <li>Browse individual model pages for in-depth documentation and code examples.</li> <li>Try out example notebooks in the examples folder for hands-on learning.</li> <li>Integrate models into your own machine learning pipeline for robust, diagnostic predictive analytics.</li> <li>Check back regularly for new models and updates as our development continues.</li> </ul> <p>\"In Machine Gnostics, every model is a step toward deeper understanding\u2014of your data, your process, and your discoveries.\"</p>"},{"location":"models/cart/dt_cls/","title":"GnosticDecisionTreeClassifier: Robust Decision Tree with Machine Gnostics","text":"<p>The <code>GnosticDecisionTreeClassifier</code> implements a single Decision Tree Classifier enhanced with iterative gnostic reweighting. It is designed to handle outliers and varying data quality by identifying and down-weighting samples with high gnostic residual or uncertainty.</p>"},{"location":"models/cart/dt_cls/#overview","title":"Overview","text":"<p>Machine Gnostics <code>GnosticDecisionTreeClassifier</code> combines the interpretability of decision trees with the robustness of Mathematical Gnostics. It iteratively refines sample weights to improve classification performance in the presence of noise.</p> <ul> <li>Robust Classification: Identifies and down-weights samples with high gnostic residual/uncertainty.</li> <li>Iterative Refinement: Updates sample weights based on classification probabilities over iterations.</li> <li>Data Quality Handling: Automatically adjusts to data with outliers or non-Gaussian noise.</li> <li>Flexible: Supports configurable depth, splitting criteria, and gnostic iterations.</li> <li>Event-Level Modeling: Handles uncertainty at the level of individual data events.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cart/dt_cls/#key-features","title":"Key Features","text":"<ul> <li>Robust classification using iterative gnostic reweighting</li> <li>Iterative refinement of sample weights</li> <li>Configurable tree depth and splitting criteria</li> <li>Identifies and handles outliers automatically</li> <li>Convergence-based early stopping</li> <li>Training history tracking for analysis</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cart/dt_cls/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>max_depth</code> <code>int</code> <code>None</code> Maximum depth of the tree. <code>min_samples_split</code> <code>int</code> <code>2</code> Minimum samples to split a node. <code>gnostic_weights</code> <code>bool</code> <code>True</code> Whether to use iterative gnostic weights. <code>max_iter</code> <code>int</code> <code>10</code> Maximum gnostic iterations. <code>tolerance</code> <code>float</code> <code>1e-4</code> Convergence tolerance. <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative). <code>verbose</code> <code>bool</code> <code>False</code> Verbosity. <code>random_state</code> <code>int</code> <code>None</code> Random seed. <code>history</code> <code>bool</code> <code>True</code> Whether to record training history. <code>scale</code> <code>str</code> <code>'auto'</code> Scaling method for input features. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected."},{"location":"models/cart/dt_cls/#attributes","title":"Attributes","text":"<ul> <li>gnostic_weights: <code>bool</code><ul> <li>Whether iterative gnostic weights were used.</li> </ul> </li> <li>max_depth: <code>int</code><ul> <li>Maximum depth of the tree.</li> </ul> </li> <li>max_iter: <code>int</code><ul> <li>Maximum number of iterations used.</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (if enabled).</li> </ul> </li> <li>tolerance, data_form, verbose, random_state, scale, early_stopping<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_cls/#methods","title":"Methods","text":""},{"location":"models/cart/dt_cls/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the model to the data.</p> <p>This method trains the decision tree classifier using the provided input features and target labels. If <code>gnostic_weights</code> is True, it iteratively refines the model by reweighting samples based on gnostic residuals.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Target labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>GnosticDecisionTreeClassifier</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_cls/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict outcomes for new data.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted class labels.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_cls/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Compute the accuracy score of the model on given data.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>True class labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Accuracy score of the model predictions.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_cls/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cart/dt_cls/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>GnosticDecisionTreeClassifier</code> with loaded parameters.</p>"},{"location":"models/cart/dt_cls/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>from machinegnostics.models import GnosticDecisionTreeClassifier\n\n# Initialize model\nmodel = GnosticDecisionTreeClassifier(\n    max_depth=3,\n    gnostic_weights=True,\n    verbose=True\n)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict\npreds = model.predict(X)\n\n# Score\nacc = model.score(X, y)\nprint(f'Accuracy: {acc:.4f}')\n</code></pre>"},{"location":"models/cart/dt_cls/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model._history</code>. This is particularly useful when <code>gnostic_weights=True</code> to observe how sample weights evolve over iterations.</p>"},{"location":"models/cart/dt_cls/#notes","title":"Notes","text":"<ul> <li>This model is resilient to outliers due to the iterative gnostic reweighting mechanism.</li> <li>Suitable for datasets where data quality varies or label noise is present.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/cart/dt_reg/","title":"GnosticDecisionTreeRegressor: Robust Decision Tree with Machine Gnostics","text":"<p>The <code>GnosticDecisionTreeRegressor</code> implements a single Decision Tree Regressor enhanced with iterative gnostic reweighting. It is designed to handle outliers and varying data quality by identifying and down-weighting samples with high gnostic residual or uncertainty, fitting a robust model directly to the data.</p>"},{"location":"models/cart/dt_reg/#overview","title":"Overview","text":"<p>Machine Gnostics <code>GnosticDecisionTreeRegressor</code> combines the interpretability of decision trees with the robustness of Mathematical Gnostics. It iteratively refines sample weights based on regression residuals to improve prediction accuracy in the presence of noise and outliers.</p> <ul> <li>Robust Single Tree: Fits a decision tree that is robust to outliers by down-weighting them.</li> <li>Iterative Refinement: Updates sample weights based on residuals over multiple iterations.</li> <li>Interpretable: Maintains the interpretability of a single decision tree.</li> <li>Data Quality Handling: Automatically adjusts to data with outliers or non-Gaussian noise.</li> <li>Event-Level Modeling: Handles uncertainty at the level of individual data events.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cart/dt_reg/#key-features","title":"Key Features","text":"<ul> <li>Robust regression using iterative gnostic reweighting</li> <li>Iterative refinement of sample weights</li> <li>Configurable tree depth and splitting criteria</li> <li>Identifies and handles outliers automatically</li> <li>Convergence-based early stopping</li> <li>Training history tracking for analysis</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cart/dt_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>max_depth</code> <code>int</code> <code>None</code> Maximum depth of the tree. <code>min_samples_split</code> <code>int</code> <code>2</code> Minimum samples to split a node. <code>gnostic_weights</code> <code>bool</code> <code>True</code> Whether to use iterative gnostic weights. <code>max_iter</code> <code>int</code> <code>10</code> Maximum gnostic iterations. <code>tolerance</code> <code>float</code> <code>1e-4</code> Convergence tolerance. <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative). <code>verbose</code> <code>bool</code> <code>False</code> Verbosity. <code>random_state</code> <code>int</code> <code>None</code> Random seed. <code>history</code> <code>bool</code> <code>True</code> Whether to record training history. <code>scale</code> <code>str</code> <code>'auto'</code> Scaling method for input features. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected."},{"location":"models/cart/dt_reg/#attributes","title":"Attributes","text":"<ul> <li>gnostic_weights: <code>bool</code><ul> <li>Whether iterative gnostic weights were used.</li> </ul> </li> <li>max_depth: <code>int</code><ul> <li>Maximum depth of the tree.</li> </ul> </li> <li>max_iter: <code>int</code><ul> <li>Maximum number of iterations used.</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (if enabled).</li> </ul> </li> <li>tolerance, data_form, verbose, random_state, scale, early_stopping<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_reg/#methods","title":"Methods","text":""},{"location":"models/cart/dt_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the model to the data.</p> <p>This method trains the decision tree regressor using the provided input features and target values. If <code>gnostic_weights</code> is True, it iteratively refines the model by reweighting samples based on gnostic residuals.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Target values.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>GnosticDecisionTreeRegressor</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_reg/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict outcomes for new data.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted target values.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_reg/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Compute the robust (gnostic) R\u00b2 score for the model.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>True target values.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Robust R\u00b2 score of the model.</li> </ul> </li> </ul>"},{"location":"models/cart/dt_reg/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cart/dt_reg/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>GnosticDecisionTreeRegressor</code> with loaded parameters.</p>"},{"location":"models/cart/dt_reg/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import GnosticDecisionTreeRegressor\n\n# Generate synthetic data with outliers\nnp.random.seed(42)\nX = np.random.rand(100, 1) * 10\n# True relation: y = 2x + 1\ny = 2 * X.ravel() + 1 + np.random.normal(0, 0.5, 100)\n# Add strong outliers\ny[::10] += 20  \n\n# Initialize and fit the robust tree model\nmodel = GnosticDecisionTreeRegressor(\n    max_depth=5,\n    gnostic_weights=True,\n    max_iter=10,\n    verbose=True\n)\nmodel.fit(X, y)\n\n# Make predictions\npreds = model.predict(X[:5])\nprint(\"Predictions:\", preds)\n\n# Score\nr2 = model.score(X, y)\nprint(f\"Robust R2: {r2:.4f}\")\n</code></pre>"},{"location":"models/cart/dt_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model._history</code>. This is particularly useful when <code>gnostic_weights=True</code> to observe how sample weights evolve over iterations.</p>"},{"location":"models/cart/dt_reg/#notes","title":"Notes","text":"<ul> <li>This model is resilient to outliers due to the iterative gnostic reweighting mechanism.</li> <li>Provides interpretability similar to a standard decision tree but with enhanced robustness.</li> </ul> <p>Author: Nirmal Parmar </p>"},{"location":"models/cart/gb_cls/","title":"GnosticBoostingClassifier: Robust Gradient Boosting with Machine Gnostics","text":"<p>The <code>GnosticBoostingClassifier</code> integrates the power of Gradient Boosting (via XGBoost) with the robustness of Mathematical Gnostics. It employs an iterative reweighting mechanism that uses gnostic loss functions to assess data quality, allowing the model to autonomously down-weight outliers and noisy samples during training.</p>"},{"location":"models/cart/gb_cls/#overview","title":"Overview","text":"<p>Machine Gnostics <code>GnosticBoostingClassifier</code> combines state-of-the-art gradient boosting with rigorous error handling. By iteratively refining sample weights based on gnostic criteria (like rentropy and information loss), it achieves superior stability and accuracy in datasets with label noise or outliers.</p> <ul> <li>Robust Boosting: Upgrades standard XGBoost with gnostic error modeling.</li> <li>Iterative Refinement: Optimizes sample weights over multiple iterations to minimize gnostic loss.</li> <li>Data Quality Handling: Automatically identifies and down-weights low-fidelity or mislabeled samples.</li> <li>Configurable: Supports standard boosting parameters along with gnostic settings.</li> <li>Event-Level Modeling: Handles uncertainty at the level of individual data events.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cart/gb_cls/#key-features","title":"Key Features","text":"<ul> <li>Robust classification using iterative gnostic reweighting</li> <li>Integration with XGBoost for high-performance gradient boosting</li> <li>Customizable gnostic loss functions (<code>'hi'</code>, etc.)</li> <li>Convergence-based early stopping</li> <li>Training history tracking for detailed analysis</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cart/gb_cls/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_estimators</code> <code>int</code> <code>100</code> Number of boosting rounds. <code>max_depth</code> <code>int</code> <code>6</code> Maximum tree depth for base learners. <code>learning_rate</code> <code>float</code> <code>0.3</code> Boosting learning rate. <code>max_iter</code> <code>int</code> <code>10</code> Maximum number of gnostic reweighting iterations. <code>tolerance</code> <code>float</code> <code>1e-4</code> Convergence tolerance for early stopping. <code>mg_loss</code> <code>str</code> <code>'hi'</code> Gnostic loss function to use. <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative). <code>verbose</code> <code>bool</code> <code>False</code> Verbosity. <code>random_state</code> <code>int</code> <code>None</code> Random seed. <code>history</code> <code>bool</code> <code>True</code> Whether to record training history. <code>scale</code> <code>str</code> | <code>float</code> <code>'auto'</code> Scaling method for input features. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected. <code>gnostic_characteristics</code> <code>bool</code> <code>False</code> Whether to compute extended gnostic metrics."},{"location":"models/cart/gb_cls/#attributes","title":"Attributes","text":"<ul> <li>model: <code>Any</code><ul> <li>The underlying XGBoost classifier instance.</li> </ul> </li> <li>weights: <code>np.ndarray</code><ul> <li>The final calibrated sample weights.</li> </ul> </li> <li>classes_: <code>np.ndarray</code><ul> <li>Class labels.</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (loss, entropy, weights).</li> </ul> </li> <li>n_estimators, max_depth, learning_rate, max_iter, tolerance<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_cls/#methods","title":"Methods","text":""},{"location":"models/cart/gb_cls/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the Gnostic Boosting model to the training data.</p> <p>This method trains the model using an iterative process. In each iteration, an XGBoost classifier is trained, predictions are made, and sample weights are updated based on the gnostic loss of the residuals. This process repeats until convergence or <code>max_iter</code>.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Target labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>GnosticBoostingClassifier</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_cls/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict class labels for input samples.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted class labels.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_cls/#predict_probamodel_input","title":"<code>predict_proba(model_input)</code>","text":"<p>Predict class probabilities for input samples.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_proba: <code>np.ndarray</code> of shape <code>(n_samples, n_classes)</code><ul> <li>Predicted class probabilities.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_cls/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Return the mean accuracy on the given test data and labels.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>True class labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Accuracy score of the model predictions.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_cls/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cart/gb_cls/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>GnosticBoostingClassifier</code> with loaded parameters.</p>"},{"location":"models/cart/gb_cls/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>from machinegnostics.models import GnosticBoostingClassifier\n\n# Initialize model\nmodel = GnosticBoostingClassifier(\n    n_estimators=50,\n    learning_rate=0.1,\n    max_iter=5,\n    verbose=True\n)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict\npreds = model.predict(X[:5])\nprint(\"Predictions:\", preds)\n\n# Score\nacc = model.score(X, y)\nprint(f'Accuracy: {acc:.4f}')\n</code></pre>"},{"location":"models/cart/gb_cls/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model._history</code>.  This includes metrics like loss, residual entropy, and weight statistics, allowing users to visualize how the model adapts to the data quality over time.</p>"},{"location":"models/cart/gb_cls/#notes","title":"Notes","text":"<ul> <li>XGBoost Requirement: This model requires <code>xgboost</code> to be installed in the environment.</li> <li>Robustness: The iterative gnostic weighting makes this model particularly robust against training data with label noise.</li> </ul> <p>Author: Machine Gnostics Team Date: 2026</p>"},{"location":"models/cart/gb_reg/","title":"GnosticBoostingRegressor: Robust Boosting with Machine Gnostics","text":"<p>The <code>GnosticBoostingRegressor</code> extends the Gradient Boosting (XGBoost) approach by integrating Mathematical Gnostics principles. It employs an iterative reweighting scheme that assesses the quality of each data sample based on the residuals of the previous iteration's model. This allows the model to autonomously down-weight outliers and noise, resulting in a robust regression model capable of handling contaminated data.</p> <p>This implementation wraps the XGBoost library, combining its high-performance gradient boosting with the robust statistical framework of Machine Gnostics.</p>"},{"location":"models/cart/gb_reg/#overview","title":"Overview","text":"<p>Machine Gnostics <code>GnosticBoostingRegressor</code> leverages the ensemble power of Gradient Boosting Trees along with the robust weighting of Machine Gnostics. It is particularly effective for regression tasks where the data may contain outliers or follow non-standard distributions.</p> <ul> <li>Robustness to Outliers: Automatically identifies and down-weights anomalous samples during training.</li> <li>Iterative Refinement: Optimizes sample weights over multiple iterations until convergence.</li> <li>Boosted Performance: Uses Gradient Boosting (XGBoost) as the underlying estimator for state-of-the-art performance.</li> <li>Event-Level Modeling: Handles uncertainty at the level of individual data events.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cart/gb_reg/#key-features","title":"Key Features","text":"<ul> <li>Robust regression using iterative gnostic reweighting</li> <li>Iterative refinement of sample weights</li> <li>XGBoost hyperparameters (n_estimators, max_depth, learning_rate)</li> <li>Identifies and handles outliers automatically</li> <li>Convergence-based early stopping</li> <li>Training history tracking for analysis</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cart/gb_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_estimators</code> <code>int</code> <code>100</code> The number of boosting rounds (trees). <code>max_depth</code> <code>int</code> <code>6</code> Maximum depth of the base learners. <code>learning_rate</code> <code>float</code> <code>0.3</code> Boosting learning rate (eta). <code>gnostic_weights</code> <code>bool</code> <code>True</code> Whether to use iterative gnostic weights. <code>max_iter</code> <code>int</code> <code>10</code> Maximum gnostic iterations. <code>tolerance</code> <code>float</code> <code>1e-4</code> Convergence tolerance. <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative). <code>verbose</code> <code>bool</code> <code>False</code> Verbosity. <code>random_state</code> <code>int</code> <code>None</code> Random seed. <code>history</code> <code>bool</code> <code>True</code> Whether to record training history. <code>scale</code> <code>str</code> | <code>float</code> <code>'auto'</code> Scaling method for input features. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected. <code>**kwargs</code> <code>dict</code> Additional arguments passed to <code>xgboost.XGBRegressor</code>."},{"location":"models/cart/gb_reg/#attributes","title":"Attributes","text":"<ul> <li>weights: <code>np.ndarray</code><ul> <li>The final calibrated sample weights assigned to the training data.</li> </ul> </li> <li>model: <code>xgboost.XGBRegressor</code><ul> <li>The underlying fitted XGBoost model.</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (loss, entropy, weights).</li> </ul> </li> <li>n_estimators, gnostic_weights, max_depth, learning_rate<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_reg/#methods","title":"Methods","text":""},{"location":"models/cart/gb_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the Gnostic Boosting model to the training data.</p> <p>This method trains the boosting regressor. If <code>gnostic_weights</code> is True, it iteratively refines the model by reweighting samples based on gnostic residuals to down-weight outliers.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Target values.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>GnosticBoostingRegressor</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_reg/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict target values for input samples using the boosted ensemble.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted target values.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_reg/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Return the robust (gnostic) coefficient of determination R\u00b2 of the prediction.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>True target values.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Robust R\u00b2 score of the model.</li> </ul> </li> </ul>"},{"location":"models/cart/gb_reg/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cart/gb_reg/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>GnosticBoostingRegressor</code> with loaded parameters.</p>"},{"location":"models/cart/gb_reg/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import GnosticBoostingRegressor\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.rand(100, 5)\n# Target is sum of features + noise\ny = np.sum(X, axis=1) + np.random.normal(0, 0.1, 100)\n\n# Add strong outliers\ny[::10] += 20  \n\n# Initialize and fit the robust boosting model\nmodel = GnosticBoostingRegressor(\n    n_estimators=50,\n    max_depth=3,\n    learning_rate=0.1,\n    gnostic_weights=True,\n    max_iter=5,\n    verbose=True\n)\nmodel.fit(X, y)\n\n# Make predictions\npreds = model.predict(X[:5])\nprint(\"Predictions:\", preds)\n\n# Score\nr2 = model.score(X, y)\nprint(f\"Robust R2: {r2:.4f}\")\n</code></pre>"},{"location":"models/cart/gb_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model._history</code>.  This helps in analyzing how the model identifies and down-weights noisy samples over time.</p>"},{"location":"models/cart/gb_reg/#notes","title":"Notes","text":"<ul> <li>This model requires <code>xgboost</code> to be installed.</li> <li>It is particularly effective for datasets where outliers would otherwise skew the boosting process significantly.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/cart/rf_cls/","title":"GnosticRandomForestClassifier: Robust Random Forest with Machine Gnostics","text":"<p>The <code>GnosticRandomForestClassifier</code> extends the standard random forest approach by integrating Mathematical Gnostics principles. It employs an iterative reweighting scheme that assesses the quality of each data sample based on the consistency of the previous iteration's model predictions. This allows the forest to autonomously down-weight outliers and noise (mislabeled samples), resulting in a more robust predictive model.</p>"},{"location":"models/cart/rf_cls/#overview","title":"Overview","text":"<p>Machine Gnostics <code>GnosticRandomForestClassifier</code> is designed for robust classification in the presence of label noise and outliers. By combining the ensemble power of Random Forests with the robust weighting of Machine Gnostics, it offers superior stability and accuracy in challenging data environments.</p> <ul> <li>Robustness to Label Noise: Automatically identifies and down-weights samples with low classification confidence.</li> <li>Iterative Refinement: Optimizes sample weights over multiple iterations until convergence.</li> <li>Ensemble Learning: Leverages multiple decision trees for reduced variance and improved generalization.</li> <li>Event-Level Modeling: Handles uncertainty at the level of individual data events.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cart/rf_cls/#key-features","title":"Key Features","text":"<ul> <li>Robust classification using iterative gnostic reweighting</li> <li>Iterative refinement of sample weights</li> <li>Standard Random Forest hyperparameters (n_estimators, max_depth, etc.)</li> <li>Identifies and handles outliers/mislabeled data automatically</li> <li>Convergence-based early stopping</li> <li>Training history tracking for analysis</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cart/rf_cls/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_estimators</code> <code>int</code> <code>100</code> The number of trees in the forest. <code>max_depth</code> <code>int</code> <code>None</code> Maximum depth of the tree. <code>min_samples_split</code> <code>int</code> <code>2</code> Minimum samples to split a node. <code>gnostic_weights</code> <code>bool</code> <code>True</code> Whether to use iterative gnostic weights. <code>max_iter</code> <code>int</code> <code>10</code> Maximum gnostic iterations. <code>tolerance</code> <code>float</code> <code>1e-4</code> Convergence tolerance. <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative). <code>verbose</code> <code>bool</code> <code>False</code> Verbosity. <code>random_state</code> <code>int</code> <code>None</code> Random seed. <code>history</code> <code>bool</code> <code>True</code> Whether to record training history. <code>scale</code> <code>str</code> | <code>float</code> <code>'auto'</code> Scaling method for input features. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected."},{"location":"models/cart/rf_cls/#attributes","title":"Attributes","text":"<ul> <li>weights: <code>np.ndarray</code><ul> <li>The final calibrated sample weights assigned to the training data.</li> </ul> </li> <li>trees: <code>list</code><ul> <li>The list of underlying decision trees (estimators) that make up the forest.</li> </ul> </li> <li>classes_: <code>np.ndarray</code><ul> <li>The classes labels.</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (loss, entropy, weights).</li> </ul> </li> <li>n_estimators, gnostic_weights, max_depth, max_iter, tolerance<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_cls/#methods","title":"Methods","text":""},{"location":"models/cart/rf_cls/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the Gnostic Forest model to the training data.</p> <p>This method trains the random forest classifier. If <code>gnostic_weights</code> is True, it iteratively refines the model by reweighting samples based on gnostic residuals to down-weight outliers.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Target labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>GnosticRandomForestClassifier</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_cls/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict class labels for input samples.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted class labels.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_cls/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Return the mean accuracy on the given test data and labels.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>True class labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Accuracy score of the model predictions.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_cls/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cart/rf_cls/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>GnosticRandomForestClassifier</code> with loaded parameters.</p>"},{"location":"models/cart/rf_cls/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>from machinegnostics.models import GnosticRandomForestClassifier\n\n# Initialize model\nmodel = GnosticRandomForestClassifier(\n    n_estimators=50,\n    gnostic_weights=True,\n    max_iter=5,\n    verbose=True\n)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict\npreds = model.predict(X[:5])\nprint(\"Predictions:\", preds)\n\n# Score\nacc = model.score(X, y)\nprint(f'Accuracy: {acc:.4f}')\n</code></pre>"},{"location":"models/cart/rf_cls/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model._history</code>.  This helps in analyzing how the model identifies and down-weights noisy samples over time.</p>"},{"location":"models/cart/rf_cls/#notes","title":"Notes","text":"<ul> <li>This model is particularly effective when the training labels contain noise or errors.</li> <li>The <code>gnostic_weights</code> mechanism allows the forest to \"self-clean\" the training data during the fitting process.</li> </ul> <p>Author: Nirmal Parmar Date: 2026</p>"},{"location":"models/cart/rf_reg/","title":"GnosticRandomForestRegressor: Robust Random Forest with Machine Gnostics","text":"<p>The <code>GnosticRandomForestRegressor</code> extends the standard random forest approach by integrating Mathematical Gnostics principles. It employs an iterative reweighting scheme that assesses the quality of each data sample based on the residuals of the previous iteration's model. This allows the forest to autonomously down-weight outliers and noise, resulting in a more robust predictive model.</p>"},{"location":"models/cart/rf_reg/#overview","title":"Overview","text":"<p>Machine Gnostics <code>GnosticRandomForestRegressor</code> combines the ensemble power of Random Forests with the robust weighting of Machine Gnostics. This makes it exceptionally capable in regression tasks where the training data contains outliers or follows non-Gaussian noise distributions.</p> <ul> <li>Robustness to Outliers: Automatically identifies and down-weights anomalous samples during training.</li> <li>Iterative Refinement: Optimizes sample weights over multiple iterations until convergence.</li> <li>Ensemble Learning: Leverages multiple decision trees for reduced variance and improved generalization.</li> <li>Event-Level Modeling: Handles uncertainty at the level of individual data events.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cart/rf_reg/#key-features","title":"Key Features","text":"<ul> <li>Robust regression using iterative gnostic reweighting</li> <li>Iterative refinement of sample weights</li> <li>Standard Random Forest hyperparameters (n_estimators, max_depth, etc.)</li> <li>Identifies and handles outliers automatically</li> <li>Convergence-based early stopping</li> <li>Training history tracking for analysis</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cart/rf_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_estimators</code> <code>int</code> <code>100</code> The number of trees in the forest. <code>max_depth</code> <code>int</code> <code>None</code> Maximum depth of the tree. <code>min_samples_split</code> <code>int</code> <code>2</code> Minimum samples to split a node. <code>gnostic_weights</code> <code>bool</code> <code>True</code> Whether to use iterative gnostic weights. <code>max_iter</code> <code>int</code> <code>10</code> Maximum gnostic iterations. <code>tolerance</code> <code>float</code> <code>1e-4</code> Convergence tolerance. <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative). <code>verbose</code> <code>bool</code> <code>False</code> Verbosity. <code>random_state</code> <code>int</code> <code>None</code> Random seed. <code>history</code> <code>bool</code> <code>True</code> Whether to record training history. <code>scale</code> <code>str</code> | <code>float</code> <code>'auto'</code> Scaling method for input features. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected."},{"location":"models/cart/rf_reg/#attributes","title":"Attributes","text":"<ul> <li>weights: <code>np.ndarray</code><ul> <li>The final calibrated sample weights assigned to the training data.</li> </ul> </li> <li>trees: <code>list</code><ul> <li>The list of underlying regression trees (estimators) that make up the forest.</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (loss, entropy, weights).</li> </ul> </li> <li>n_estimators, gnostic_weights, max_depth, max_iter, tolerance<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_reg/#methods","title":"Methods","text":""},{"location":"models/cart/rf_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the Gnostic Forest model to the training data.</p> <p>This method trains the random forest classifier. If <code>gnostic_weights</code> is True, it iteratively refines the model by reweighting samples based on gnostic residuals to down-weight outliers.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Target values.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>GnosticRandomForestRegressor</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_reg/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict target values for input samples.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted target values.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_reg/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Return the robust (gnostic) coefficient of determination R\u00b2 of the prediction.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>True target values.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Robust R\u00b2 score of the model.</li> </ul> </li> </ul>"},{"location":"models/cart/rf_reg/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cart/rf_reg/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>GnosticRandomForestRegressor</code> with loaded parameters.</p>"},{"location":"models/cart/rf_reg/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import GnosticRandomForestRegressor\n\n# Generate synthetic data with outliers\nnp.random.seed(42)\nX = np.random.rand(100, 1) * 10\n# True relation: y = 2x + 1\ny = 2 * X.ravel() + 1 + np.random.normal(0, 0.5, 100)\n# Add strong outliers\ny[::10] += 20  \n\n# Initialize and fit the robust forest model\nmodel = GnosticRandomForestRegressor(\n    n_estimators=50,\n    gnostic_weights=True,\n    max_iter=5,\n    verbose=True\n)\nmodel.fit(X, y)\n\n# Make predictions\npreds = model.predict(X[:5])\nprint(\"Predictions:\", preds)\n\n# Score\nr2 = model.score(X, y)\nprint(f\"Robust R2: {r2:.4f}\")\n</code></pre>"},{"location":"models/cart/rf_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model._history</code>.  This helps in analyzing how the model identifies and down-weights noisy samples over time.</p>"},{"location":"models/cart/rf_reg/#notes","title":"Notes","text":"<ul> <li>This model is particularly effective when the training data contains localized outliers or non-Gaussian noise.</li> <li>The <code>gnostic_weights</code> mechanism allows the forest to \"self-clean\" the data during training.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/cls/log_reg/","title":"LogisticRegressor: Robust Logistic Regression with Machine Gnostics","text":"<p>The <code>LogisticRegressor</code> is a robust logistic regression model built on the Machine Gnostics framework. It provides feature-rich binary classification with polynomial feature expansion, custom loss functions, and detailed history tracking, designed to be resilient to outliers and non-Gaussian noise.</p>"},{"location":"models/cls/log_reg/#overview","title":"Overview","text":"<p>Machine Gnostics <code>LogisticRegressor</code> brings deterministic, event-level modeling to binary classification. By leveraging gnostic algebra and geometry, it provides robust, interpretable, and reproducible results, even in challenging scenarios.</p> <ul> <li>Deterministic &amp; Finite: No randomness or probability; all computations are reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events.</li> <li>Robust: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> <li>Flexible: Supports polynomial feature expansion and multiple probability estimation methods.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cls/log_reg/#key-features","title":"Key Features","text":"<ul> <li>Fits a logistic regression model for binary classification</li> <li>Polynomial feature expansion up to a user-specified degree</li> <li>Robust to outliers and non-Gaussian noise</li> <li>Choice of probability estimation method: 'gnostic' or standard 'sigmoid'</li> <li>Iterative optimization with early stopping and convergence tolerance</li> <li>Adaptive sample weighting</li> <li>Training history tracking for analysis and visualization</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cls/log_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>degree</code> <code>int</code> <code>1</code> Degree of polynomial features to use for input expansion. <code>max_iter</code> <code>int</code> <code>100</code> Maximum number of iterations for the optimization algorithm. <code>tolerance</code> <code>float</code> <code>1e-2</code> Tolerance for convergence. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected. <code>verbose</code> <code>bool</code> <code>False</code> If True, prints detailed logs during training. <code>scale</code> <code>str</code> | <code>int</code> | <code>float</code> <code>'auto'</code> Scaling method for input features. <code>data_form</code> <code>str</code> <code>'a'</code> Internal data representation format. <code>gnostic_characteristics</code> <code>bool</code> <code>False</code> If True, computes and records gnostic characteristics. <code>history</code> <code>bool</code> <code>True</code> If True, records the optimization history for analysis."},{"location":"models/cls/log_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>np.ndarray</code><ul> <li>Fitted model coefficients after training.</li> </ul> </li> <li>weights: <code>np.ndarray</code><ul> <li>Sample weights used during training.</li> </ul> </li> <li>params: <code>list of dict</code><ul> <li>List of model parameters (for compatibility and inspection).</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (loss, coefficients, entropy, etc.).</li> </ul> </li> <li>degree, max_iter, tolerance, early_stopping, verbose, scale, data_form, gnostic_characteristics<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cls/log_reg/#methods","title":"Methods","text":""},{"location":"models/cls/log_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the logistic regression model to the data.</p> <p>This method trains the logistic regression model using the provided input features and target labels. It supports polynomial feature expansion and early stopping based on convergence criteria.</p> <p>Parameters</p> <ul> <li>X: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for training.</li> </ul> </li> <li>y: <code>array-like</code> of shape <code>(n_samples,)</code><ul> <li>Target labels for training (binary 0 or 1).</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>LogisticRegressor</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cls/log_reg/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict class labels for new data.</p> <p>Parameters</p> <ul> <li>model_input: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted class labels (0 or 1).</li> </ul> </li> </ul>"},{"location":"models/cls/log_reg/#predict_probamodel_input","title":"<code>predict_proba(model_input)</code>","text":"<p>Predict class probabilities for new data.</p> <p>Parameters</p> <ul> <li>model_input: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for probability prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_proba: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted probabilities for the positive class.</li> </ul> </li> </ul>"},{"location":"models/cls/log_reg/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Compute the F1 score of the model on given data.</p> <p>Parameters</p> <ul> <li>X: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>array-like</code> of shape <code>(n_samples,)</code><ul> <li>True binary labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>F1 score of the model predictions.</li> </ul> </li> </ul>"},{"location":"models/cls/log_reg/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cls/log_reg/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>LogisticRegressor</code> with loaded parameters.</p>"},{"location":"models/cls/log_reg/#example-usage","title":"Example Usage","text":"PythonPlotOutput <pre><code>import numpy as np\nfrom machinegnostics.models import LogisticRegressor\n\n# Generate two-moons dataset (non-linearly separable)\ndef make_moons(n_samples=30, noise=0.15):\n    n_samples_out = n_samples // 2\n    n_samples_in = n_samples - n_samples_out\n\n    # First moon\n    outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples_out))\n    outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples_out))\n\n    # Second moon\n    inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, n_samples_in))\n    inner_circ_y = -np.sin(np.linspace(0, np.pi, n_samples_in)) - 0.5\n\n    X = np.vstack([\n        np.stack([outer_circ_x, outer_circ_y], axis=1),\n        np.stack([inner_circ_x, inner_circ_y], axis=1)\n    ])\n    y = np.array([0] * n_samples_out + [1] * n_samples_in)\n\n    # Add noise\n    X += np.random.normal(scale=noise, size=X.shape)\n\n    return X, y\n\n# Fit the model with polynomial features (degree=3 for non-linear boundary)\nmodel = LogisticRegressor(degree=3, verbose=False, early_stopping=True, tolerance=0.001)\nmodel.fit(X, y)\n\n# Make predictions\ny_pred = model.predict(X)\ny_proba = model.predict_proba(X)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\n# Create decision boundary plot\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                    np.linspace(y_min, y_max, 200))\n\n# Predict on grid\ngrid = np.c_[xx.ravel(), yy.ravel()]\nZ = model.predict_proba(grid)\nZ = Z.reshape(xx.shape)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.contourf(xx, yy, Z, levels=20, cmap='RdYlGn', alpha=0.6)\nplt.colorbar(label='Predicted Probability (Class 1)')\nplt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n\n# Plot data points\nplt.scatter(X[y==0, 0], X[y==0, 1], c='red', s=80, edgecolors='k', \n            label='Class 0', marker='o', alpha=0.8)\nplt.scatter(X[y==1, 0], X[y==1, 1], c='green', s=80, edgecolors='k', \n            label='Class 1', marker='s', alpha=0.8)\n\nplt.xlabel('Feature 1', fontsize=11)\nplt.ylabel('Feature 2', fontsize=11)\nplt.title('Decision Boundary with Probability Contours', fontsize=12, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/cls/log_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model.params</code> and <code>model._history</code>. Each entry contains details like loss, coefficients, and entropy, enabling in-depth analysis of the learning process.</p>"},{"location":"models/cls/log_reg/#notes","title":"Notes","text":"<ul> <li>The model supports only binary classification tasks.</li> <li>More information on gnostic characteristics can be found in the Machine Gnostics documentation.</li> </ul> <p>Author: Nirmal Parmar</p>"},{"location":"models/cls/multi_class/","title":"MulticlassClassifier: Robust Multiclass Classification with Machine Gnostics","text":"<p>The <code>MulticlassClassifier</code> is a robust multiclass classification model built on the Machine Gnostics framework. It provides feature-rich classification with polynomial feature expansion, softmax activation, and detailed history tracking, designed to be resilient to outliers and improve model stability.</p>"},{"location":"models/cls/multi_class/#overview","title":"Overview","text":"<p>Machine Gnostics <code>MulticlassClassifier</code> brings deterministic, event-level modeling to multiclass problems. By leveraging gnostic algebra and geometry, it provides robust, interpretable, and reproducible results.</p> <ul> <li>Deterministic &amp; Finite: No randomness or probability; all computations are reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events.</li> <li>Robust: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> <li>Flexible: Supports polynomial feature expansion and automatic class detection.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/cls/multi_class/#key-features","title":"Key Features","text":"<ul> <li>Multiclass classification using softmax activation</li> <li>Polynomial feature expansion up to a user-specified degree</li> <li>Gnostic weights for robust handling of outliers</li> <li>Automatic detection of number of classes</li> <li>Iterative optimization with early stopping and convergence tolerance</li> <li>Training history tracking for analysis and visualization</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/cls/multi_class/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>degree</code> <code>int</code> <code>1</code> Degree of polynomial features to use for input expansion. <code>max_iter</code> <code>int</code> <code>100</code> Maximum number of iterations for the optimization algorithm. <code>tolerance</code> <code>float</code> <code>1e-1</code> Tolerance for convergence. <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop training early if convergence is detected. <code>verbose</code> <code>bool</code> <code>False</code> If True, prints detailed logs during training. <code>scale</code> <code>str</code> | <code>int</code> | <code>float</code> <code>'auto'</code> Scaling method for gnostic weight calculations. <code>data_form</code> <code>str</code> <code>'a'</code> Internal data representation format. <code>gnostic_characteristics</code> <code>bool</code> <code>False</code> If True, computes and records gnostic characteristics. <code>history</code> <code>bool</code> <code>True</code> If True, records the optimization history for analysis."},{"location":"models/cls/multi_class/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>np.ndarray</code><ul> <li>Fitted model coefficients after training.</li> </ul> </li> <li>weights: <code>np.ndarray</code><ul> <li>Sample weights used during training.</li> </ul> </li> <li>num_classes: <code>int</code><ul> <li>Number of unique classes in the training data.</li> </ul> </li> <li>cross_entropy_loss: <code>float</code><ul> <li>Cross-entropy loss computed during training.</li> </ul> </li> <li>params: <code>list of dict</code><ul> <li>List of model parameters (for compatibility and inspection).</li> </ul> </li> <li>_history: <code>list</code><ul> <li>List of dictionaries containing training history (loss, coefficients, entropy, etc.).</li> </ul> </li> <li>degree, max_iter, tolerance, early_stopping, verbose, scale, data_form, gnostic_characteristics<ul> <li>Configuration parameters as set at initialization.</li> </ul> </li> </ul>"},{"location":"models/cls/multi_class/#methods","title":"Methods","text":""},{"location":"models/cls/multi_class/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit the multiclass classifier to the data.</p> <p>This method trains the classifier using the provided input features and target labels. It supports polynomial feature expansion, softmax activation, and early stopping.</p> <p>Parameters</p> <ul> <li>X: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for training.</li> </ul> </li> <li>y: <code>array-like</code> of shape <code>(n_samples,)</code><ul> <li>Target labels for training (class integers starting from 0).</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>MulticlassClassifier</code><ul> <li>Returns the fitted model instance for chaining.</li> </ul> </li> </ul>"},{"location":"models/cls/multi_class/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict class labels for new data.</p> <p>Parameters</p> <ul> <li>model_input: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_pred: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Predicted class labels (integers).</li> </ul> </li> </ul>"},{"location":"models/cls/multi_class/#predict_probamodel_input","title":"<code>predict_proba(model_input)</code>","text":"<p>Predict class probabilities for new data.</p> <p>Parameters</p> <ul> <li>model_input: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input data for probability prediction.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>y_proba: <code>np.ndarray</code> of shape <code>(n_samples, n_classes)</code><ul> <li>Predicted probabilities for each class.</li> </ul> </li> </ul>"},{"location":"models/cls/multi_class/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>Compute the accuracy score of the model on given data.</p> <p>Parameters</p> <ul> <li>X: <code>array-like</code> or <code>DataFrame</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features for evaluation.</li> </ul> </li> <li>y: <code>array-like</code> of shape <code>(n_samples,)</code><ul> <li>True class labels.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Accuracy score of the model predictions.</li> </ul> </li> </ul>"},{"location":"models/cls/multi_class/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cls/multi_class/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>MulticlassClassifier</code> with loaded parameters.</p>"},{"location":"models/cls/multi_class/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import MulticlassClassifier\n\n# Generate synthetic multiclass data (3 classes)\nnp.random.seed(42)\nn_samples = 150\nX = np.random.randn(n_samples, 2)\n# Assign classes based on regions\ny = np.zeros(n_samples, dtype=int)\ny[X[:, 0] &gt; 0.5] = 1\ny[X[:, 1] &lt; -0.5] = 2\n\n# Initialize model\nmodel = MulticlassClassifier(\n    degree=2,\n    max_iter=100,\n    verbose=True,\n    tolerance=1e-3\n)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Predict\nX_test = np.array([[1.0, 1.0], [-1.0, -1.0], [0.0, -1.0]])\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)\n\n# Score\nacc = model.score(X, y)\nprint(f'Accuracy: {acc:.4f}')\n</code></pre>"},{"location":"models/cls/multi_class/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model.params</code> and <code>model._history</code>. Each entry contains details like cross-entropy loss, coefficients, and entropy.</p>"},{"location":"models/cls/multi_class/#notes","title":"Notes","text":"<ul> <li>The model automatically detects the number of classes from the training data.</li> <li>Uses softmax activation for multiclass probability estimation.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/cluster/glc/","title":"GnosticLocalClustering: Density-Based Clustering","text":"<p>The <code>GnosticLocalClustering</code> model performs density-based clustering using Estimating Local Distribution Functions (ELDF). Unlike distance-based methods like K-Means, this approach identifies clusters as high-density regions (modes) separated by low-density regions (valleys) in the estimated probability density function (PDF). This allows it to naturaly determine the number of clusters and handle non-convex cluster shapes.</p>"},{"location":"models/cluster/glc/#overview","title":"Overview","text":"<p>Machine Gnostics <code>GnosticLocalClustering</code> builds a continuous probability density Estimate (ELDF) of the data and analyzes its topology. - Peaks (Modes): serve as cluster centroids. - Valleys (Minima): serve as the boundaries between clusters.</p> <p>It includes mechanisms to automatically determine the optimal scale parameter (\\(S\\)) which controls the \"resolution\" of the clustering, effectively deciding how detailed the cluster structure should be.</p> <ul> <li>Automatic Cluster Detection: No need to specify <code>n_clusters</code> beforehand; the model discovers them based on data density.</li> <li>Density-Based: Can separate clusters of arbitrary shape and density.</li> <li>Robustness: Inherits the robust properties of Gnostic Distribution Functions.</li> <li>Interpretability: Provides visualization of the underlying PDF, peaks, and boundaries.</li> </ul> <p>Dimensionality</p> <p>Currently, <code>GnosticLocalClustering</code> is optimized for 1D data (univariate clustering). If multi-dimensional data is provided, it is flattened.</p>"},{"location":"models/cluster/glc/#key-features","title":"Key Features","text":"<ul> <li>Density-based clustering using Gnostic ELDF</li> <li>Automatically determines the number of clusters</li> <li>Can search for optimal scale parameters (Grid Search or Auto)</li> <li>Identifies cluster boundaries (valleys) and centroids (peaks)</li> <li>Visualization method <code>plot()</code> included</li> <li>Robust to outliers</li> <li>Compatible with numpy arrays</li> </ul>"},{"location":"models/cluster/glc/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>start_S</code> <code>float</code> <code>0.1</code> Starting value for scale parameter \\(S\\) grid search. <code>end_S</code> <code>float</code> <code>2.0</code> Ending value for scale parameter \\(S\\) grid search. <code>step_S</code> <code>float</code> <code>0.1</code> Step size for \\(S\\). <code>varS</code> <code>bool</code> <code>False</code> Uses variable scale parameter (heteroscedasticity) if True. <code>auto_S</code> <code>bool</code> <code>True</code> If True, uses ELDF's internal auto-optimization for \\(S\\). <code>verbose</code> <code>bool</code> <code>False</code> Verbosity mode. <code>history</code> <code>bool</code> <code>True</code> Record search history (S, entropy, clusters). <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative)."},{"location":"models/cluster/glc/#attributes","title":"Attributes","text":"<ul> <li>centroids: <code>np.ndarray</code><ul> <li>Detected cluster modes (peaks of the PDF).</li> </ul> </li> <li>cluster_boundaries: <code>np.ndarray</code><ul> <li>Detected boundaries between clusters (valleys of the PDF).</li> </ul> </li> <li>labels: <code>np.ndarray</code><ul> <li>Cluster labels for each sample.</li> </ul> </li> <li>optimal_S: <code>float</code><ul> <li>The scale parameter value (\\(S\\)) of the selected best model.</li> </ul> </li> <li>best_model: <code>ELDF</code><ul> <li>The fitted ELDF model object.</li> </ul> </li> <li>results: <code>pd.DataFrame</code><ul> <li>History of the grid search (S values, residual entropy, n_clusters).</li> </ul> </li> </ul>"},{"location":"models/cluster/glc/#methods","title":"Methods","text":""},{"location":"models/cluster/glc/#fitx-ynone","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the density-based clustering model to the data.</p> <p>This process involves: 1.  Running an optimization loop (Auto or Grid) to find the best Scale Parameter \\(S\\). 2.  Estimating the PDF using ELDF. 3.  Identifying peaks (centroids) and valleys (boundaries). 4.  Minimizing Residual Entropy to select the best model.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code><ul> <li>Input data. (Flattened if &gt; 1D).</li> </ul> </li> <li>y: <code>Ignored</code><ul> <li>Not used.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>GnosticLocalClustering</code></li> </ul>"},{"location":"models/cluster/glc/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict cluster labels for new data. Labels are assigned based on which \"valley-bounded\" interval the data point falls into.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code><ul> <li>New data to predict.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>labels: <code>np.ndarray</code><ul> <li>Predicted cluster labels (integers 0, 1, ...).</li> </ul> </li> </ul>"},{"location":"models/cluster/glc/#scorex-ynone","title":"<code>score(X, y=None)</code>","text":"<p>Return the Residual Entropy of the fitted model.</p> <p>Unlike K-Means inertia, this metric measures the \"unexplained\" information in the data given the distribution model. Lower is better.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code><ul> <li>Input features.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Residual entropy.</li> </ul> </li> </ul>"},{"location":"models/cluster/glc/#plot","title":"<code>plot()</code>","text":"<p>Visualize the clustering results.</p> <ul> <li>Plots the estimated Gnostic PDF.</li> <li>Marks Peaks (Centroids) with red dots.</li> <li>Marks Valleys (Boundaries) with blue crosses and lines.</li> <li>If a grid search was performed, plots the Residual Entropy and Cluster Count vs \\(S\\).</li> </ul>"},{"location":"models/cluster/glc/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cluster/glc/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>GnosticLocalClustering</code> with loaded parameters.</p>"},{"location":"models/cluster/glc/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import GnosticLocalClustering\n\n# Generate multimodal data (3 mixed gaussians)\nnp.random.seed(42)\nX = np.concatenate([\n    np.random.normal(0, 0.5, 50),\n    np.random.normal(5, 0.5, 50),\n    np.random.normal(10, 0.5, 50)\n])\n\n# Initialize using Auto-S optimization\nmodel = GnosticLocalClustering(auto_S=True, verbose=True)\n\n# Fit\nmodel.fit(X)\n\n# Inspect\nprint(f\"Optimal S: {model.optimal_S:.2f}\")\nprint(f\"Detected Clusters: {model.n_clusters}\")\nprint(f\"Centroids (Peaks): \\n{model.centroids}\")\n\n# Predict\nlabels = model.predict(X[:5])\nprint(\"Labels:\", labels)\n\n# Plot results\n# model.plot() \n</code></pre>"},{"location":"models/cluster/glc/#notes","title":"Notes","text":"<ul> <li>Scale Parameter (\\(S\\)): This parameter is crucial. A small \\(S\\) results in a jagged PDF with many peaks (over-clustering), while a large \\(S\\) results in a smooth PDF with fewer peaks (under-clustering). The <code>auto_S=True</code> setting attempts to find an information-theoretic optimum.</li> <li>Boundaries: Data points exactly on a boundary (valley) are assigned to the cluster on the right (higher value).</li> </ul> <p>Author: Nirmal Parmar </p>"},{"location":"models/cluster/kmeans/","title":"KMeansClustering: Robust K-Means with Machine Gnostics","text":"<p>The <code>KMeansClustering</code> model performs clustering using robust, gnostic loss functions and adaptive sample weights. By integrating principles from Mathematical Gnostics, it offers superior resilience to outliers and non-Gaussian noise compared to standard K-Means implementations.</p>"},{"location":"models/cluster/kmeans/#overview","title":"Overview","text":"<p>Machine Gnostics <code>KMeansClustering</code> extends the traditional K-Means algorithm by introducing an iterative reweighting mechanism (<code>gnostic_weights</code>). This allows the model to dynamically down-weight contributions from anomalous data points during the centroid update phase, ensuring that cluster centers are not pulled away by outliers.</p> <ul> <li>Robustness: Minimizes the influence of outliers using gnostic loss functions.</li> <li>Adaptive Weighting: Automatically adjusts sample weights based on data quality.</li> <li>Initialization Options: Supports 'random' and 'kmeans++' initialization.</li> <li>Optimization: Includes convergence checks and early stopping.</li> </ul>"},{"location":"models/cluster/kmeans/#key-features","title":"Key Features","text":"<ul> <li>Robust clustering using gnostic loss functions ('hi' or 'hj')</li> <li>Adaptive sample weights for outlier suppression</li> <li>Detailed history tracking of optimization process</li> <li>Compatible with numpy arrays</li> <li>Easy model persistence (save/load)</li> <li>Standard Scikit-learn style API (fit, predict, score)</li> </ul>"},{"location":"models/cluster/kmeans/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_clusters</code> <code>int</code> <code>3</code> The number of clusters to form. <code>scale</code> <code>str</code> | <code>int</code> | <code>float</code> <code>'auto'</code> Scaling method or value for gnostic calculations. <code>max_iter</code> <code>int</code> <code>100</code> Maximum number of optimization iterations. <code>tolerance</code> <code>float</code> <code>1e-1</code> Convergence tolerance. <code>mg_loss</code> <code>str</code> <code>'hi'</code> Gnostic loss function to use ('hi' or 'hj'). <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop early upon convergence. <code>verbose</code> <code>bool</code> <code>False</code> Verbosity mode. <code>data_form</code> <code>str</code> <code>'a'</code> Data form: 'a' (additive) or 'm' (multiplicative). <code>gnostic_characteristics</code> <code>bool</code> <code>False</code> Compute and record gnostic characteristics. <code>history</code> <code>bool</code> <code>True</code> Whether to record optimization history. <code>init</code> <code>str</code> <code>'random'</code> Initialization method ('random' or 'kmeans++')."},{"location":"models/cluster/kmeans/#attributes","title":"Attributes","text":"<ul> <li>centroids: <code>np.ndarray</code><ul> <li>Fitted cluster centroids of shape <code>(n_clusters, n_features)</code>.</li> </ul> </li> <li>labels: <code>np.ndarray</code><ul> <li>Index of the cluster each sample belongs to.</li> </ul> </li> <li>weights: <code>np.ndarray</code><ul> <li>Final weights assigned to each sample after robust fitting.</li> </ul> </li> <li>params: <code>list</code><ul> <li>List of parameter snapshots (loss, weights, centroids) at each iteration.</li> </ul> </li> <li>_history: <code>list</code><ul> <li>Internal optimization history data.</li> </ul> </li> </ul>"},{"location":"models/cluster/kmeans/#methods","title":"Methods","text":""},{"location":"models/cluster/kmeans/#fitx-ynone","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the robust k-means clustering model to the data.</p> <p>This method iteratively optimizes cluster centroids and sample weights to minimize the influence of outliers.</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>Ignored</code><ul> <li>Not used, present for API consistency.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>KMeansClustering</code><ul> <li>Returns the fitted model instance.</li> </ul> </li> </ul>"},{"location":"models/cluster/kmeans/#predictmodel_input","title":"<code>predict(model_input)</code>","text":"<p>Predict the closest cluster each sample in <code>model_input</code> belongs to.</p> <p>Parameters</p> <ul> <li>model_input: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>New data to predict.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>labels: <code>np.ndarray</code> of shape <code>(n_samples,)</code><ul> <li>Index of the predicted cluster.</li> </ul> </li> </ul>"},{"location":"models/cluster/kmeans/#scorex-ynone","title":"<code>score(X, y=None)</code>","text":"<p>Compute the negative inertia score (sum of squared distances to closest centroid).</p> <p>Parameters</p> <ul> <li>X: <code>np.ndarray</code> of shape <code>(n_samples, n_features)</code><ul> <li>Input features.</li> </ul> </li> <li>y: <code>Ignored</code><ul> <li>Not used.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Negative inertia score. (Higher is better, consistent with sklearn).</li> </ul> </li> </ul>"},{"location":"models/cluster/kmeans/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/cluster/kmeans/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>KMeansClustering</code> with loaded parameters.</p>"},{"location":"models/cluster/kmeans/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import KMeansClustering\n\n# Generate synthetic data with 3 clusters\nnp.random.seed(42)\nX = np.concatenate([\n    np.random.normal(0, 1, (50, 2)),\n    np.random.normal(5, 1, (50, 2)),\n    np.random.normal(10, 1, (50, 2))\n])\n\n# Add some outliers\nX = np.vstack([X, np.random.uniform(-5, 15, (10, 2))])\n\n# Initialize and fit\nmodel = KMeansClustering(\n    n_clusters=3,\n    max_iter=50,\n    verbose=True\n)\nmodel.fit(X)\n\n# Predict cluster labels\nlabels = model.predict(X[:5])\nprint(\"Labels:\", labels)\n\n# Get centroids\nprint(\"Centroids:\", model.centroids)\n\n# Score\nscore = model.score(X)\nprint(f\"Inertia Score: {score:.2f}\")\n</code></pre>"},{"location":"models/cluster/kmeans/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed optimization history, including the evolution of centroids and weights. This is stored in <code>model.params</code> and <code>model._history</code>.</p>"},{"location":"models/cluster/kmeans/#notes","title":"Notes","text":"<ul> <li>This model uses principles from Mathematical Gnostics to derive robust weights.</li> <li>It is especially useful in automated pipelines where data quality cannot be guaranteed.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/forc/ar/","title":"AutoRegressor: Robust Time Series Forecasting","text":"<p>The <code>AutoRegressor</code> model implements an Autoregressive (AR) process for time series forecasting, enhanced by Mathematical Gnostics. Unlike standard AR models which often use Ordinary Least Squares (OLS), this model employs Iterative Gnostic Reweighting. This allows the model to autonomously down-weight outliers and anomalies in the historical data, resulting in a forecast that is robust to non-Gaussian noise and transient disruptions.</p>"},{"location":"models/forc/ar/#overview","title":"Overview","text":"<p>Machine Gnostics <code>AutoRegressor</code> models the next value in a time series as a linear combination of its previous values (lags).</p> \\[ y_t = c + w_1 y_{t-1} + w_2 y_{t-2} + \\dots + w_p y_{t-p} + \\epsilon_t \\] <p>The key innovation is the estimation of the coefficients (\\(w\\)). Instead of minimizing squared error (which is sensitive to outliers), it minimizes Gnostic Entropy via an iteratively reweighted least squares approach.</p> <ul> <li>Robust Forecasting: Resilient to outliers and structural breaks in history.</li> <li>Trend Support: Can model constant (bias) and linear time trends.</li> <li>Recursive Forecasting: Supports multi-step ahead prediction.</li> <li>Iterative Refinement: Optimizes weights to ignore noisy historical points.</li> </ul>"},{"location":"models/forc/ar/#key-features","title":"Key Features","text":"<ul> <li>Robust Autoregressive (AR) modeling</li> <li>Outlier-resilient coefficient estimation</li> <li>Configurable lag order (\\(p\\))</li> <li>Supports Constant ('c') and Constant+Linear ('ct') trends</li> <li>Recursive multi-step forecasting</li> <li>Gnostic diagnostics (weights, entropy)</li> </ul>"},{"location":"models/forc/ar/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>lags</code> <code>int</code> <code>1</code> Number of past observations to use (\\(p\\)). <code>trend</code> <code>str</code> <code>'c'</code> Trend type:<code>'c'</code> (constant/bias), <code>'ct'</code> (constant + linear trend), <code>'n'</code> (no trend). <code>scale</code> <code>str</code> | <code>float</code> <code>'auto'</code> Scaling method or value for gnostic calculations. <code>max_iter</code> <code>int</code> <code>100</code> Maximum reweighting iterations. <code>tolerance</code> <code>float</code> <code>1e-3</code> Convergence tolerance. <code>learning_rate</code> <code>float</code> <code>0.1</code> Learning rate for weight updates. <code>mg_loss</code> <code>str</code> <code>'hi'</code> Gnostic loss function ('hi' or 'hj'). <code>early_stopping</code> <code>bool</code> <code>True</code> Stop early if converged. <code>verbose</code> <code>bool</code> <code>False</code> Print progress. <code>history</code> <code>bool</code> <code>True</code> Record training history."},{"location":"models/forc/ar/#attributes","title":"Attributes","text":"<ul> <li>weights: <code>np.ndarray</code></li> <li>The final robust weights assigned to the training samples. Low weights indicate potential outliers in the history.</li> <li>coefficients: <code>np.ndarray</code></li> <li>The fitted autoregressive coefficients (including trend parameters).</li> <li>training_data_: <code>np.ndarray</code></li> <li>Stored history used for recursive forecasting.</li> <li>_history: <code>list</code></li> <li>Training process history (loss, entropy) per iteration.</li> </ul>"},{"location":"models/forc/ar/#methods","title":"Methods","text":""},{"location":"models/forc/ar/#fity-xnone","title":"<code>fit(y, X=None)</code>","text":"<p>Fit the Autoregressor to the time series <code>y</code>.</p> <p>Parameters</p> <ul> <li>y: <code>array-like</code></li> <li>Target time series.</li> <li>X: <code>Ignored</code></li> <li>Not used.</li> </ul> <p>Returns</p> <ul> <li>self: <code>AutoRegressor</code></li> </ul>"},{"location":"models/forc/ar/#predictsteps1","title":"<code>predict(steps=1)</code>","text":"<p>Forecast future values recursively.</p> <p>Parameters</p> <ul> <li>steps: <code>int</code></li> <li>Number of time steps to forecast into the future.</li> </ul> <p>Returns</p> <ul> <li>forecast: <code>np.ndarray</code></li> <li>Predicted values for the next <code>steps</code>.</li> </ul>"},{"location":"models/forc/ar/#scorey-xnone","title":"<code>score(y, X=None)</code>","text":"<p>Evaluate the model on a time series <code>y</code> using Robust R\u00b2. This reconstructs the lag features for <code>y</code> and calculates the one-step-ahead prediction accuracy.</p> <p>Parameters</p> <ul> <li>y: <code>array-like</code></li> <li>Time series to evaluate.</li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code></li> <li>Robust R\u00b2 score.</li> </ul>"},{"location":"models/forc/ar/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/forc/ar/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>AutoRegressor</code> with loaded parameters.</p>"},{"location":"models/forc/ar/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import AutoRegressor\nimport matplotlib.pyplot as plt\n\n# Generate synthetic AR data with outliers\nnp.random.seed(42)\nt = np.arange(100)\n# Sine wave + Noise\ny = np.sin(t * 0.2) + np.random.normal(0, 0.1, 100)\n# Add spikes (outliers)\ny[20] = 5.0\ny[50] = -5.0\ny[80] = 5.0\n\n# Initialize AR(10) with constant trend\nmodel = AutoRegressor(lags=10, trend='c', verbose=True)\n\n# Fit\nmodel.fit(y)\n\n# Forecast next 20 steps\nfuture_steps = 20\nforecast = model.predict(steps=future_steps)\n\nprint(\"Forecast:\", forecast)\n\n# Visualization\nplt.plot(np.arange(100), y, label='History')\nplt.plot(np.arange(100, 100+future_steps), forecast, label='Forecast', color='red')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"models/forc/ar/#notes","title":"Notes","text":"<ul> <li>Lag Generation: The model automatically creates the lag matrix \\(X\\) where rows are windows \\([y_{t-1}, \\dots, y_{t-p}]\\) and targets are \\(y_t\\).</li> <li>Trend Handling:</li> <li><code>'c'</code>: Adds a column of 1s (bias).</li> <li><code>'ct'</code>: Adds a column of 1s and a column of time indices \\(t\\).</li> <li>Scaling: Input data is scaled automatically (default) to ensure stable optimization of gnostic weights.</li> </ul> <p>Author: Nirmal Parmar</p>"},{"location":"models/forc/arima/","title":"ARIMA: Robust AutoRegressive Integrated Moving Average","text":"<p>The <code>ARIMA</code> model (AutoRegressive Integrated Moving Average) extends robust time series forecasting to non-stationary data and moving average processes. Enhanced by Mathematical Gnostics, this implementation uses iterative reweighting to minimize the influence of outliers on the estimation of \\(p\\), \\(d\\), and \\(q\\) parameters.</p>"},{"location":"models/forc/arima/#overview","title":"Overview","text":"<p>Machine Gnostics <code>ARIMA</code> brings robustness to the classic ARIMA framework. Standard estimation methods (like MLE) are highly sensitive to outliers. This implementation uses Gnostic Weighted Least Squares to robustly estimate autoregressive and moving average parameters.</p> <ul> <li>Integrated (d): Automatically handles differencing to stationarize data.</li> <li>AutoRegressive (p): Models relationships with past values.</li> <li>Moving Average (q): Models relationships with past forecast errors.</li> <li>Auto-Optimization: Can search for the optimal \\((p, d, q)\\) order.</li> <li>Robustness: Down-weights anomalous time steps during training.</li> </ul>"},{"location":"models/forc/arima/#key-features","title":"Key Features","text":"<ul> <li>Robust ARIMA(p,d,q) modeling</li> <li>Automatic order selection (Grid Search)</li> <li>Handles non-stationary data via differencing</li> <li>Supports Constant ('c') and Constant+Linear ('ct') trends</li> <li>Recursive multi-step forecasting</li> <li>Gnostic diagnostics (weights, entropy)</li> </ul>"},{"location":"models/forc/arima/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>order</code> <code>tuple</code> <code>(1, 0, 0)</code> The\\((p, d, q)\\) order of the model. <code>optimize</code> <code>bool</code> <code>False</code> If True, searches for optimal order within <code>max_order_search</code>. <code>max_order_search</code> <code>tuple</code> <code>(5, 1, 5)</code> Max\\((p, d, q)\\) limits for optimization. <code>trend</code> <code>str</code> <code>'c'</code> Trend type:<code>'c'</code> (bias), <code>'ct'</code> (linear), <code>'n'</code> (none). <code>scale</code> <code>str</code> | <code>float</code> <code>'auto'</code> Scaling method for gnostic calculations. <code>max_iter</code> <code>int</code> <code>100</code> Maximum reweighting iterations. <code>tolerance</code> <code>float</code> <code>1e-3</code> Convergence tolerance. <code>learning_rate</code> <code>float</code> <code>0.1</code> Learning rate for weight updates. <code>mg_loss</code> <code>str</code> <code>'hi'</code> Gnostic loss function ('hi' or 'hj'). <code>early_stopping</code> <code>bool</code> <code>True</code> Stop early if converged. <code>verbose</code> <code>bool</code> <code>False</code> Print progress. <code>history</code> <code>bool</code> <code>True</code> Record training history."},{"location":"models/forc/arima/#attributes","title":"Attributes","text":"<ul> <li>weights: <code>np.ndarray</code></li> <li>The final robust weights assigned to the training samples.</li> <li>coefficients: <code>np.ndarray</code></li> <li>The fitted model parameters (\\(\\phi\\), \\(\\theta\\), trend).</li> <li>p, d, q: <code>int</code></li> <li>The final model order (updated if <code>optimize=True</code>).</li> <li>training_data_raw_: <code>np.ndarray</code></li> <li>Original training series (used for inverse differencing during forecast).</li> <li>training_residuals_: <code>np.ndarray</code></li> <li>Estimated residuals from the training phase.</li> </ul>"},{"location":"models/forc/arima/#methods","title":"Methods","text":""},{"location":"models/forc/arima/#fity-xnone","title":"<code>fit(y, X=None)</code>","text":"<p>Fit the ARIMA model to the time series <code>y</code>.</p> <p>If <code>optimize=True</code>, this runs a grid search over \\((p, d, q)\\) combinations to minimize RMSE on a validation split before fitting the final model.</p> <p>Parameters</p> <ul> <li>y: <code>array-like</code></li> <li>Target time series.</li> <li>X: <code>Ignored</code></li> <li>Not used.</li> </ul> <p>Returns</p> <ul> <li>self: <code>ARIMA</code></li> </ul>"},{"location":"models/forc/arima/#predictsteps1","title":"<code>predict(steps=1)</code>","text":"<p>Forecast future values recursively.</p> <p>This method handles:</p> <ol> <li>Recursive prediction of differenced values.</li> <li>Inverse differencing to return forecasts on the original scale.</li> </ol> <p>Parameters</p> <ul> <li>steps: <code>int</code></li> <li>Number of time steps to forecast.</li> </ul> <p>Returns</p> <ul> <li>forecast: <code>np.ndarray</code></li> <li>Predicted values for the next <code>steps</code>.</li> </ul>"},{"location":"models/forc/arima/#scorey-xnone","title":"<code>score(y, X=None)</code>","text":"<p>Evaluate the model on a time series <code>y</code> using Robust R\u00b2. Currently, scoring is primarily supported for in-sample evaluation (on the training data) due to the recursive nature of MA terms.</p> <p>Parameters</p> <ul> <li>y: <code>array-like</code></li> <li>Time series to evaluate.</li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code></li> <li>Robust R\u00b2 score (on the stationary/differenced series).</li> </ul>"},{"location":"models/forc/arima/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/forc/arima/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>ARIMA</code> with loaded parameters.</p>"},{"location":"models/forc/arima/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import ARIMA\nimport matplotlib.pyplot as plt\n\n# Generate synthetic ARIMA(1,1,1) process\nnp.random.seed(42)\nn = 150\n# Random walk with drift\ny = np.cumsum(np.random.normal(0.1, 1, n))\n\n# Add outliers\ny[50] += 10\ny[100] -= 10\n\n# Initialize model with automic order search\nmodel = ARIMA(\n    optimize=True,\n    max_order_search=(3, 2, 3),\n    trend='c',\n    verbose=True\n)\n\n# Fit (will search for best p,d,q)\nmodel.fit(y)\n\nprint(f\"Best Order Found: ({model.p}, {model.d}, {model.q})\")\n\n# Forecast\nforecast = model.predict(steps=20)\n\n# Plot\nplt.plot(np.arange(n), y, label='History')\nplt.plot(np.arange(n, n+20), forecast, label='Forecast', color='red')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"models/forc/arima/#notes","title":"Notes","text":"<ul> <li>Initial Residuals: For MA terms (\\(q &gt; 0\\)), initial residuals are estimated using the Hannan-Rissanen algorithm (approximated via a long AR model).</li> <li>Optimization: The order search evaluates models based on RMSE on the last 20% of the data.</li> <li>Differencing: The internal states store the differenced series. Forecasts are integrated back to original scale automatically.</li> </ul> <p>Author: Nirmal Parmar</p>"},{"location":"models/forc/sarima/","title":"SARIMA: Robust Seasonal ARIMA","text":"<p>The <code>SARIMA</code> model (Seasonal AutoRegressive Integrated Moving Average) extends the robust forecasting capabilities of Gnostic ARIMA to data with seasonal patterns. It supports both non-seasonal \\((p,d,q)\\) and seasonal \\((P,D,Q,s)\\) components, estimated using robust Gnostic Weighted Least Squares.</p>"},{"location":"models/forc/sarima/#overview","title":"Overview","text":"<p>Machine Gnostics <code>SARIMA</code> allows for robust modeling of complex time series that exhibit periodic behavior (seasonality), such as monthly sales or daily temperatures, while maintaining resilience against outliers.</p> <ul> <li>Seasonal Integrated (D): Handles seasonal non-stationarity (e.g., year-over-year growth).</li> <li>Seasonal AR/MA (P, Q): Models relationships at seasonal lags.</li> <li>Robust Estimation: Down-weights outliers that might distort seasonal pattern detection.</li> <li>Recursive Forecasting: Supports multi-step ahead prediction by unrolling the differencing layers.</li> </ul>"},{"location":"models/forc/sarima/#key-features","title":"Key Features","text":"<ul> <li>Robust SARIMA(p,d,q)x(P,D,Q,s) modeling</li> <li>Automatic handling of seasonal differencing</li> <li>Supports Constant ('c') and Constant+Linear ('ct') trends</li> <li>Robust to outliers via Gnostic Weights</li> <li>Recursive multi-step forecasting</li> </ul>"},{"location":"models/forc/sarima/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>order</code> <code>tuple</code> <code>(1, 0, 0)</code> The non-seasonal \\((p, d, q)\\) order. <code>seasonal_order</code> <code>tuple</code> <code>(0, 0, 0, 0)</code> The seasonal \\((P, D, Q, s)\\) order. \\(s\\) is the periodicity. <code>trend</code> <code>str</code> <code>'c'</code> Trend type: <code>'c'</code> (bias), <code>'ct'</code> (linear), <code>'n'</code> (none). <code>optimize</code> <code>bool</code> <code>False</code> (Experimental) Auto-select orders. <code>scale</code> <code>str</code> | <code>float</code> <code>'auto'</code> Scaling method for gnostic calculations. <code>max_iter</code> <code>int</code> <code>100</code> Maximum reweighting iterations. <code>tolerance</code> <code>float</code> <code>1e-3</code> Convergence tolerance. <code>learning_rate</code> <code>float</code> <code>0.1</code> Learning rate for weight updates. <code>mg_loss</code> <code>str</code> <code>'hi'</code> Gnostic loss function ('hi' or 'hj'). <code>early_stopping</code> <code>bool</code> <code>True</code> Stop early if converged. <code>history</code> <code>bool</code> <code>True</code> Record training history."},{"location":"models/forc/sarima/#attributes","title":"Attributes","text":"<ul> <li>weights: <code>np.ndarray</code><ul> <li>The final robust weights assigned to the training samples.</li> </ul> </li> <li>coefficients: <code>np.ndarray</code><ul> <li>The fitted model parameters.</li> </ul> </li> <li>training_data_diff_: <code>np.ndarray</code><ul> <li>The fully stationarized series (after both differences) used internally.</li> </ul> </li> <li>training_residuals_: <code>np.ndarray</code><ul> <li>Estimated residuals from the training phase.</li> </ul> </li> </ul>"},{"location":"models/forc/sarima/#methods","title":"Methods","text":""},{"location":"models/forc/sarima/#fity-xnone","title":"<code>fit(y, X=None)</code>","text":"<p>Fit the SARIMA model to the time series <code>y</code>.</p> <p>Parameters</p> <ul> <li>y: <code>array-like</code><ul> <li>Target time series.</li> </ul> </li> <li>X: <code>Ignored</code><ul> <li>Not used.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>self: <code>SARIMA</code></li> </ul>"},{"location":"models/forc/sarima/#predictsteps1","title":"<code>predict(steps=1)</code>","text":"<p>Forecast future values recursively.</p> <p>This method handles the complex inverse transformation of both regular (\\(d\\)) and seasonal (\\(D\\)) differencing to return forecasts on the original scale.</p> <p>Parameters</p> <ul> <li>steps: <code>int</code><ul> <li>Number of time steps to forecast.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>forecast: <code>np.ndarray</code><ul> <li>Predicted values for the next <code>steps</code>.</li> </ul> </li> </ul>"},{"location":"models/forc/sarima/#scorey-xnone","title":"<code>score(y, X=None)</code>","text":"<p>Evaluate the model on the training time series <code>y</code>. Returns the Robust R\u00b2 calculated on the stationary (fully differenced) series.</p> <p>Parameters</p> <ul> <li>y: <code>array-like</code><ul> <li>Time series to evaluate.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>score: <code>float</code><ul> <li>Robust R\u00b2 score.</li> </ul> </li> </ul>"},{"location":"models/forc/sarima/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/forc/sarima/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns</p> <p>Instance of <code>SARIMA</code> with loaded parameters.</p>"},{"location":"models/forc/sarima/#example-usage","title":"Example Usage","text":"PythonExample Output <pre><code>import numpy as np\nfrom machinegnostics.models import SARIMA\nimport matplotlib.pyplot as plt\n\n# Generate synthetic Seasonal Data (Period=12)\nnp.random.seed(42)\nt = np.arange(120)\nseasonal = np.sin(2 * np.pi * t / 12)\ntrend = 0.05 * t\nnoise = np.random.normal(0, 0.2, 120)\ny = seasonal + trend + noise\n\n# Add seasonal outlier\ny[13] += 5.0 # Spike in month 2 of year 2\n\n# Initialize SARIMA(1,0,0)x(1,1,0,12)\n# Simple AR(1) with Seasonal Integration and Seasonal AR(1)\nmodel = SARIMA(\n    order=(1, 0, 0),\n    seasonal_order=(1, 1, 0, 12),\n    trend='c',\n    verbose=True\n)\n\n# Fit\nmodel.fit(y)\n\n# Forecast next 24 steps (2 seasons)\nforecast = model.predict(steps=24)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(np.arange(120), y, label='History')\nplt.plot(np.arange(120, 144), forecast, label='Forecast', color='red')\nplt.title(\"Gnostic SARIMA Forecast\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"models/forc/sarima/#notes","title":"Notes","text":"<ul> <li>Initial Residuals: For MA terms, initial residuals are estimated using a high-order AR model covering the seasonal period.</li> <li>Differencing: The model applies seasonal differencing (\\(D\\)) first, followed by regular differencing (\\(d\\)). Forecasts reverse this order.</li> <li>Complexity: Higher seasonal orders (large \\(P, Q, s\\)) significantly increase the number of parameters and required history length.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/reg/lin_reg/","title":"LinearRegressor: Robust Linear Regression with Machine Gnostics","text":"<p>The <code>LinearRegressor</code> is a robust linear regression model built on the Machine Gnostics framework. Unlike traditional statistical models that rely on probabilistic assumptions, this model uses algebraic and geometric structures to provide deterministic, resilient, and interpretable regression for real-world data.</p>"},{"location":"models/reg/lin_reg/#overview","title":"Overview","text":"<p>The Machine Gnostics LinearRegressor is designed for robust regression tasks, especially where data may contain outliers, noise, or non-Gaussian distributions. It leverages the core principles of Mathematical Gnostics (MG) to deliver reliable results even in challenging scenarios.</p> <ul> <li>Deterministic &amp; Finite: No randomness or probability; all computations are reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events.</li> <li>Algebraic Inference: Utilizes gnostic algebra and error geometry for robust learning.</li> <li>Resilient: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> <li>Flexible: Supports numpy arrays, pandas DataFrames, and pyspark DataFrames.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/reg/lin_reg/#key-features","title":"Key Features","text":"<ul> <li>Fits a linear regression model</li> <li>Robust to outliers and non-Gaussian noise</li> <li>Iterative optimization with early stopping and convergence tolerance</li> <li>Adaptive sample weighting using gnostic loss</li> <li>Training history tracking for analysis and visualization</li> <li>Customizable loss functions and scaling strategies</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/reg/lin_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>scale</code> {'auto', int, float} 'auto' Scaling method or value for input features. <code>max_iter</code> int 100 Maximum number of optimization iterations. <code>tolerance</code> float 1e-2 Tolerance for convergence. <code>mg_loss</code> str 'hi' Gnostic loss function to use (<code>'hi'</code>, <code>'fi'</code>, etc.). <code>early_stopping</code> bool True Whether to stop early if convergence is detected. <code>verbose</code> bool False If True, prints progress and diagnostics during fitting. <code>data_form</code> str 'a' Internal data representation format. <code>gnostic_characteristics</code> bool False If True, computes and records gnostic properties (fi, hi, etc.). <code>history</code> bool True If True, records the optimization history for analysis."},{"location":"models/reg/lin_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>np.ndarray</code>   Fitted linear regression coefficients.</li> <li>weights: <code>np.ndarray</code>   Final sample weights after robust fitting.</li> <li>params: <code>list of dict</code>   Parameter snapshots (loss, weights, gnostic properties) at each iteration.</li> <li>_history: <code>list</code> or <code>None</code>   Internal optimization history (if enabled).</li> <li>degree, max_iter, tolerance, mg_loss, early_stopping, verbose, scale, data_form, gnostic_characteristics:   Configuration parameters as set at initialization.</li> </ul>"},{"location":"models/reg/lin_reg/#methods","title":"Methods","text":""},{"location":"models/reg/lin_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the linear regressor to input features <code>X</code> and targets <code>y</code> using robust, gnostic loss minimization. Iteratively optimizes coefficients and sample weights, optionally recording history.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Target values.</li> </ul> <p>Returns: <code>self</code> (fitted model instance)</p>"},{"location":"models/reg/lin_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts target values for new input features using the fitted model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features for prediction.</li> </ul> <p>Returns: <code>y_pred</code>: <code>np.ndarray</code>, shape <code>(n_samples,)</code> Predicted target values.</p>"},{"location":"models/reg/lin_reg/#scorex-y-casei","title":"<code>score(X, y, case='i')</code>","text":"<p>Computes the robust (gnostic) R\u00b2 score for the linear regressor model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features for scoring.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   True target values.</li> <li>case: <code>str</code>, default <code>'i'</code>   Specifies the case or variant of the R\u00b2 score to compute.</li> </ul> <p>Returns: <code>score</code>: <code>float</code> Robust R\u00b2 score of the model on the provided data.</p>"},{"location":"models/reg/lin_reg/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/reg/lin_reg/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns: Instance of <code>LinearRegressor</code> with loaded parameters.</p>"},{"location":"models/reg/lin_reg/#example-usage","title":"Example Usage","text":"PythonPlotOutput <pre><code>import numpy as np\nfrom machinegnostics.models import LinearRegressor\nimport matplotlib.pyplot as plt\n\n# Generate linear data with noise and outliers\nnp.random.seed(42)\nX = np.linspace(0, 2, 10).reshape(-1, 1)\ny = 10 * X.ravel() + 1  # True: y = 10x + 1\n\n# Add noise and an outlier\ny += np.random.normal(0, 2, 10)\ny[2] += 28.0  # Outlier\n\n# Fit the model\nmodel = LinearRegressor(max_iter=100, verbose=True)\nmodel.fit(X, y)\n\n# Make predictions\ny_pred = model.predict(X)\n\nprint(\"Model fitted successfully!\")\nprint(f\"Coefficients (Intercept, Slope): {model.coefficients}\")\nprint(f\"MSE: {np.mean((y_pred - y)**2):.4f}\")\n\n# Visualization code omitted for brevity; see output image\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\n# Generate smooth line for visualization\nX_test = np.linspace(0, 2, 100).reshape(-1, 1)\ny_test = model.predict(X_test)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='blue', s=100, label='Data', zorder=3, alpha=0.6)\nplt.scatter(X[2], y[2], color='red', s=150, label='Outlier', zorder=4, marker='X')\nplt.plot(X_test, y_test, 'g-', linewidth=2, label='Fitted Model')\n\n# True relationship for reference\ny_true = 10 * X_test + 1\nplt.plot(X_test, y_true, 'k--', linewidth=1, alpha=0.5, label='True: y=10x+1')\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression Fit')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/reg/lin_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model.params</code> and <code>model._history</code>. Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>loss</code>: Gnostic loss value</li> <li><code>coefficients</code>: Regression coefficients at this iteration (normalized)</li> <li><code>rentropy</code>: Rentropy value (residual entropy)</li> <li><code>weights</code>: Sample weights at this iteration</li> <li><code>gnostic_characteristics</code>: (if enabled) fi, hi, etc.</li> </ul> <p>This enables in-depth analysis and visualization of the training process.</p>"},{"location":"models/reg/lin_reg/#notes","title":"Notes","text":"<ul> <li>The model is robust to outliers and suitable for datasets with non-Gaussian noise.</li> <li>Supports integration with mlflow for experiment tracking and deployment.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/reg/poly_reg/","title":"PolynomialRegressor: Robust Polynomial Regression with Machine Gnostics","text":"<p>The <code>PolynomialRegressor</code> is a robust polynomial regression model built on the Machine Gnostics framework. Unlike traditional statistical models that rely on probabilistic assumptions, this model uses algebraic and geometric structures to provide deterministic, resilient, and interpretable regression for real-world data.</p>"},{"location":"models/reg/poly_reg/#overview","title":"Overview","text":"<p>The Machine Gnostics PolynomialRegressor is designed for robust regression tasks, especially where data may contain outliers, noise, or non-Gaussian distributions. It leverages the core principles of Mathematical Gnostics (MG) to deliver reliable results even in challenging scenarios.</p> <ul> <li>Deterministic &amp; Finite: No randomness or probability; all computations are reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events.</li> <li>Algebraic Inference: Utilizes gnostic algebra and error geometry for robust learning.</li> <li>Resilient: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> <li>Flexible: Supports numpy arrays, pandas DataFrames, and pyspark DataFrames.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"models/reg/poly_reg/#key-features","title":"Key Features","text":"<ul> <li>Fits a polynomial regression model</li> <li>Flexible polynomial degree (linear and higher-order)</li> <li>Robust to outliers and non-Gaussian noise</li> <li>Iterative optimization with early stopping and convergence tolerance</li> <li>Adaptive sample weighting using gnostic loss</li> <li>Training history tracking for analysis and visualization</li> <li>Customizable loss functions and scaling strategies</li> <li>Compatible with numpy arrays for input/output</li> </ul>"},{"location":"models/reg/poly_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>degree</code> <code>int</code> <code>2</code> Degree of the polynomial to fit. <code>scale</code> <code>str</code> | <code>int</code> | <code>float</code> <code>'auto'</code> Scaling method or value for input features. <code>max_iter</code> <code>int</code> <code>100</code> Maximum number of optimization iterations. <code>tolerance</code> <code>float</code> <code>1e-2</code> Tolerance for convergence. <code>mg_loss</code> <code>str</code> <code>'hi'</code> Loss function to use ('hi', 'fi', etc.). <code>early_stopping</code> <code>bool</code> <code>True</code> Whether to stop early if convergence is detected. <code>verbose</code> <code>bool</code> <code>False</code> If True, prints progress and diagnostics during fitting. <code>data_form</code> <code>str</code> <code>'a'</code> Internal data representation format. <code>gnostic_characteristics</code> <code>bool</code> <code>False</code> If True, computes and records gnostic characteristics. <code>history</code> <code>bool</code> <code>True</code> If True, records the optimization history for analysis."},{"location":"models/reg/poly_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>np.ndarray</code>   Fitted polynomial coefficients.</li> <li>weights: <code>np.ndarray</code>   Final sample weights after robust fitting.</li> <li>params: <code>list of dict</code>   Parameter snapshots (loss, weights, gnostic properties) at each iteration.</li> <li>_history: <code>list</code> or <code>None</code>   Internal optimization history (if enabled).</li> <li>degree, max_iter, tolerance, mg_loss, early_stopping, verbose, scale, data_form, gnostic_characteristics   Configuration parameters as set at initialization.</li> </ul>"},{"location":"models/reg/poly_reg/#methods","title":"Methods","text":""},{"location":"models/reg/poly_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the polynomial regressor to input features <code>X</code> and targets <code>y</code> using robust, gnostic loss minimization. Iteratively optimizes coefficients and sample weights, optionally recording history.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   Target values.</li> </ul> <p>Returns: <code>self</code> (fitted model instance)</p>"},{"location":"models/reg/poly_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts target values for new input features using the fitted model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features for prediction.</li> </ul> <p>Returns: <code>y_pred</code>: <code>np.ndarray</code>, shape <code>(n_samples,)</code> Predicted target values.</p>"},{"location":"models/reg/poly_reg/#scorex-y-casei","title":"<code>score(X, y, case='i')</code>","text":"<p>Computes the robust (gnostic) R\u00b2 score for the polynomial regressor model.</p> <ul> <li>X: <code>np.ndarray</code>, shape <code>(n_samples, n_features)</code>   Input features for scoring.</li> <li>y: <code>np.ndarray</code>, shape <code>(n_samples,)</code>   True target values.</li> <li>case: <code>str</code>, default <code>'i'</code>   Specifies the case or variant of the R\u00b2 score to compute.</li> </ul> <p>Returns: <code>score</code>: <code>float</code> Robust R\u00b2 score of the model on the provided data.</p>"},{"location":"models/reg/poly_reg/#savepath","title":"<code>save(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"models/reg/poly_reg/#loadpath","title":"<code>load(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: str   Directory path where the model is saved.</li> </ul> <p>Returns: Instance of <code>PolynomialRegressor</code> with loaded parameters.</p>"},{"location":"models/reg/poly_reg/#example-usage","title":"Example Usage","text":"PythonPlotOutput <pre><code>from machinegnostics.models import PolynomialRegressor\nimport numpy as np\n\n# Generate synthetic data with outliers\nnp.random.seed(42)\nX_train = np.random.rand(50, 1) * 10\ny_train = 0.5 * X_train.flatten()**2 - 2 * X_train.flatten() + 5 + np.random.normal(0, 1, 50)\n\n# Introduce outliers\ny_train[::10] += 20 * np.random.choice([-1, 1], size=5)\n\n# Initialize model\nmodel = PolynomialRegressor(\n    degree=2, \n    max_iter=200, \n    verbose=True,\n    tolerance=1e-3\n)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict\nX_test = np.linspace(0, 10, 10).reshape(-1, 1)\ny_pred = model.predict(X_test)\n\n# Score\nr2 = model.score(X_train, y_train)\nprint(f'Robust R2 score: {r2:.4f}')\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\n# Generate smooth curve for visualization\nX_test = np.linspace(0, 2, 100).reshape(-1, 1)\ny_test = model.predict(X_test)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='blue', s=100, label='Data', zorder=3, alpha=0.6)\nplt.scatter(X[8:], y[8:], color='red', s=150, label='Outliers', zorder=4, marker='X')\nplt.plot(X_test, y_test, 'g-', linewidth=2, label='Fitted Model')\n\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Polynomial Regression Fit')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/reg/poly_reg/#training-history","title":"Training History","text":"<p>If <code>history=True</code>, the model records detailed training history at each iteration, accessible via <code>model.params</code> and <code>model._history</code>. Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>loss</code>: Gnostic loss value</li> <li><code>coefficients</code>: Regression coefficients at this iteration</li> <li><code>rentropy</code>: Rentropy value (residual entropy)</li> <li><code>weights</code>: Sample weights at this iteration</li> <li><code>gnostic_characteristics</code>: (if enabled) fi, hi, etc.</li> </ul> <p>This enables in-depth analysis and visualization of the training process.</p>"},{"location":"models/reg/poly_reg/#notes","title":"Notes","text":"<ul> <li>The model is robust to outliers and suitable for datasets with non-Gaussian noise.</li> <li>Supports integration with mlflow for experiment tracking and deployment.</li> </ul> <p>Author: Nirmal Parmar  </p>"},{"location":"models/sup/cross_val/","title":"CrossValidator: Custom k-Fold Cross-Validation","text":"<p>The <code>CrossValidator</code> class provides a simple, flexible implementation of k-fold cross-validation for evaluating machine learning models. It is designed to work with any model that implements <code>fit(X, y)</code> and <code>predict(X)</code> methods, and supports custom scoring functions for regression or classification tasks.</p>"},{"location":"models/sup/cross_val/#overview","title":"Overview","text":"<p>Cross-validation is a robust technique for assessing the generalization performance of machine learning models. The <code>CrossValidator</code> class splits your dataset into <code>k</code> folds, trains the model on <code>k-1</code> folds, and evaluates it on the remaining fold, repeating this process for each fold. The results are aggregated to provide a reliable estimate of model performance.</p>"},{"location":"models/sup/cross_val/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model</code> object \u2014 A machine learning model with <code>fit(X, y)</code> and <code>predict(X)</code> methods. <code>X</code> array-like \u2014 Feature matrix of shape <code>(n_samples, n_features)</code>. <code>y</code> array-like \u2014 Target labels of shape <code>(n_samples,)</code>. <code>k</code> int 5 Number of folds for cross-validation. <code>shuffle</code> bool True Whether to shuffle the dataset before splitting into folds. <code>random_seed</code> int/None None Seed for reproducible shuffling (ignored if <code>shuffle=False</code>)."},{"location":"models/sup/cross_val/#attributes","title":"Attributes","text":"<ul> <li>folds: <code>list of tuple</code>   List of <code>(train_indices, test_indices)</code> for each fold.</li> </ul>"},{"location":"models/sup/cross_val/#methods","title":"Methods","text":""},{"location":"models/sup/cross_val/#split","title":"<code>split()</code>","text":"<p>Splits the dataset into <code>k</code> folds.</p> <ul> <li>Returns:   <code>folds</code>: list of tuple   Each tuple contains <code>(train_indices, test_indices)</code> for a fold.</li> </ul>"},{"location":"models/sup/cross_val/#evaluatescoring_func","title":"<code>evaluate(scoring_func)</code>","text":"<p>Performs k-fold cross-validation and returns evaluation scores.</p> <ul> <li>Parameters:<code>scoring_func</code>: callableA function that takes <code>y_true</code> and <code>y_pred</code> and returns a numeric score (e.g., <code>mean_squared_error</code>, <code>accuracy_score</code>).</li> <li>Returns:   <code>scores</code>: list of float   Evaluation scores for each fold.</li> </ul>"},{"location":"models/sup/cross_val/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models import CrossValidator, LinearRegressor\nfrom machinegnostics.metircs import mean_squared_error\nimport numpy as np\n\n# Generate random data\nX = np.random.rand(100, 10)\ny = np.random.rand(100)\n\n# Initialize model and cross-validator\nmodel = LinearRegressor()\ncv = CrossValidator(model, X, y, k=5, shuffle=True, random_seed=42)\n\n# Evaluate using mean squared error\nscores = cv.evaluate(mean_squared_error)\nprint(\"Cross-Validation Scores:\", scores)\nprint(\"Mean Score:\", np.mean(scores))\n</code></pre>"},{"location":"models/sup/cross_val/#notes","title":"Notes","text":"<ul> <li>The model is re-initialized and trained from scratch for each fold.</li> <li>Supports any model with <code>fit</code> and <code>predict</code> methods.</li> <li>Works with any scoring function that accepts <code>y_true</code> and <code>y_pred</code>.</li> <li>Shuffling with a fixed <code>random_seed</code> ensures reproducible splits.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-05-01</p>"},{"location":"models/sup/mlflow/","title":"MLflow Integration with Machine Gnostics","text":"<p>This guide provides a comprehensive workflow for tracking experiments, versioning, and deploying Machine Gnostics models using MLflow.</p>"},{"location":"models/sup/mlflow/#overview","title":"Overview","text":"<p>Machine Gnostics integrates seamlessly with MLflow to provide industrial-grade MLOps capabilities for your robust models. This integration allows you to:</p> <ul> <li>Experiment Tracking: Log hyperparameters, gnostic metrics (like R-Entropy), and model artifacts automatically.</li> <li>Model Versioning: Manage different versions of your gnostic models in a centralized registry.</li> <li>Reproducibility: Capture the exact environment, data, and parameters used for training.</li> <li>Deployment: Serve models using standard deployment patterns via the <code>pyfunc</code> interface (Docker, generic APIs).</li> </ul>"},{"location":"models/sup/mlflow/#1-installation-setup","title":"1. Installation &amp; Setup","text":"<p>Ensure you have both libraries installed in your environment:</p> <pre><code>pip install machinegnostics mlflow\n</code></pre>"},{"location":"models/sup/mlflow/#initial-configuration","title":"Initial Configuration","text":"<p>To configure MLflow in your Python script or notebook, set the tracking URI and experiment name.</p> <pre><code>import mlflow\nfrom machinegnostics.integration import mlflow as mg_mlflow\n\n# 1. Set the tracking URI (where data is stored)\n# Default is a local folder called ./mlruns\nmlflow.set_tracking_uri(\"./mlruns\")\n\n# 2. Set the experiment name\n# Best practice: Name this after your specific project or problem\nmlflow.set_experiment(\"Machine_Gnostics_Experiment\")\n\nprint(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n</code></pre>"},{"location":"models/sup/mlflow/#2-basic-model-logging","title":"2. Basic Model Logging","text":"<p>The core workflow involves training a model inside an <code>mlflow.start_run()</code> context block. This ensures all metrics and artifacts are tied to a specific \"run\".</p> <pre><code>import numpy as np\nfrom mlflow.models import infer_signature\nfrom machinegnostics.models import LogisticRegressor\n\n# Sample Data\nX_train = np.random.rand(100, 2)\ny_train = np.random.randint(0, 2, 100)\n\n# Start the Run\nwith mlflow.start_run(run_name='Gnostic_LogReg_v1') as run:\n\n    # 1. Initialize and Train\n    model = LogisticRegressor(degree=2, gnostic_characteristics=True)\n    model.fit(X_train, y_train)\n\n    # 2. Log Parameters (Hyperparameters)\n    mlflow.log_param(\"degree\", model.degree)\n    mlflow.log_param(\"gnostic_characteristics\", True)\n    mlflow.log_param(\"model_type\", \"LogisticRegressor\")\n\n    # 3. Log Metrics (Performance)\n    # R-Entropy (re) is a specific internal metric for Machine Gnostics\n    mlflow.log_metric(\"rentropy\", model.re) \n\n    # Optional: Log training iterations if available\n    if hasattr(model, '_history') and isinstance(model._history, list):\n         mlflow.log_metric(\"iterations\", len(model._history))\n\n    # 4. Create Signature (Describes Input/Output Schema)\n    # This helps MLflow validate input data types during serving\n    predictions = model.predict(X_train)\n    signature = infer_signature(X_train, predictions)\n\n    # 5. Log the Model\n    # Uses machinegnostics custom integration to package the model correctly\n    mg_mlflow.log_model(\n        model, \n        artifact_path=\"model\",\n        signature=signature,\n        input_example=X_train[:3]\n    )\n\n    print(f\"Run ID: {run.info.run_id}\")\n</code></pre>"},{"location":"models/sup/mlflow/#3-loading-models","title":"3. Loading Models","text":"<p>There are two primary ways to load a saved model, depending on whether you are analyzing the model interactively or deploying it to production.</p>"},{"location":"models/sup/mlflow/#method-a-load-as-pyfunc-production","title":"Method A: Load as PyFunc (Production)","text":"<p>Use this standard method if you only need the <code>predict()</code> function. This format is generic and works with deployment tools like Docker, AWS SageMaker, or Azure ML.</p> <pre><code>import pandas as pd\n\n# Replace with your actual Run ID\nrun_id = \"YOUR_FITTED_RUN_ID\"\nlogged_model_uri = f\"runs:/{run_id}/model\"\n\n# Load generic model wrapper\nloaded_model = mlflow.pyfunc.load_model(logged_model_uri)\n\n# Predict \n# MLflow PyFunc generally expects DataFrame inputs for compatibility\n# It ensures columns match the signature provided during logging\ntest_df = pd.DataFrame(X_train[:5])\npredictions = loaded_model.predict(test_df)\n\nprint(predictions)\n</code></pre>"},{"location":"models/sup/mlflow/#method-b-load-native-model-research","title":"Method B: Load Native Model (Research)","text":"<p>Use this method if you need to inspect internal attributes like <code>weights</code>, <code>history</code>, or <code>re</code> (R-Entropy) that are specific to the Machine Gnostics object.</p> <pre><code>from machinegnostics.integration.mlflow_flavor import load_model\n\n# Download artifacts locally\nlocal_path = mlflow.artifacts.download_artifacts(\n    run_id=run_id, \n    artifact_path=\"model\"\n)\n\n# Load original Machine Gnostics object instance\nnative_model = load_model(local_path)\n\n# Now you have full access to the class attributes\nprint(f\"Model Degree: {native_model.degree}\")\nprint(f\"Model Weights: {native_model.weights}\")\nprint(f\"R-Entropy: {native_model.re}\")\n</code></pre>"},{"location":"models/sup/mlflow/#4-comparing-experiments-in-the-ui","title":"4. Comparing Experiments in the UI","text":"<p>You can visualize your runs to compare hyperparameters and performance metrics.</p> <ol> <li>Open your terminal in the directory where you ran the script.</li> <li>Run the command:     <pre><code>mlflow ui\n</code></pre></li> <li>Open your browser to http://localhost:5000.</li> </ol> <p>What to look for:</p> <ul> <li>Metrics: Compare <code>rentropy</code> (Residual Entropy) or <code>accuracy</code> across different runs to find the most robust model.</li> <li>Parameters: See which combinations of <code>degree</code> or regularization settings produced the best results.</li> <li>Artifacts: Download the saved model files or custom plots manually if needed.</li> </ul>"},{"location":"models/sup/mlflow/#5-model-registry-deployment","title":"5. Model Registry &amp; Deployment","text":"<p>The Model Registry acts as a centralized store for managing the lifecycle of your models (e.g., Staging vs. Production).</p>"},{"location":"models/sup/mlflow/#step-1-register-a-model","title":"Step 1: Register a Model","text":"<p>You can register a model programmatically from an existing run, assigning it a unique name.</p> <pre><code>model_name = \"Production_Gnostic_Classifier\"\nrun_id = \"BEST_RUN_ID_FROM_COMPARISON\"\n\n# Register the model\nregistered_model = mlflow.register_model(\n    model_uri=f\"runs:/{run_id}/model\",\n    name=model_name\n)\n\nprint(f\"Version: {registered_model.version}\")\n</code></pre>"},{"location":"models/sup/mlflow/#step-2-manage-stages","title":"Step 2: Manage Stages","text":"<p>Move models through lifecycle stages using the <code>MlflowClient</code>. For example, promoting a model to 'Production'.</p> <pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\n# Transition specific version to Production\nclient.transition_model_version_stage(\n    name=model_name,\n    version=1,\n    stage=\"Production\"\n)\n</code></pre>"},{"location":"models/sup/mlflow/#step-3-load-production-model","title":"Step 3: Load Production Model","text":"<p>In your production application (e.g., FastAPI service), load the model using its stage alias. This allows you to update the underlying model version without minimizing downtime or changing your application code.</p> <pre><code># Load specifically from the \"Production\" alias\nmodel_uri = f\"models:/{model_name}/Production\"\nproduction_model = mlflow.pyfunc.load_model(model_uri)\n\n# Ready to serve\nresult = production_model.predict(new_data)\n</code></pre>"},{"location":"models/sup/mlflow/#6-advanced-features","title":"6. Advanced Features","text":""},{"location":"models/sup/mlflow/#logging-custom-artifacts","title":"Logging Custom Artifacts","text":"<p>You can save plots, data arrays, or text files alongside your model for better documentation and auditability.</p> <pre><code>import matplotlib.pyplot as plt\n\nwith mlflow.start_run():\n    # ... training code ...\n\n    # 1. Save a plot locally\n    plt.plot(model._history) # Assuming history is suitable for plotting\n    plt.title(\"Optimization Trajectory\")\n    plt.savefig(\"optimization_curve.png\")\n\n    # 2. Log the image file to MLflow\n    mlflow.log_artifact(\"optimization_curve.png\")\n\n    # 3. Log raw weights for audit\n    np.save('weights.npy', model.weights)\n    mlflow.log_artifact('weights.npy')\n</code></pre>"},{"location":"models/sup/mlflow/#tags-and-notes","title":"Tags and Notes","text":"<p>Adding metadata helps organize large projects and provides context for team members.</p> <pre><code>with mlflow.start_run():\n    # Set searchable key-value tags\n    mlflow.set_tag(\"dataset\", \"sensor_data_v2\")\n    mlflow.set_tag(\"developer\", \"Nirmal\")\n    mlflow.set_tag(\"gnostic_type\", \"RobustRegression\")\n    mlflow.set_tag(\"production_ready\", \"yes\")\n\n    # Add a comprehensive description note\n    mlflow.set_tag(\"mlflow.note.content\", \n                   \"This model uses the new gnostic weighting algorithm to handle \"\n                   \"outliers in the sensor stream. Trained on 200 samples.\")\n</code></pre>"},{"location":"models/sup/mlflow/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Always use <code>infer_signature</code>: This ensures your model deployment knows exactly what input shape and types to expect.</li> <li>Log <code>input_example</code>: This stores a snippet of your data with the model, making it easier for consumers to understand the required input format.</li> <li>Track R-Entropy: Since Machine Gnostics focuses on minimizing gnostic entropy/information, always log <code>model.re</code> as a key metric.</li> <li>Use Context Managers: Always use <code>with mlflow.start_run():</code> to ensure runs are closed properly, even if exceptions occur during training.</li> </ol>"},{"location":"models/sup/train_test_split/","title":"train_test_split: Random Train/Test Data Splitter","text":"<p>The <code>train_test_split</code> function provides a simple and flexible way to split your dataset into random training and testing subsets. It is compatible with numpy arrays and can also handle lists or tuples as input. This function is essential for evaluating machine learning models on unseen data and is a core utility in most ML workflows.</p>"},{"location":"models/sup/train_test_split/#overview","title":"Overview","text":"<p>Splitting your data into training and testing sets is a fundamental step in machine learning. The <code>train_test_split</code> function allows you to:</p> <ul> <li>Randomly partition your data into train and test sets.</li> <li>Specify the proportion or absolute number of test samples.</li> <li>Shuffle your data for unbiased splitting.</li> <li>Use a random seed for reproducibility.</li> <li>Split both features (<code>X</code>) and targets (<code>y</code>) in a consistent manner.</li> </ul>"},{"location":"models/sup/train_test_split/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>X</code> array-like \u2014 Feature data to be split. Must be indexable and of consistent length. <code>y</code> array-like or None None Target data to be split alongside X. Must be same length as X. <code>test_size</code> float or int 0.25 If float, fraction of data for test set (0.0 &lt; test_size &lt; 1.0). If int, absolute number of test samples. <code>shuffle</code> bool True Whether to shuffle the data before splitting. <code>random_seed</code> int or None None Controls the shuffling for reproducibility."},{"location":"models/sup/train_test_split/#returns","title":"Returns","text":"<ul> <li> <p>X_train, X_test: <code>np.ndarray</code>   Train-test split of X.</p> </li> <li> <p>y_train, y_test: <code>np.ndarray</code> or <code>None</code>   Train-test split of y. If y is None, these will also be None.</p> </li> </ul>"},{"location":"models/sup/train_test_split/#raises","title":"Raises","text":"<ul> <li> <p>ValueError   If inputs are invalid or <code>test_size</code> is not appropriate.</p> </li> <li> <p>TypeError   If <code>test_size</code> is not a float or int.</p> </li> </ul>"},{"location":"models/sup/train_test_split/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.models import train_test_split\n\n# Create sample data\nX = np.arange(20).reshape(10, 2)\ny = np.arange(10)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, shuffle=True, random_seed=42\n)\n\nprint(\"X_train:\", X_train)\nprint(\"X_test:\", X_test)\nprint(\"y_train:\", y_train)\nprint(\"y_test:\", y_test)\n</code></pre>"},{"location":"models/sup/train_test_split/#notes","title":"Notes","text":"<ul> <li>If <code>y</code> is not provided, only <code>X</code> will be split and <code>y_train</code>, <code>y_test</code> will be <code>None</code>.</li> <li>If <code>test_size</code> is a float, it must be between 0.0 and 1.0 (exclusive).</li> <li>If <code>test_size</code> is an int, it must be between 1 and <code>len(X) - 1</code>.</li> <li>Setting <code>shuffle=False</code> will split the data in order, without randomization.</li> <li>Use <code>random_seed</code> for reproducible splits.</li> </ul> <p>Author: Nirmal Parmar Date: 2025-05-01</p>"},{"location":"ref/references/","title":"References","text":"<p>Machine Gnostics Publications</p> <p>Publications on Machine Gnostics will be available soon. This website is designed to provide the essential fundamentals to help you get started.</p> <p>Below is a curated list of key publications where the concept of Mathematical Gnostics is introduced, explained, and applied in research and practice.</p>"},{"location":"ref/references/#books","title":"Books","text":"<p> Kovanic, P.; Humber, M.B. The Economics of Information\u2014Mathematical Gnostics for Data Analysis. 2015. </p> <p> Kovanic, P. Mathematical Gnostics, 2023. DOI: 10.1201/9780429441196 </p>"},{"location":"ref/references/#research-papers","title":"Research Papers","text":"<p> Parmar, N.; Bendov\u00e1, M.; Wagner, Z., Heat capacity measurements by a Setaram \u03bcDSC3 evo microcalorimeter: Estimation of deviation in the measurement, advanced data analysis by mathematical gnostics, and prediction by the artificial neural network. J Therm Anal Calorim 150, 313\u2013325 (2025). https://doi.org/10.1007/s10973-024-13505-w </p> <p> Parmar, N.; Bendov\u00e1, M.; Wagner, Z.; P\u011bnkavov\u00e1, V.; Douihri, I.; Jacquemin, J., Carbon nanotube-based ionanofluids for efficient energy storage: Thermophysical properties\u2019 determination and advanced data analysis.  Industrial &amp; Engineering Chemistry Research 2021, 60 (20), 7714\u20137728. DOI: 10.1021/acs.iecr.0c06008 </p> <p> Parmar, N. et al., A study of changes in the heat capacity of carbon nanotube-based ionanofluids prepared from a series of imidazolium ionic liquids.  https://doi.org/10.1039/D2CP02110B </p> <p> Wagner, Z.; Bendov\u00e1, M.; Rotrekl, J.; Sykorova, A.; Canji, M.; Parmar, N., Density and sound velocity measurement by an Anton Paar DSA 5000 density meter: Precision and long-time stability.  J Mol Liq 329, 2021, 115547. ISSN 0167-7322. </p> <p> Wagner, Z.; Bendov\u00e1, M.; Rotrekl, J.; Parmar, N.; Koc\u0131, S.; Vrbka, P., Thermochemical properties of menthol and terpineol.  J Solution Chem 49, 1267\u20131278, 2020. </p>"},{"location":"stories/history/","title":"From Mathematical Gnostics to Machine Gnostics: A History","text":""},{"location":"stories/history/#introduction","title":"Introduction","text":"<p>Abstract</p> <p>Mathematical Gnostics is a unique approach to understanding data uncertainty\u2014one that challenges conventional statistical thinking. Rather than viewing uncertainty as indeterministic or purely random, Mathematical Gnostics treats every data point as the result of measurable, material causes, even if not all influencing factors are directly observable. This philosophy underpins the Machine Gnostics library, which brings these ideas into the realm of modern data analysis and machine learning.</p> <p>This page traces the origins of Mathematical Gnostics, its sources of inspiration, the birth of Machine Gnostics, and the impact these ideas are having today.</p>"},{"location":"stories/history/#historical-background","title":"Historical Background","text":"<p>The origins of Mathematical Gnostics trace back to the early 1980s, when Pavel Kovanic, a researcher at the Institute of Information Theory and Automation of the Czechoslovak Academy of Sciences in Prague, first published his pioneering work. His approach was met with skepticism by mainstream statisticians, as it departed radically from accepted paradigms. Nevertheless, the theory continued to develop and gain traction through both theoretical advances and practical applications.</p>"},{"location":"stories/history/#key-milestones","title":"Key Milestones","text":"<p>Timeline of Development</p> Era Milestone Late 1970s FoundationPavel Kovanic laid the foundation of Mathematical Gnostics, establishing a new paradigm of data variability. 1984 First PublicationThe first printed publications and presentation at an international conference, introducing the theory to the wider scientific community. 1990s Early Adopters &amp; SoftwarePractical applications in economics and financial analysis. Early private software implementations in BASIC, C, and S-PLUS. 2000s Open Source BeginningsThe first free/public software implementation in R, marking a shift towards open availability. 2015 International CollaborationCollaboration with Prof. M.B. Humber (USA), resulting in a co-authored book and further dissemination of the theory. 2017 Scientific Analysis ToolsZ. Wagner developed the Octave gnostic software, crucial for analyzing atmospheric aerosols at the Czech Academy of Sciences. 2018\u20132022 Research ExpansionApplication of gnostic methods expanded into environmental and medical research, including major EU projects. Late 2022 The Machine Learning ConnectionNirmal Parmar identified the link between Mathematical Gnostics and Machine Learning, initiating explorations into non-statistical AI (Machine Gnostics). 2023 Machine Gnostics &amp; LegacyPavel Kovanic published his paperback book shortly before his passing. Concurrently, Machine Gnostics was born, bridging the theory with modern AI. <p>For more information on the history and development of mathematical gnostics, visit: math-gnostics.eu</p>"},{"location":"stories/history/#sources-of-inspiration","title":"Sources of Inspiration","text":"<p>Unlike traditional statistical methods, which rely on large data samples and probabilistic assumptions, Mathematical Gnostics is rooted in the laws of nature and the analysis of individual data items\u2014even in small samples. This non-statistical approach treats data uncertainty as a consequence of real, measurable conditions, not as mere randomness.</p> <p>The development of Mathematical Gnostics was inspired by both the limitations of classical statistics and the foundational principles of several scientific disciplines:</p> Theory of General Systems The gnostic cycle of observation and feedback, emphasizing knowledge (from the Greek \u201cgnosis\u201d) rather than a priori assumptions. Theory of Measurement H. von Helmholtz\u2019s work on quantification and the mathematical structure of measurement. Geometry The use of non-Euclidean geometries (Riemannian, Minkowskian) to model uncertainty and variability. Relativistic Physics Insights from Einstein\u2019s mechanics, connecting data variability to the movement of relativistic particles. Thermodynamics The original thermodynamic concept of entropy, as introduced by R. Clausius, applied to data uncertainty. Electromagnetism J.C. Maxwell\u2019s theories, including the concept of Maxwell\u2019s demon, linking entropy and information. Matrix Algebra The manipulation of data structures and operations, as formalized in modern algebra and implemented in computational tools. <p>These inspirations, drawn from the natural sciences, provide a robust and universal foundation for understanding data uncertainty\u2014making Mathematical Gnostics a powerful alternative to classical statistics.</p> <p>Core Philosophy</p> <p>\u201cLet data speak for themselves.\u201d</p> <p>This guiding principle reflects the core philosophy of Mathematical Gnostics: to extract maximum information from data, relying on the data values themselves, and to model uncertainty in a way that is consistent with the laws of nature.</p>"},{"location":"stories/history/#the-birth-of-machine-gnostics","title":"The Birth of Machine Gnostics","text":"<p>The realization that the robust, nature-inspired foundation of Mathematical Gnostics could serve as the basis for a new generation of artificial intelligence and machine learning was a key motivation for the creation of Machine Gnostics. In 2022, Dr. Nirmal Parmar began exploring the integration of mathematical gnostics with modern machine learning, seeking to create models and algorithms that are both assumption-free and deeply aligned with the laws of nature.</p> <p>This work led to the birth of Machine Gnostics: an open source project dedicated to providing a unified framework for data analysis models, machine learning models, and\u2014looking ahead\u2014a future deep learning framework. Machine Gnostics aims to empower researchers, engineers, and practitioners with tools that combine the rigor of mathematical gnostics with the flexibility and power of contemporary AI.</p> <p>Open Source Vision</p> <p>By making Machine Gnostics open source, the project invites collaboration and innovation from the global community, ensuring that the theory and its applications continue to evolve and serve a wide range of scientific and practical needs.</p> <p>The impact of this integration is already being felt, and the vision for Machine Gnostics continues to grow.</p>"},{"location":"stories/history/#impact-and-vision","title":"Impact and Vision","text":"<p>The journey from Mathematical Gnostics to Machine Gnostics marks a significant shift in how we approach data uncertainty, analysis, and artificial intelligence. By grounding our methods in the laws of nature and embracing a non-statistical, axiomatic foundation, we have opened new avenues for robust, assumption-free data science. Machine Gnostics is already empowering researchers and practitioners to extract deeper insights from data\u2014whether in small samples, complex systems, or real-world applications where traditional statistics fall short.</p> <p>Looking ahead, the vision for Machine Gnostics is ambitious: to become a universal framework for data analysis, machine learning, and, ultimately, deep learning. By remaining open source and community-driven, Machine Gnostics invites collaboration, innovation, and critical feedback from scientists, engineers, and thinkers worldwide. Together, we can continue to push the boundaries of what is possible in data-driven discovery.</p>"},{"location":"stories/history/#testimonials","title":"Testimonials","text":"<p>Dr. Zden\u011bk Wagner \u2014 'Listening to the data'</p> <p>New discoveries and paradigms usually arrive when there is a need to get a solution to an unsolvable problem. They have their time to come. Many years ago I had to solve a complex task, regression of high pressure vapour-liquid equilibria (HPVLE). It poses a lot of difficulties with convergence because at the mixture critical point some derivatives are infinite, with slightly modified interaction parameters in the mixing rules the equations have no solution, and, last but not least, values measured by different authors are not always in agreement and thus oscillations in the iterative algorithm can occur. The statistical tools did not work for me. At that time I visited a seminar and met Dr. Pavel Kovanic who presented his novel paradigm of uncertainty, mathematical gnostics (MG). We discussed almost the whole night because it looked promising, especially with the key rule, let the data speak for themselves, i.e. do not force them to have a particular distribution of measurement errors but get the shape of the distribution function from the data. We met uncountable number of time after the seminar and I received full support from him. I first tried MG on simple tasks, I made my first three programs (the next was always a little more complex and more versatile than the preceding one) and found that it really can solve the tasks better.</p> <p>It was not easy to persuade statisticians that MG is a valid tool especially because HPVLE was still difficult. And another need came. A colleague who dealt with atmospheric aerosols had twenty thousand data sets and did not know how to analyze them. He told me that he would believe MG if I analyze the data and get useful results. And after some time I told him that there is something strange every day approximately half an hour before midnight and half an hour after midnight. And his response was: \u201cOh, these bad guys!\u201d The data were measured at the top of a mountain by several groups and many instruments. They agreed that they would leave the cars below the mountain in order not to take samples of the exhaust gases. People from one group had to change the filters in the instrument exactly at the midnight and they decided to go up to the top by cars because nobody sees them. And analysis of the data by MG found it. Since then I have analyzed many millions of data sets for the aerosol community.</p> <p>Yet another need came when we were in danger of losing a job. At that time my new colleague, Dr. Magdalena Bendov\u00e1, got interest in MG. We decided that we should show that we have special knowledge. Being inspired by Nassim Nicholas Taleb we turned MG into our \u201cblack swan\u201d. Explaining her this new paradigm helped me to realize what the reviewers would understand. Together we managed to promote MG so that it is now accepted by reviewers of good scientific journals. MG then became our main tool for data analysis and recently Magdalena's PhD student, Dr. Nirmal Parmar, opens a new horizon for novel applications. And I still learn how to teach this subject.</p> <p>Dr. Magdalena Bendov\u00e0 \u2014 'My Journey with Mathematical Gnostics'</p> <p>When I joined the Eduard H\u00e1la Laboratory of Thermodynamics in 2003, I had no idea that something like the theory of Mathematical Gnostics even existed. It was only when I began discussing how to analyze the phase equilibrium data I had just measured with my colleague, Dr. Zden\u011bk Wagner, that the path revealed itself. He suggested trying a robust regression method to estimate parameters of the thermodynamic models I proposed to use, using MG. That led to our first paper together and to my introduction to both the method and its author, Dr. Pavel Kovanic.</p> <p>In that first publication, Zden\u011bk tried his best to present the robust regression along a gnostic influence function in the simplest way possible, just enough so that the reviewers that were used to using statistical methods wouldn\u2019t get too mad at us. Years have passed, and such precautions are no longer necessary.</p> <p>Over time, I learned how to use the custom-made scripts that Zden\u011bk had coded; to analyze my data, to critically assess it, and to detect outliers. This led to many more papers, each allowing us to push the boundaries of MG (and of the referees\u2019 comprehension) further. For my part, I know I have only scratched the surface of this remarkable theory. Yet I remain convinced that it is one of the most powerful tools in data analysis. Not a rival to statistics, but rather a complementary approach that fills the gaps where traditional statistical methods fall short.</p> <p>I\u2019m especially proud that our Ph.D. student, Nirmal Parmar, who has since soared toward his own horizons, embraced this theory and made it his own. He\u2019s now exploring how to combine machine learning with mathematical gnostics. It won\u2019t be smooth sailing. The theory is complex and the project ambitious. But I am confident that, with more successful implementations, it will reach a broader audience and gain recognition in both the scientific and industrial communities for its profound usefulness.</p>"},{"location":"stories/history/#acknowledgments","title":"Acknowledgments","text":"<p>Acknowledgments</p> <p>The development of Mathematical Gnostics and its evolution into Machine Gnostics would not have been possible without the dedication and insight of many individuals. I would like to express my deepest gratitude to:</p> <ul> <li> <p>Dr. Pavel Kovanic (1942\u20132023), for his foundational work and vision in creating Mathematical Gnostics. His legacy continues to inspire this project and the broader scientific community. His guidance and pioneering spirit remain a guiding light for all who build upon his work.</p> </li> <li> <p>Dr. Magdalena Bendov\u00e1, my PhD supervisor, for her guidance, encouragement, and support throughout my research journey.</p> </li> <li> <p>Dr. Zden\u011bk Wagner, my expert supervisor, whose expertise in Mathematical Gnostics and his development of the Octave software for data analysis were instrumental in my understanding of the field. His mentorship inspired me to extend these ideas further and integrate them with machine learning and artificial intelligence.</p> </li> </ul> <p>I am also grateful to all colleagues, collaborators, and students who have contributed ideas, feedback, and encouragement along the way. The open source community, with its spirit of sharing and innovation, continues to inspire the ongoing growth of this project.</p> <p>Dr. Nirmal Parmar</p> <p>If you are interested in contributing, collaborating, or simply learning more, we welcome you to join us on this journey.</p> <p>Explore the documentation, try the tools, and help shape the future of Machine Gnostics.</p>"},{"location":"tutorials/overview/","title":"Welcome to the Machine Gnostics Tutorials","text":"<p>These tutorials are designed to help you master the Machine Gnostics framework for robust, interpretable, and assumption-free data analysis and machine learning. Whether you are a beginner or an advanced user, you will find step-by-step guides, practical code examples, and conceptual explanations to accelerate your learning.</p>"},{"location":"tutorials/overview/#getting-started","title":"Getting Started","text":"<p>Before you begin, please ensure that Machine Gnostics is installed correctly in your environment.</p> <ul> <li>See the Installation Guide for setup instructions.</li> <li>Check out Tutorials Library for step-by-step instructions.</li> </ul> <p>Note</p> <p>Machine Gnostics is based on Mathematical Gnostics theorems. The procedures and inference of results may differ from standard statistical methods, offering new perspectives and robust diagnostics.</p>"},{"location":"tutorials/overview/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Data Analysis</p> <ul> <li>Gnostic Distribution Function</li> <li>Gnostic Data Tests</li> <li>Advanced Data Analysis</li> </ul> </li> <li> <p> Machine Learning</p> <ul> <li>Regression</li> <li>Classification</li> <li>Clustering</li> <li>Forecasting</li> <li>MLflow Integration</li> </ul> </li> <li> <p> Metrics</p> <ul> <li>Regular ML Metrics</li> <li>Gnostic Metrics</li> </ul> </li> <li> <p> Deep Learning</p> <ul> <li>Neural Networks</li> <li>Coming soon!</li> </ul> </li> </ul>"},{"location":"tutorials/overview/#example-notebooks","title":"Example Notebooks","text":"<p>Explore hands-on Jupyter notebooks for each topic in the Tutorials section.</p> <p>Ready to begin?</p> <p>Start with the first tutorial or jump to the topic that interests you most. Each tutorial is self-contained and includes code, explanations, and practical tips.</p>"},{"location":"tutorials/tutorials/","title":"Machine Gnostics \u2013 Example Gallery","text":"<p>Explore practical examples and Jupyter notebooks demonstrating the use of Machine Gnostics for data analysis and machine learning. </p> <p>Each example includes:</p> <ul> <li>Comprehensive code examples</li> <li>Theoretical background and explanations</li> <li>Immediate execution via Google Colab</li> <li>Source code access on GitHub</li> </ul>"},{"location":"tutorials/tutorials/#tutorials-library","title":"Tutorials Library","text":"Data Analysis ModelsMachine Learning ModelsIntegrations &amp; ToolsDeep Learning <ul> <li> <p> Deep Learning Models</p> <p>Advanced neural network architectures using Machine Gnostics.</p> <p>Coming soon!</p> <p> Read more about Magnet</p> </li> </ul> <p>Notebooks Source</p> <p>All tutorials are hosted in our GitHub repository. You can download them directly or run them in Colab.</p>"},{"location":"tutorials/tutorials/#metrics","title":"Metrics","text":"<ul> <li> <p> Machine Gnostics Basic Metrics</p> <p>Learn how to calculate and interpret core Gnostics metrics.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"},{"location":"tutorials/tutorials/#data-tests","title":"Data Tests","text":"<ul> <li> <p> Data Homogeneity Test</p> <p>Assess if your data comes from a single population or distribution.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Homoscedasticity Test</p> <p>Test for constant variance across your dataset (homoscedasticity).</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Data Membership Test</p> <p>Evaluate whether new data points belong to the training distribution.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"},{"location":"tutorials/tutorials/#advanced-analysis","title":"Advanced Analysis","text":"<ul> <li> <p> Gnostic Distribution Functions</p> <p>Visualize and analyze data using Gnostics Distribution Functions.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Marginal Cluster Analysis</p> <p>Perform clustering analysis using marginal distributions.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Marginal Interval Analysis</p> <p>Analyze data intervals and bounds with marginal analysis.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Uncertainty Interval Analysis</p> <p>Quantify and analyze uncertainty within your data intervals.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"},{"location":"tutorials/tutorials/#regression","title":"Regression","text":"<ul> <li> <p> Linear Regression</p> <p>Standard linear regression implementation.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Polynomial Regression</p> <p>Regression with polynomial features.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Wine Quality Regression</p> <p>Real-world example: Predicting wine quality.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Decision Tree Regressor</p> <p>Non-linear regression using Decision Trees.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Random Forest Regressor</p> <p>Ensemble regression using Random Forests.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Boosting Regressor</p> <p>Advanced regression using Boosting techniques.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"},{"location":"tutorials/tutorials/#classification","title":"Classification","text":"<ul> <li> <p> Logistic Regression</p> <p>Binary classification fundamentals.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Multiclass Classification</p> <p>Handling multiple classes in classification tasks.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Decision Tree Classifier</p> <p>Classification using Decision Tree algorithms.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Random Forest Classifier</p> <p>Robust classification with Random Forests.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Boosting Classifier</p> <p>High-performance classification using Boosting.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"},{"location":"tutorials/tutorials/#clustering","title":"Clustering","text":"<ul> <li> <p> KMeans Clustering</p> <p>Standard K-Means clustering implementation.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> Gnostic Local Clustering</p> <p>Clustering based on local Gnostic properties.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"},{"location":"tutorials/tutorials/#forecasting","title":"Forecasting","text":"<ul> <li> <p> AutoRegressor (AR)</p> <p>Time series forecasting with AutoRegression.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> ARIMA</p> <p>Forecasting with ARIMA models.</p> <p> Open in Colab \u00b7  GitHub</p> </li> <li> <p> SARIMA</p> <p>Seasonal ARIMA for complex time series.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"},{"location":"tutorials/tutorials/#mlflow","title":"MLflow","text":"<ul> <li> <p> MLflow Integration</p> <p>Track experiments and manage models with MLflow.</p> <p> Open in Colab \u00b7  GitHub</p> </li> </ul>"}]}