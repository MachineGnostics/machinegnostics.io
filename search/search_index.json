{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Gnostics","text":"<p>Welcome to Machine Gnostics, an innovative Python library designed to implement the principles of Mathematical Gnostics for robust data analysis, modeling, and inference. Unlike traditional statistical approaches that depend heavily on probabilistic assumptions, Machine Gnostics harnesses deterministic algebraic and geometric structures. This unique foundation enables the library to deliver exceptional resilience against outliers, noise, and corrupted data, making it a powerful tool for challenging real-world scenarios.</p> <p>Machine Gnostics is an open-source initiative that seeks to redefine the mathematical underpinnings of machine learning. While most conventional ML libraries are grounded in probabilistic and statistical frameworks, Machine Gnostics explores alternative paradigms\u2014drawing from deterministic algebra, information theory, and geometric methods. This approach opens new avenues for building robust, interpretable, and reliable analysis tools that can withstand the limitations of traditional models.</p> <p>Note</p> <p>As a pioneering project, Machine Gnostics invites users to adopt a fresh perspective and develop a new understanding of machine learning. The library is currently in its infancy, and as such, some features may require refinement and fixes. We are actively working to expand its capabilities, with new models and methods planned for the near future. Community support and collaboration are essential to realizing Machine Gnostics\u2019 full potential. Together, let\u2019s build a new AI grounded in a rational and resilient paradigm.</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Machine Gnostics offers a comprehensive suite of tools for robust analysis:</p> <ul> <li>Robust Regression Models \u2013 Polynomial regression models with gnostic-based weighting for optimal resilience to outliers</li> <li>Gnostic Metrics \u2013 Alternative evaluation metrics that provide more reliable performance assessment in the presence of corrupted data</li> <li>Mathematical Gnostics Calculations \u2013 Core implementations of gnostic statistics including robust measures of central tendency, dispersion, and correlation</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udee1\ufe0f Exceptional Outlier Resistance \u2013 Automatically detects and downweights anomalous observations without manual intervention</li> <li>\ud83d\udd0d Information-Theoretic Foundation \u2013 Based on rigorous mathematical principles rather than probabilistic assumptions</li> <li>\ud83d\udd27 Drop-in Replacements \u2013 Use gnostic alternatives to common statistical measures like mean, median, correlation</li> <li>\ud83d\udcca MLflow Integration \u2013 Seamless model tracking, versioning, and deployment</li> <li>\ud83e\uddea Scientifically Validated \u2013 Tested on real-world problems across multiple domains including thermodynamics, materials science, and engineering</li> </ul>"},{"location":"#references","title":"References","text":"<p>Books</p> <ul> <li>Kovanic P., Humber M.B.: The Economics of Information-Mathematical Gnostics for Data Analysis, book 717 pp., 2015</li> <li>Kovanic P., Mathematical Gnostics, 2023, DOI: 10.1201/9780429441196</li> </ul> <p>Research Papers</p> <ul> <li>Parmar, N., Bendov\u00e1, M. &amp; Wagner, Z. Heat capacity measurements by a Setaram \u03bcDSC3 evo microcalorimeter: estimation of deviation in the measurement, advanced data analysis by mathematical gnostics, and prediction by the artificial neural network. J Therm Anal Calorim 150, 313\u2013325 (2025). https://doi.org/10.1007/s10973-024-13505-w</li> <li>Nirmal Parmar, Magdalena Bendov\u00e1, Zden\u011bk Wagner, V\u011bra P\u011bnkavov\u00e1, Ilias Douihri, and Johan Jacquemin, Carbon Nanotube-Based Ionanofluids for Efficient Energy Storage: Thermophysical Properties\u2019 Determination and Advanced Data Analysis, Industrial &amp; Engineering Chemistry Research 2021 60 (20), 7714-7728 DOI: 10.1021/acs.iecr.0c06008</li> <li>Parmar, N. et. al, A study of changes in the heat capacity of carbon nanotube-based ionanofluids prepared from a series of imidazolium ionic liquids, https://doi.org/10.1039/D2CP02110B</li> <li>Zdenek Wagner, Magdalena Bendova, Jan Rotrekl, Adela Sykorova, Maja Canji, Nirmal Parmar, Density and sound velocity measurement by an Anton Paar DSA 5000 density meter: Precision and long-time stability, J Mol Liq, Volume 329, 2021, 115547, ISSN 0167-7322</li> <li>Zdenek Wagner, Magdalena Bendova, Jan Rotrekl, Nirmal Parmar, Stanislav Koc\u0131 and Pavel Vrbka Thermochemical Properties of Menthol and Terpineol. J Solution Chem 49, 1267\u20131278, 2020</li> </ul>"},{"location":"accuracy/","title":"accuracy_score: Classification Accuracy Metric","text":"<p>The <code>accuracy_score</code> function computes the accuracy of classification models by comparing predicted labels to true labels. It is a fundamental metric for evaluating the performance of classifiers in binary and multiclass settings.</p>"},{"location":"accuracy/#overview","title":"Overview","text":"<p>Accuracy is defined as the proportion of correct predictions among the total number of cases examined. It is a simple yet powerful metric for assessing how well a model is performing, especially when the classes are balanced.</p>"},{"location":"accuracy/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like or pandas Series Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series Estimated target values as returned by a classifier. Shape: (n_samples,) <ul> <li>Both <code>y_true</code> and <code>y_pred</code> can be numpy arrays, lists, or pandas Series.  </li> <li>If a pandas DataFrame is passed, a <code>ValueError</code> is raised (select a column instead).</li> </ul>"},{"location":"accuracy/#returns","title":"Returns","text":"<ul> <li>accuracy: <code>float</code>   The accuracy score as a float in the range [0, 1].</li> </ul>"},{"location":"accuracy/#raises","title":"Raises","text":"<ul> <li>ValueError </li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> </ul>"},{"location":"accuracy/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import accuracy_score\n\n# Example 1: Using lists\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(accuracy_score(y_true, y_pred))  # Output: 0.8\n\n# Example 2: Using pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(accuracy_score(df['true'], df['pred']))  # Output: 0.6666666666666666\n</code></pre>"},{"location":"accuracy/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>The accuracy metric is most informative when the dataset is balanced. For imbalanced datasets, consider additional metrics such as precision, recall, or F1 score.</li> </ul>"},{"location":"accuracy/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"architecture/","title":"Machine Gnostics Architecture","text":"<p>This diagram presents the conceptual architecture of the Machine Gnostics paradigm. Unlike traditional machine learning rooted in statistical theory, this new approach is built on the foundation of Mathematical Gnostics (MG)\u2014a finite, deterministic, and physically inspired framework.</p> <p></p>"},{"location":"architecture/#1-data-base-layer","title":"1. DATA (Base Layer)","text":"<p>The foundation of Machine Gnostics is DATA, interpreted differently from statistical frameworks:</p> <ul> <li>Each data point is a real event with individual importance and uncertainty.</li> <li>No reliance on large sample assumptions or population-level abstractions.</li> <li>Adheres to the principle: \u201cLet the data speak for themselves.\u201d</li> </ul>"},{"location":"architecture/#2-mathematical-gnostics","title":"2. Mathematical Gnostics","text":"<p>This is the theoretical base of the system. It replaces the assumptions of probability with deterministic modeling:</p> <ul> <li>Uses Riemannian geometry, Einsteinian relativity, vector bi-algebra, and thermodynamics.</li> <li>Models uncertainty at the level of individual events, not populations.</li> <li>Establishes a finite theory for finite data, with robust treatment of variability.</li> </ul>"},{"location":"architecture/#3-magcal-mathematical-gnostics-calculations","title":"3. MAGCAL (Mathematical Gnostics Calculations)","text":"<p>MAGCAL is the computational engine that enables gnostic inference:</p> <ul> <li>Performs deterministic, non-statistical calculations.</li> <li>Enables robust modeling using gnostic algebra and error geometry.</li> <li>Resilient to outliers, corrupted data, and distributional shifts.</li> </ul>"},{"location":"architecture/#4-models-metrics-magnet","title":"4. Models | Metrics | Magnet","text":"<p>This layer maps to familiar components of ML pipelines but with MG-specific logic:</p> <ul> <li>Models: Trained using MAGCAL with finite-event inference.</li> <li>Metrics: Evaluate using gnostic loss functions and event-level error propagation.</li> <li>Magnet: A novel neural architecture based on Mathematical Gnostics, avoiding probabilistic backpropagation and inspired by algebraic learning.</li> </ul>"},{"location":"architecture/#5-mlflow-integration","title":"5. mlflow Integration","text":"<p>Despite its theoretical novelty, Machine Gnostics fits smoothly into modern ML workflows:</p> <ul> <li>mlflow provides tracking, model registry, and reproducibility.</li> <li>Ensures that experiments and deployments align with standard ML practices.</li> </ul>"},{"location":"architecture/#6-machine-gnostics-integration-layer","title":"6. Machine Gnostics (Integration Layer)","text":"<p>This layer unifies all components into a working system:</p> <ul> <li>Integrates MAGCAL, Magnet, and MG-based components.</li> <li>Functions as a complete ML framework based on a deterministic, finite, and algebraic paradigm.</li> <li>Enables seamless data-to-model pipelines rooted in the principles of Mathematical Gnostics.</li> </ul>"},{"location":"architecture/#summary","title":"Summary","text":"Traditional ML (Statistics) Machine Gnostics Based on probability theory Based on deterministic finite theory Relies on large datasets Works directly with small datasets Uses averages and distributions Uses individual error and event modeling Rooted in Euclidean geometry Rooted in Riemannian geometry &amp; physics Vulnerable to outliers Robust to real-world irregularities"},{"location":"architecture/#further-reading","title":"Further Reading","text":"<ul> <li>Pavel Kovanic, Mathematical Gnostics (2023)</li> <li>P. Kovanic &amp; M.B. Humber, The Economics of Information (2015)</li> </ul> <p>Machine Gnostics is not just an alternative\u2014it is a new foundation for AI, capable of rational, robust, and interpretable data modeling.</p>"},{"location":"classification_report/","title":"classification_report: Classification Metrics Summary","text":"<p>The <code>classification_report</code> function generates a comprehensive summary of key classification metrics\u2014precision, recall, F1 score, and support\u2014for each class in your dataset. It supports both string and dictionary output formats, making it suitable for both human-readable reports and programmatic analysis.</p>"},{"location":"classification_report/#overview","title":"Overview","text":"<p>This function provides a detailed breakdown of classifier performance for each class, including:</p> <ul> <li>Precision: Proportion of positive identifications that were actually correct.</li> <li>Recall: Proportion of actual positives that were correctly identified.</li> <li>F1 Score: Harmonic mean of precision and recall.</li> <li>Support: Number of true instances for each class.</li> </ul> <p>It also computes weighted averages across all classes.</p>"},{"location":"classification_report/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>labels</code> array-like or None None List of labels to include in the report. If None, uses sorted unique labels from y_true and y_pred. <code>target_names</code> list of str or None None Optional display names matching the labels (same order). <code>digits</code> int 2 Number of digits for formatting output. <code>output_dict</code> bool False If True, return output as a dict. If False, return as a formatted string. <code>zero_division</code> {0, 1, 'warn'} 0 Value to return when there is a zero division (no predicted samples for a class)."},{"location":"classification_report/#returns","title":"Returns","text":"<ul> <li>report: <code>str</code> or <code>dict</code>   Text summary or dictionary of the precision, recall, F1 score, and support for each class.</li> </ul>"},{"location":"classification_report/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import classification_report\n\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\n\n# String report\nprint(classification_report(y_true, y_pred))\n\n# Dictionary report\nreport_dict = classification_report(y_true, y_pred, output_dict=True)\nprint(report_dict)\n</code></pre>"},{"location":"classification_report/#output-example","title":"Output Example","text":"<p>String Output:</p> <pre><code>Class             Precision    Recall   F1-score    Support\n==========================================================\n0                    1.00      0.50      0.67          2\n1                    0.00      0.00      0.00          1\n2                    1.00      1.00      1.00          2\n==========================================================\nAvg/Total            0.80      0.60      0.67          5\n</code></pre> <p>Dictionary Output:</p> <pre><code>{\n  '0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.67, 'support': 2},\n  '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n  '2': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2},\n  'avg/total': {'precision': 0.8, 'recall': 0.6, 'f1-score': 0.67, 'support': 5}\n}\n</code></pre>"},{"location":"classification_report/#notes","title":"Notes","text":"<ul> <li>The function uses <code>precision_score</code>, <code>recall_score</code>, and <code>f1_score</code> from the Machine Gnostics metrics module for consistency.</li> <li>If <code>target_names</code> is provided, its length must match the number of labels.</li> <li>For imbalanced datasets, the weighted average provides a more informative summary than the unweighted mean.</li> <li>The <code>zero_division</code> parameter controls the behavior when a class has no predicted samples.</li> </ul>"},{"location":"classification_report/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"contact/","title":"Contact","text":"<p>If you have any questions, suggestions, or issues related to Machine Gnostics, feel free to reach out. We welcome feedback from the community and are happy to assist you.</p> <p>Contact Person Dr. Nirmal Parmar \ud83d\udce7 machinegnostics@gmail.com</p> <p>We appreciate your interest in Machine Gnostics and look forward to collaborating with researchers, developers, and practitioners passionate about robust and interpretable machine learning.</p> <p>For issues or feature requests, please consider opening a ticket on our GitHub repository.</p>"},{"location":"cross_val/","title":"CrossValidator: Custom k-Fold Cross-Validation","text":"<p>The <code>CrossValidator</code> class provides a simple, flexible implementation of k-fold cross-validation for evaluating machine learning models. It is designed to work with any model that implements <code>fit(X, y)</code> and <code>predict(X)</code> methods, and supports custom scoring functions for regression or classification tasks.</p>"},{"location":"cross_val/#overview","title":"Overview","text":"<p>Cross-validation is a robust technique for assessing the generalization performance of machine learning models. The <code>CrossValidator</code> class splits your dataset into <code>k</code> folds, trains the model on <code>k-1</code> folds, and evaluates it on the remaining fold, repeating this process for each fold. The results are aggregated to provide a reliable estimate of model performance.</p>"},{"location":"cross_val/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model</code> object \u2014 A machine learning model with <code>fit(X, y)</code> and <code>predict(X)</code> methods. <code>X</code> array-like \u2014 Feature matrix of shape <code>(n_samples, n_features)</code>. <code>y</code> array-like \u2014 Target labels of shape <code>(n_samples,)</code>. <code>k</code> int 5 Number of folds for cross-validation. <code>shuffle</code> bool True Whether to shuffle the dataset before splitting into folds. <code>random_seed</code> int/None None Seed for reproducible shuffling (ignored if <code>shuffle=False</code>)."},{"location":"cross_val/#attributes","title":"Attributes","text":"<ul> <li>folds: <code>list of tuple</code>   List of <code>(train_indices, test_indices)</code> for each fold.</li> </ul>"},{"location":"cross_val/#methods","title":"Methods","text":""},{"location":"cross_val/#split","title":"<code>split()</code>","text":"<p>Splits the dataset into <code>k</code> folds.</p> <ul> <li>Returns:   <code>folds</code>: list of tuple   Each tuple contains <code>(train_indices, test_indices)</code> for a fold.</li> </ul>"},{"location":"cross_val/#evaluatescoring_func","title":"<code>evaluate(scoring_func)</code>","text":"<p>Performs k-fold cross-validation and returns evaluation scores.</p> <ul> <li>Parameters:<code>scoring_func</code>: callableA function that takes <code>y_true</code> and <code>y_pred</code> and returns a numeric score (e.g., <code>mean_squared_error</code>, <code>accuracy_score</code>).</li> <li>Returns:   <code>scores</code>: list of float   Evaluation scores for each fold.</li> </ul>"},{"location":"cross_val/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models import CrossValidator, LinearRegressor\nfrom machinegnostics.metircs import mean_squared_error\nimport numpy as np\n\n# Generate random data\nX = np.random.rand(100, 10)\ny = np.random.rand(100)\n\n# Initialize model and cross-validator\nmodel = LinearRegressor()\ncv = CrossValidator(model, X, y, k=5, shuffle=True, random_seed=42)\n\n# Evaluate using mean squared error\nscores = cv.evaluate(mean_squared_error)\nprint(\"Cross-Validation Scores:\", scores)\nprint(\"Mean Score:\", np.mean(scores))\n</code></pre>"},{"location":"cross_val/#notes","title":"Notes","text":"<ul> <li>The model is re-initialized and trained from scratch for each fold.</li> <li>Supports any model with <code>fit</code> and <code>predict</code> methods.</li> <li>Works with any scoring function that accepts <code>y_true</code> and <code>y_pred</code>.</li> <li>Shuffling with a fixed <code>random_seed</code> ensures reproducible splits.</li> </ul>"},{"location":"cross_val/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"divI/","title":"divI: Divergence Information (DivI) Metric","text":"<p>The <code>divI</code> function computes the Divergence Information (DivI) metric, a robust measure for evaluating the divergence between observed data and model predictions. This metric is based on gnostic characteristics and is particularly useful for assessing the quality of model fits, especially in the presence of noise or outliers.</p>"},{"location":"divI/#overview","title":"Overview","text":"<p>Divergence Information (DivI) quantifies how much the information content of the predicted values diverges from that of the true values. Unlike classical divergence measures, DivI leverages gnostic algebra, making it robust to irregularities and non-Gaussian data.</p> <p>Mathematically, DivI is defined as:</p> \\[ \\text{DivI} = \\frac{1}{N} \\sum_{i=1}^N \\frac{I(y_i)}{I(\\hat{y}_i)} \\] <p>where:</p> <ul> <li>\\(I(y_i)\\) is the E-information of the observed value \\(y_i\\),</li> <li>\\(I(\\hat{y}_i)\\) is the E-information of the fitted value \\(\\hat{y}_i\\),</li> <li>\\(N\\) is the number of data points.</li> </ul> <p>DivI compares the information content of the dependent variable and its fit. The better the fit, the closer DivI is to 1. If the fit is highly uncertain or poor, DivI decreases.</p>"},{"location":"divI/#interpretation","title":"Interpretation","text":"<ul> <li>Higher DivI: Indicates that the fitted values retain more of the information content of the observed data, suggesting a better model fit.</li> <li>Lower DivI: Indicates greater divergence between the distributions of the observed and fitted values, suggesting a poorer fit or higher uncertainty in the model.</li> </ul> <p>DivI is particularly useful in robust model evaluation, as it is less sensitive to outliers and non-normal data distributions.</p>"},{"location":"divI/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>."},{"location":"divI/#returns","title":"Returns","text":"<ul> <li>float   The computed Divergence Information (DivI) value.</li> </ul>"},{"location":"divI/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"divI/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import divI\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = divI(y, y_fit)\nprint(result)  # Output: 0.06666666666666667\n</code></pre>"},{"location":"divI/#notes","title":"Notes","text":"<ul> <li>DivI is calculated using gnostic characteristics, providing a robust way to measure divergence between distributions.</li> <li>The metric is especially useful for model evaluation in real-world scenarios where data may be noisy or contain outliers.</li> <li>In the context of model evaluation, DivI is often used alongside other criteria such as Robust R-squared (RobR2) and the Geometric Mean of Multiplicative Fitting Errors (GMMFE) to provide a comprehensive assessment of model performance.</li> </ul>"},{"location":"divI/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis</li> </ul>"},{"location":"divI/#license","title":"License","text":"<p>Machine Gnostic - Machine Gnostics Library Copyright (C) 2025  Machine Gnosticsg Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"evalmet/","title":"evalMet: Composite Evaluation Metric","text":"<p>The <code>evalMet</code> function computes the Evaluation Metric (EvalMet), a composite score that combines three robust criteria\u2014Robust R-squared (RobR2), Geometric Mean of Model Fit Error (GMMFE), and Divergence Information (DivI)\u2014to provide a comprehensive assessment of model performance.</p>"},{"location":"evalmet/#overview","title":"Overview","text":"<p>EvalMet is designed to quantify the overall quality of a model fit by integrating three complementary metrics:</p> <ul> <li>RobR2: Measures the proportion of variance explained by the model, robust to outliers.</li> <li>GMMFE: Captures the average multiplicative fitting error on a logarithmic scale.</li> <li>DivI: Quantifies the divergence in information content between the observed data and the model fit.</li> </ul> <p>The combined metric is calculated as:</p> \\[ \\text{EvalMet} = \\frac{\\text{RobR2}}{\\text{GMMFE} \\cdot \\text{DivI}} \\] <p>A higher EvalMet value indicates a better model fit, balancing explained variance, error magnitude, and information divergence.</p>"},{"location":"evalmet/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y</code> np.ndarray \u2014 Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray \u2014 Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>w</code> np.ndarray None Optional weights for data points. 1D array, same shape as <code>y</code>. If not provided, equal weights are used."},{"location":"evalmet/#returns","title":"Returns","text":"<ul> <li>float   The computed Evaluation Metric (EvalMet) value.</li> </ul>"},{"location":"evalmet/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>w</code> is provided and does not have the same shape as <code>y</code>.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"evalmet/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import evalMet\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = evalMet(y, y_fit)\nprint(result)\n</code></pre>"},{"location":"evalmet/#notes","title":"Notes","text":"<ul> <li>EvalMet is most informative when used to compare multiple models or methods on the same dataset.</li> <li>The metric is robust to outliers and non-Gaussian data due to its use of gnostic algebra.</li> <li>EvalMet is especially useful in benchmarking and model selection scenarios, as it integrates multiple aspects of fit quality into a single score.</li> </ul>"},{"location":"evalmet/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis</li> </ul>"},{"location":"evalmet/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"examples/","title":"Examples \u2013 Machine Gnostics","text":"<p>Explore practical examples and Jupyter notebooks demonstrating the use of Machine Gnostics for various machine learning tasks. Each example includes code, explanations, and links to downloadable notebooks.</p>"},{"location":"examples/#example-notebooks","title":"Example Notebooks","text":"<ol> <li> <p>Small Data Regression \u2013 Linear Regression A simple linear regression example using a small dataset to illustrate the basics of model fitting and evaluation.</p> </li> <li> <p>Wine Quality: Multidimensional Linear Regression Regression on the wine quality dataset with multiple input features (X), showcasing how to handle multivariate data using linear regression.</p> </li> <li> <p>Small Data Polynomial Regression Polynomial regression on a small dataset, demonstrating how to fit and evaluate nonlinear relationships.</p> </li> <li> <p>Wine Quality: Multidimensional Polynomial Regression Polynomial regression applied to the wine quality dataset with multiple features, highlighting advanced regression techniques.</p> </li> <li> <p>Basic Binary Logistic Regression A straightforward binary classification example using logistic regression, including model training and evaluation.</p> </li> <li> <p>Logistic Regression with MLflow Integration An end-to-end example of logistic regression with experiment tracking and reproducibility using MLflow.</p> </li> </ol>"},{"location":"examples/#access-the-notebooks","title":"Access the Notebooks","text":"<p>You can download or view the Jupyter notebooks for each example from the examples directory in the repository.</p>"},{"location":"f1_score/","title":"f1_score: Classification F1 Score Metric","text":"<p>The <code>f1_score</code> function computes the F1 score for classification models, supporting both binary and multiclass settings. The F1 score is the harmonic mean of precision and recall, providing a balanced measure that is especially useful when classes are imbalanced.</p>"},{"location":"f1_score/#overview","title":"Overview","text":"<p>The F1 score combines precision and recall into a single metric by taking their harmonic mean.</p> <p>This metric is particularly important when you want to balance the trade-off between precision and recall, such as in information retrieval, medical diagnosis, and fraud detection.</p>"},{"location":"f1_score/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred."},{"location":"f1_score/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the F1 score for each class as an array.</li> </ul>"},{"location":"f1_score/#returns","title":"Returns","text":"<ul> <li>f1: <code>float</code> or <code>array of floats</code>   F1 score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of F1 values for each class.</li> </ul>"},{"location":"f1_score/#raises","title":"Raises","text":"<ul> <li>ValueError </li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"f1_score/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import f1_score\n\n# Example 1: Macro-averaged F1 for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(f1_score(y_true, y_pred, average='macro'))  # Output: 0.7777777777777777\n\n# Example 2: Binary F1 with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(f1_score(df['true'], df['pred'], average='binary'))  # Output: 0.8\n</code></pre>"},{"location":"f1_score/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"f1_score/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"foundation/","title":"Foundations of Mathematical Gnostics","text":"<p>Mathematical Gnostics (MG) offers a fundamentally different approach to data analysis and uncertainty compared to traditional mathematical statistics. Understanding these differences is crucial for users of the Machine Gnostics library, as it shapes the philosophy, algorithms, and practical outcomes of gnostic-based data analysis.</p>"},{"location":"foundation/#key-differences-between-statistics-and-gnostics","title":"Key Differences Between Statistics and Gnostics","text":""},{"location":"foundation/#1-focus-on-the-individual-event","title":"1. Focus on the Individual Event","text":"<ul> <li>Statistics: Traditional statistics investigates the regularities and properties of large collections of uncertain events, relying on the Law of Large Numbers and the Central Limit Theorem. The theory is built for infinite or very large sample sizes, and results for finite datasets are often extrapolated from these infinite models.</li> <li>Gnostics:   MG concentrates on the uncertainty of a single event. It builds mathematical and physical models that directly address finite (even small) collections of uncertain events. This approach is more natural for real-world scenarios, where data is always finite.</li> </ul>"},{"location":"foundation/#2-treatment-of-data-and-uncertainty","title":"2. Treatment of Data and Uncertainty","text":"<ul> <li>Statistics: Assumes the existence of a mean and standard deviation for an underlying probability distribution. Data is often treated as samples from an idealized random process, and analysis is based on population-level properties.</li> <li>Gnostics:   Treats each data point as an image of a real, existing event governed by the laws of nature. MG respects the actual values of the data, following the principle: \u201cLet data speak for themselves.\u201d The weight or importance of each data item is determined by its own individual error, not by its class or family.</li> </ul>"},{"location":"foundation/#3-mathematical-and-physical-foundations","title":"3. Mathematical and Physical Foundations","text":"<ul> <li>Statistics: Relies primarily on Euclidean geometry and Newtonian mechanics, with mathematical theory of measure as its foundation.</li> <li>Gnostics:   Utilizes Riemannian geometry and Einstein\u2019s relativistic mechanics, along with vector bi-algebra and thermodynamics. MG also introduces quantification theory as a foundational measurement theory.</li> </ul>"},{"location":"foundation/#4-aggregation-and-analysis","title":"4. Aggregation and Analysis","text":"<ul> <li>Statistics: Aggregates observed data additively, focusing on population-level summaries.</li> <li>Gnostics:   Suggests that additive aggregation should be applied to the parameters of the Ideal Gnostic Cycle, not directly to the observed data.</li> </ul>"},{"location":"foundation/#scientific-bases-a-comparative-diagram","title":"Scientific Bases: A Comparative Diagram","text":"<p>Figure: The scientific foundations of statistics (left) and gnostics (right) span mathematics, physics, geometry, and measurement theory, but differ fundamentally in their approach and underlying principles. [Pavel Kovanic, Mathematical Gnostics (2023)]</p>"},{"location":"foundation/#approaches-to-data-uncertainty","title":"Approaches to Data Uncertainty","text":"<p>Figure: Statistics builds its theory for infinite sample sizes and extrapolates results for finite datasets. Gnostics, in contrast, constructs its theory directly for finite (even single) events, providing a more natural fit for real-world data. [Pavel Kovanic, Mathematical Gnostics (2023)]</p>"},{"location":"foundation/#paradigm-shift-from-statistics-to-gnostics","title":"Paradigm Shift: From Statistics to Gnostics","text":"<p>Mathematical gnostics represents a paradigm shift in how we approach data variability and analysis:</p> <ul> <li>Statistics is rooted in the behavior of large numbers and infinite limits, often requiring extrapolation to address finite datasets.</li> <li>Gnostics is designed for the finite world, modeling uncertainty at the level of individual events and small datasets.</li> </ul> <p>This shift requires a new way of thinking, much like moving from Newtonian to Einsteinian physics. While statistics is easily demonstrated with simple experiments (like coin tosses), the power of gnostics is revealed through its algorithms and their performance on real-world, finite data.</p>"},{"location":"foundation/#principles-of-the-gnostic-paradigm","title":"Principles of the Gnostic Paradigm","text":"<ol> <li>Concentration on Individual Events: MG focuses on the regularities and uncertainty of individual events, not just large populations.</li> <li>Respect for Data Values: Data is taken as it is, with each value carrying its own information and uncertainty.</li> <li>Use of Advanced Geometry and Mechanics: MG employs Riemannian geometry and relativistic mechanics, providing a richer mathematical framework for modeling uncertainty.</li> <li>Individual Error Weighting:    The importance of each data point is determined by its own error, not by group-level properties.</li> </ol>"},{"location":"foundation/#why-adopt-the-gnostic-approach","title":"Why Adopt the Gnostic Approach?","text":"<ul> <li>Natural Fit for Finite Data: Real-world data is always finite. MG provides tools and theory that are directly applicable without relying on extrapolation from infinite models.</li> <li>Robustness: By focusing on individual data points and their uncertainties, MG offers greater resilience to outliers and corrupted data.</li> <li>Paradigm-Changing Power:   MG overcomes many limitations of traditional statistics, especially in cases where statistical assumptions break down.</li> </ul>"},{"location":"foundation/#further-reading","title":"Further Reading","text":"<p>For a deeper dive into the foundations and applications of mathematical gnostics, see:</p> <ul> <li>Pavel Kovanic, Mathematical Gnostics (2023)</li> <li>Pavel Kovanic &amp; M.B. Humber, The Economics of Information: Mathematical Gnostics for Data Analysis (2015)</li> </ul> <p>Mathematical Gnostics is a new paradigm for data analysis\u2014one that respects the individuality of data, leverages advanced mathematics, and is designed for the finite, real world.</p>"},{"location":"g_correlation/","title":"gcorrelation: Gnostic Correlation Metric","text":"<p>The <code>gcorrelation</code> function computes the Gnostic correlation coefficient between two data samples using robust irrelevance-based weighting. This metric provides a robust alternative to the classical Pearson correlation, making it less sensitive to outliers and non-normal data distributions.</p>"},{"location":"g_correlation/#overview","title":"Overview","text":"<p>Gnostic correlation leverages irrelevance functions to construct robust weights for each data point, following the gnostic framework described by Kovanic &amp; Humber (2015). This approach allows for a more reliable measure of association between variables, especially in the presence of noise or outliers.</p> <ul> <li>Robust to outliers: Uses irrelevance-based weighting.</li> <li>No normality assumption: Works well with non-Gaussian data.</li> <li>Flexible: Supports both 1D and 2D data (column-wise correlation).</li> </ul>"},{"location":"g_correlation/#parameters","title":"Parameters","text":"Parameter Type Description <code>data_1</code> np.ndarray, pandas Series, or DataFrame First data sample (1D or 2D). Each column is treated as a variable. <code>data_2</code> np.ndarray, pandas Series, or DataFrame Second data sample (must have same number of rows as <code>data_1</code>)."},{"location":"g_correlation/#returns","title":"Returns","text":"<ul> <li>float, np.ndarray, or pandas.DataFrame   The calculated Gnostic correlation coefficient(s):</li> <li>If both inputs are 1D: returns a float.</li> <li>If either input is 2D: returns a correlation matrix (np.ndarray or pandas DataFrame if input was pandas).</li> </ul>"},{"location":"g_correlation/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If input arrays have different lengths.</li> <li>If inputs are empty or not numpy arrays/pandas Series/DataFrame.</li> <li>If input shapes are incompatible.</li> </ul>"},{"location":"g_correlation/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import gcorrelation\n\n# Example 1: 1D arrays (robust analog of Pearson correlation)\nx = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\ny = np.array([0.9, 2.1, 2.9, 4.2, 4.8])\ngcor = gcorrelation(x, y)\nprint(f\"Estimation correlation: {gcor:.3f}\")  # Output: Estimation correlation: 0.999\n\n# Example 2: DataFrames (column-wise correlation matrix)\nimport pandas as pd\ndf1 = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\ndf2 = pd.DataFrame({'c': [1, 2, 1], 'd': [6, 5, 4]})\ncorr_matrix = gcorrelation(df1, df2)\nprint(corr_matrix)\n</code></pre>"},{"location":"g_correlation/#notes","title":"Notes","text":"<ul> <li>The location parameter is set by the mean (can be replaced by G-median for higher robustness).</li> <li>The geometric mean of the weights is used as the \"best\" weighting vector.</li> <li>For 2D arrays or DataFrames, the function computes the correlation for each pair of columns.</li> <li>The output is a DataFrame with appropriate column and index names if the input was pandas.</li> </ul>"},{"location":"g_correlation/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis, Chapter 24</li> </ul>"},{"location":"g_correlation/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.  </p>"},{"location":"g_relevance/","title":"hc: Gnostic Characteristics (Hc) Metric","text":"<p>The <code>hc</code> function computes the Gnostic Characteristics (Hc) metric, a robust measure for evaluating the relevance or irrelevance between true and predicted values. Here, <code>c</code> denotes case of <code>i</code>- Estimation or <code>j</code> - Quantification condition.  This metric is part of the Machine Gnostics framework and is particularly useful for assessing model performance in the presence of noise or outliers.</p>"},{"location":"g_relevance/#overview","title":"Overview","text":"<p>The Hc metric quantifies the relationship between predicted and true values using gnostic algebra. It can be used in two modes:</p> <ul> <li>Relevance (<code>case='i'</code>): Measures how relevant the predictions are to the true values as per mathematical gnostics.</li> <li>Irrelevance (<code>case='j'</code>): Measures how irrelevant the predictions are to the true values as per mathematical gnostics.</li> </ul> <p>The metric is calculated as the normalized sum of squared gnostic characteristics, providing a robust alternative to classical error metrics.</p>"},{"location":"g_relevance/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like \u2014 True (ground truth) values. <code>y_pred</code> array-like \u2014 Predicted values from the model. <code>case</code> str 'i' Calculation mode:<code>'i'</code> for relevance, <code>'j'</code> for irrelevance."},{"location":"g_relevance/#returns","title":"Returns","text":"<ul> <li>float   The calculated Hc value (normalized sum of squared gnostic characteristics).</li> </ul>"},{"location":"g_relevance/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> and <code>y_pred</code> have different lengths.</li> <li>If <code>case</code> is not <code>'i'</code> or <code>'j'</code>.</li> </ul>"},{"location":"g_relevance/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import hc\n\ny_true = [1, 2, 3]\ny_pred = [1, 2, 3]\n\n# Calculate gnostic relevance\nhc_value = hc(y_true, y_pred, case='i')\nprint(hc_value)\n</code></pre>"},{"location":"g_relevance/#notes","title":"Notes","text":"<ul> <li>The function uses the <code>GnosticsCharacteristics</code> class from the Machine Gnostics library.</li> <li>The ratio \\(R = y_{\\text{true}} / y_{\\text{pred}}\\) is used to compute the gnostic characteristics.</li> <li>The result is normalized by the number of samples.</li> <li>Use <code>case='i'</code> for relevance and <code>case='j'</code> for irrelevance, depending on your analysis needs.</li> </ul>"},{"location":"g_relevance/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis</li> </ul>"},{"location":"g_relevance/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"gmmfe/","title":"gmmfe: Geometric Mean of Model Fit Error (GMMFE) Metric","text":"<p>The <code>gmmfe</code> function computes the Geometric Mean of Model Fit Error (GMMFE), a robust metric for evaluating the average relative error between observed data and model predictions on a logarithmic scale. GMMFE is especially useful for datasets with a wide range of values or when the data is multiplicative in nature.</p>"},{"location":"gmmfe/#overview","title":"Overview","text":"<p>GMMFE quantifies the average multiplicative error between the true and predicted values, making it less sensitive to outliers and scale differences than classical metrics. It is one of the three core criteria (alongside RobR2 and DivI) for evaluating model performance in the Machine Gnostics framework.</p> <p>Mathematically, GMMFE is defined as:</p> \\[ \\text{GMMFE} = \\exp\\left( \\frac{1}{N} \\sum_{i=1}^N \\left| \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) \\right| \\right) \\] <p>where:</p> <ul> <li>\\(y_i\\) is the observed value,</li> <li>\\(\\hat{y}_i\\) is the fitted (predicted) value,</li> <li>\\(N\\) is the number of data points.</li> </ul> <p>A lower GMMFE indicates a better fit, as it means the geometric mean of the relative errors is smaller.</p>"},{"location":"gmmfe/#interpretation","title":"Interpretation","text":"<ul> <li>Lower GMMFE: Indicates smaller average multiplicative errors and a better model fit.</li> <li>Higher GMMFE: Indicates larger average multiplicative errors and a poorer fit.</li> </ul> <p>GMMFE is particularly valuable when comparing models across datasets with different scales or when the error distribution is multiplicative.</p>"},{"location":"gmmfe/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>."},{"location":"gmmfe/#returns","title":"Returns","text":"<ul> <li>float   The computed Geometric Mean of Model Fit Error (GMMFE) value.</li> </ul>"},{"location":"gmmfe/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"gmmfe/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import gmmfe\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nresult = gmmfe(y, y_fit)\nprint(result)  # Output: 0.06666666666666667\n</code></pre>"},{"location":"gmmfe/#notes","title":"Notes","text":"<ul> <li>GMMFE is calculated using the weighted geometric mean of the relative errors.</li> <li>It is robust to outliers and scale differences, making it suitable for a wide range of regression problems.</li> <li>In the Machine Gnostics framework, GMMFE is used alongside RobR2 and DivI to provide a comprehensive evaluation of model performance.</li> <li>The overall evaluation metric can be computed as:</li> </ul> <p>$$   \\text{EvalMet} = \\frac{\\text{RobR2}}{\\text{GMMFE} \\cdot \\text{DivI}}   $$</p> <p>where a higher EvalMet indicates better model performance.</p>"},{"location":"gmmfe/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis</li> </ul>"},{"location":"gmmfe/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"installation/","title":"Installation Guide","text":"<p>Machine Gnostics is distributed as a standard Python package and is designed for easy installation and integration into your data science workflow. The library has been tested on macOS with Python 3.11 and is fully compatible with standard data science libraries such as NumPy, pandas, and SciPy.</p>"},{"location":"installation/#1-create-a-python-virtual-environment","title":"1. Create a Python Virtual Environment","text":"<p>It is best practice to use a virtual environment to manage your project dependencies and avoid conflicts with other Python packages.</p> <pre><code># Create a new virtual environment named 'machine-gnostics-env'\npython3 -m venv machinegnostics-env\n\n# Activate the environment (macOS/Linux)\nsource machinegnostics-env/bin/activate\n\n# (On Windows, use: machinegnostics-env\\Scripts\\activate)\n</code></pre>"},{"location":"installation/#2-install-machine-gnostics","title":"2. Install Machine Gnostics","text":"<p>Install the Machine Gnostics library using pip:</p> <pre><code>pip install machinegnostics\n</code></pre> <p>This command will install Machine Gnostics and automatically resolve its dependencies.</p>"},{"location":"installation/#3-optional-install-standard-data-science-libraries","title":"3. (Optional) Install Standard Data Science Libraries","text":"<p>If you do not already have them, install the most common data science libraries:</p> <pre><code>pip install numpy pandas scipy\n</code></pre>"},{"location":"installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>You can verify that Machine Gnostics and its dependencies are installed correctly by importing them in a Python session:</p> <pre><code>import machinegnostics\nimport numpy\n\nprint(\"All libraries imported successfully!\")\n</code></pre>"},{"location":"installation/#5-quick-usage-example","title":"5. Quick Usage Example","text":"<p>Machine Gnostics is designed to be as simple to use as other machine learning libraries. You can call its functions and classes directly after installation.</p> <pre><code>import numpy as np\nfrom machinegnostics.models import RobustRegressor\n\n# Example data\nX = np.array([[1], [2], [3], [4]])\ny = np.array([2, 4, 6, 8])\n\n# Create and fit a robust polynomial regression model\nmodel = RobustRegressor(degree=1)\nmodel.fit(X, y)\n\n# Make predictions\ny_pred = model.predict(X)\n\nprint(\"Predictions:\", y_pred)\n</code></pre>"},{"location":"installation/#6-platform-and-environment","title":"6. Platform and Environment","text":"<ul> <li>Operating System: Tested on macOS (Apple Silicon and Intel)</li> <li>Python Version: 3.11 recommended</li> <li>Dependencies: Compatible with NumPy, pandas, SciPy, and other standard data science libraries</li> </ul>"},{"location":"installation/#7-troubleshooting","title":"7. Troubleshooting","text":"<ul> <li>zEnsure your virtual environment is activated before installing or running Machine Gnostics.</li> <li>If you encounter issues, try upgrading pip:</li> </ul> <p><pre><code>pip install --upgrade pip\n</code></pre> - For further help, consult us or open an issue on the GitHub repository.</p> <p>Machine Gnostics is designed for simplicity and reliability, making robust machine learning accessible for all Python users.</p>"},{"location":"lin_reg/","title":"LinearRegressor: Robust Linear Regression with Machine Gnostics","text":"<p>LinearRegressor is a robust linear regression model built on the Machine Gnostics framework. Unlike traditional statistical models that rely on probabilistic assumptions, this model uses algebraic and geometric structures to provide deterministic, resilient, and interpretable regression for real-world data.</p>"},{"location":"lin_reg/#overview","title":"Overview","text":"<p>The Machine Gnostics LinearRegressor is designed for robust regression tasks, especially where data may contain outliers, noise, or non-Gaussian distributions. It leverages the core principles of Mathematical Gnostics (MG) to deliver reliable results even in challenging scenarios.</p> <ul> <li>Deterministic &amp; Finite: No randomness or probability; all computations are reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events.</li> <li>Algebraic Inference: Utilizes gnostic algebra and error geometry for robust learning.</li> <li>Resilient: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> <li>Flexible: Supports numpy arrays, pandas DataFrames, and pyspark DataFrames.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"lin_reg/#key-features","title":"Key Features","text":"<ul> <li>Fits a linear regression model (polynomial degree = 1)</li> <li>Least squares estimation for coefficient calculation</li> <li>Robust to outliers and non-Gaussian noise</li> <li>Training history tracking for analysis and visualization</li> <li>Customizable loss functions and scaling strategies</li> </ul>"},{"location":"lin_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>max_iter</code> int 100 Maximum number of training iterations. <code>tol</code> float 1e-3 Convergence threshold for loss or coefficient changes. <code>mg_loss</code> str 'hi' Type of gnostic loss:<code>'hi'</code> (estimation relevance), <code>'hj'</code> (irrelevance). <code>early_stopping</code> bool or int True Enables early stopping or sets window size. <code>verbose</code> bool False Prints progress and debug information. <code>scale</code> {'auto', float} 'auto' Scaling strategy for the gnostic loss. <code>history</code> bool True Records training history at each iteration. <code>data_form</code> str 'a' Input data form:<code>'a'</code> (additive), <code>'m'</code> (multiplicative)."},{"location":"lin_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>ndarray</code>Final learned regression coefficients.</li> <li>weights: <code>ndarray</code>Final sample weights after convergence.</li> <li>_history: <code>list of dict</code>   Training history, including iteration, loss, coefficients, rentropy, and weights.</li> </ul>"},{"location":"lin_reg/#methods","title":"Methods","text":""},{"location":"lin_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the model to training data using polynomial expansion and gnostic loss minimization.</p> <ul> <li>X: array-like, shape (n_samples, n_features)Input features (numpy, pandas, or pyspark DataFrame).</li> <li>y: array-like, shape (n_samples,)   Target values.</li> </ul>"},{"location":"lin_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts output values for new input samples using the trained model.</p> <ul> <li>X: array-like, shape (n_samples, n_features)Input features for prediction.</li> <li>Returns:   <code>y_pred</code>: ndarray, shape (n_samples,)   Predicted target values.</li> </ul>"},{"location":"lin_reg/#save_modelpath","title":"<code>save_model(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"lin_reg/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: strDirectory path where the model is saved.</li> <li>Returns:   Instance of <code>LinearRegressor</code> with loaded parameters.</li> </ul>"},{"location":"lin_reg/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models import LinearRegressor\n\n# Initialize the model\nmodel = LinearRegressor(max_iter=200, mg_loss='hi', verbose=True)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Access coefficients\nprint(\"Coefficients:\", model.coefficients)\n\n# Save the model\nmodel.save_model(\"my_model_dir/\")\n\n# Load the model\nloaded_model = LinearRegressor.load_model(\"my_model_dir/\")\n</code></pre>"},{"location":"lin_reg/#training-history","title":"Training History","text":"<p>The model records training history at each iteration, accessible via <code>model._history</code>.Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>h_loss</code>: Gnostic loss value</li> <li><code>coefficients</code>: Regression coefficients at this iteration</li> <li><code>rentropy</code>: Rentropy value</li> <li><code>weights</code>: Sample weights at this iteration</li> </ul> <p>This enables detailed analysis and visualization of the training process.</p>"},{"location":"lin_reg/#notes","title":"Notes","text":"<ul> <li>The model is robust to outliers and suitable for datasets with non-Gaussian noise.</li> <li>Supports integration with mlflow for experiment tracking and deployment.</li> <li>For more information, visit: https://machinegnostics.info/</li> <li>Source code: https://github.com/MachineGnostics/machinegnostics</li> </ul>"},{"location":"lin_reg/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p> <p>Author: Nirmal Parmar Date: 2025-10-01</p>"},{"location":"log_reg/","title":"LogisticRegressor: Robust Logistic Regression with Machine Gnostics","text":"<p>LogisticRegressor is a robust, flexible logistic regression model built on the Machine Gnostics framework. It is designed for binary classification tasks and is resilient to outliers, heavy-tailed distributions, and non-Gaussian noise. The model supports polynomial feature expansion, robust weighting, early stopping, and seamless MLflow integration for experiment tracking and deployment.</p>"},{"location":"log_reg/#overview","title":"Overview","text":"<p>The Machine Gnostics LogisticRegressor brings deterministic, event-level modeling to binary classification. It leverages gnostic algebra and geometry to provide robust, interpretable, and reproducible results, even in challenging real-world scenarios.</p> <ul> <li>Robust to Outliers: Gnostic weighting minimizes the influence of noisy or corrupted samples.</li> <li>Polynomial Feature Expansion: Configurable degree for flexible, nonlinear decision boundaries.</li> <li>Flexible Probability Output: Choose between gnostic-based or standard sigmoid probabilities.</li> <li>Early Stopping &amp; Convergence: Monitors loss and entropy for efficient training.</li> <li>MLflow Integration: For experiment tracking, reproducibility, and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"log_reg/#key-features","title":"Key Features","text":"<ul> <li>Robust to outliers and non-Gaussian noise</li> <li>Polynomial feature expansion (configurable degree)</li> <li>Flexible probability output: gnostic or sigmoid</li> <li>Customizable scaling of data (auto or manual)</li> <li>Early stopping based on residual entropy or log loss</li> <li>Full training history tracking (loss, entropy, coefficients, weights)</li> <li>MLflow integration for model tracking and deployment</li> <li>Save and load model using joblib</li> </ul>"},{"location":"log_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>degree</code> int 1 Degree of the polynomial for feature expansion (1 = linear). <code>max_iter</code> int 100 Maximum number of training iterations. <code>tol</code> float 1e-3 Convergence threshold for loss or coefficient changes. <code>scale</code> {'auto', float} 'auto' Scaling mode for gnostic transformation. <code>early_stopping</code> bool True Enables early stopping based on convergence criteria. <code>history</code> bool True Records training history at each iteration. <code>proba</code> {'gnostic','sigmoid'} 'gnostic' Probability output mode. <code>verbose</code> bool False Prints progress and debug information. <code>data_form</code> str 'a' Input data form:<code>'a'</code> (additive), <code>'m'</code> (multiplicative)."},{"location":"log_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>ndarray</code>Final learned polynomial regression coefficients.</li> <li>weights: <code>ndarray</code>Final sample weights after convergence.</li> <li>_history: <code>list of dict</code>   Training history, including loss, entropy, coefficients, and weights at each iteration.</li> </ul>"},{"location":"log_reg/#methods","title":"Methods","text":""},{"location":"log_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the model to training data using polynomial expansion and robust loss minimization.</p> <ul> <li>X: array-like, pandas.DataFrame, or numpy.ndarray of shape (n_samples, n_features)Training input samples.</li> <li>y: array-like or numpy.ndarray of shape (n_samples,)Target binary labels (0 or 1).</li> <li>Returns:   <code>self</code>: LogisticRegressor (for method chaining)</li> </ul>"},{"location":"log_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts class labels (0 or 1) for new input samples using the trained model.</p> <ul> <li>X: array-like, pandas.DataFrame, or numpy.ndarray of shape (n_samples, n_features)Input samples for prediction.</li> <li>Returns:   <code>y_pred</code>: numpy.ndarray of shape (n_samples,)   Predicted binary class labels.</li> </ul>"},{"location":"log_reg/#predict_probax","title":"<code>predict_proba(X)</code>","text":"<p>Predicts probabilities for new input samples using the trained model.</p> <ul> <li>X: array-like, pandas.DataFrame, or numpy.ndarray of shape (n_samples, n_features)Input samples for probability prediction.</li> <li>Returns:   <code>proba</code>: numpy.ndarray of shape (n_samples,)   Predicted probabilities for the positive class (label 1).</li> </ul>"},{"location":"log_reg/#save_modelpath","title":"<code>save_model(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"log_reg/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: strDirectory path where the model is saved.</li> <li>Returns:   Instance of <code>LogisticRegressor</code> with loaded parameters.</li> </ul>"},{"location":"log_reg/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models import LogisticRegressor\n\n# Initialize the model\nmodel = LogisticRegressor(degree=2, proba='gnostic', verbose=True)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict class labels\ny_pred = model.predict(X_test)\n\n# Predict probabilities\ny_proba = model.predict_proba(X_test)\n\n# Access coefficients and weights\nprint(\"Coefficients:\", model.coefficients)\nprint(\"Weights:\", model.weights)\n\n# Save the model\nmodel.save_model(\"my_logreg_model\")\n\n# Load the model\nloaded = LogisticRegressor.load_model(\"my_logreg_model\")\ny_pred2 = loaded.predict(X_test)\n</code></pre>"},{"location":"log_reg/#training-history","title":"Training History","text":"<p>The model records training history at each iteration, accessible via <code>model._history</code>.Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>loss</code>: Loss value (gnostic or log loss)</li> <li><code>entropy</code>: Residual entropy value</li> <li><code>coefficients</code>: Regression coefficients at this iteration</li> <li><code>weights</code>: Sample weights at this iteration</li> </ul> <p>This enables detailed analysis and visualization of the training process.</p>"},{"location":"log_reg/#notes","title":"Notes","text":"<ul> <li>The model supports numpy arrays, pandas DataFrames, and pyspark DataFrames as input.</li> <li>For best results, ensure input features are appropriately scaled and encoded.</li> <li>Supports integration with MLflow for experiment tracking and deployment.</li> <li>For more information, visit: https://machinegnostics.info/</li> <li>Source code: https://github.com/MachineGnostics/machinegnostics</li> </ul>"},{"location":"log_reg/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p> <p>Author: Nirmal Parmar Date: 2025-10-01</p>"},{"location":"mae/","title":"mean_absolute_error: Mean Absolute Error (MAE) Metric","text":"<p>The <code>mean_absolute_error</code> function computes the mean absolute error (MAE) between true and predicted values. MAE is a fundamental regression metric that measures the average magnitude of errors in a set of predictions, without considering their direction.</p>"},{"location":"mae/#overview","title":"Overview","text":"<p>Mean Absolute Error is defined as the average of the absolute differences between actual and predicted values.</p> <p>MAE is widely used in regression analysis to quantify how close predictions are to the actual outcomes. Lower MAE values indicate better model performance.</p>"},{"location":"mae/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values."},{"location":"mae/#returns","title":"Returns","text":"<ul> <li>float   The average absolute difference between actual and predicted values.</li> </ul>"},{"location":"mae/#raises","title":"Raises","text":"<ul> <li>TypeError   If <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"mae/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import mean_absolute_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_absolute_error(y_true, y_pred))  # Output: 0.5\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(mean_absolute_error(y_true, y_pred))  # Output: 0.3333333333333333\n</code></pre>"},{"location":"mae/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>MAE is robust to outliers but does not penalize large errors as strongly as mean squared error (MSE).</li> </ul>"},{"location":"mae/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"magnet/","title":"Magnet: Machine Gnostic Neural Network","text":"<p>Magnet is the Machine Gnostic (Neural) Network, a novel architecture inspired by the principles of Mathematical Gnostics (MG). Unlike traditional neural networks that rely on probabilistic backpropagation and statistical learning, Magnet is built on a deterministic, finite, and algebraic foundation.</p>"},{"location":"magnet/#key-features","title":"Key Features","text":"<ul> <li>Deterministic Learning: No reliance on probability or randomness; all computations are finite and reproducible.</li> <li>Event-Level Modeling: Handles uncertainty and error at the level of individual data events, not populations.</li> <li>Algebraic Inference: Utilizes gnostic algebra and error geometry for robust, interpretable learning.</li> <li>Resilient Architecture: Designed to be robust against outliers, corrupted data, and distributional shifts.</li> </ul>"},{"location":"magnet/#status","title":"Status","text":"<p>Magnet is currently under development. Coming soon: Detailed documentation, architecture diagrams, and implementation guides.</p> <p>Stay tuned for updates as we bring the next generation of neural networks to Machine Gnostics!</p> <p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p>"},{"location":"mse/","title":"mean_squared_error: Mean Squared Error (MSE) Metric","text":"<p>The <code>mean_squared_error</code> function computes the mean squared error (MSE) between true and predicted values. MSE is a fundamental regression metric that measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value.</p>"},{"location":"mse/#overview","title":"Overview","text":"<p>Mean Squared Error is defined as the average of the squared differences between actual and predicted values.</p> <p>MSE is widely used in regression analysis to quantify the accuracy of predictions. Lower MSE values indicate better model performance, while higher values indicate larger errors.</p>"},{"location":"mse/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values."},{"location":"mse/#returns","title":"Returns","text":"<ul> <li>float   The average of squared differences between actual and predicted values.</li> </ul>"},{"location":"mse/#raises","title":"Raises","text":"<ul> <li>TypeError   If <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"mse/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import mean_squared_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(mean_squared_error(y_true, y_pred))  # Output: 0.375\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(mean_squared_error(y_true, y_pred))  # Output: 0.3333333333333333\n</code></pre>"},{"location":"mse/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>MSE penalizes larger errors more than MAE (mean absolute error), making it sensitive to outliers.</li> </ul>"},{"location":"mse/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"poly_reg/","title":"RobustRegressor: Polynomial Robust Regression with Machine Gnostics","text":"<p>RobustRegressor is a polynomial regression model built on the Machine Gnostics framework. It is designed for robust, interpretable regression in the presence of outliers, noise, and non-Gaussian data distributions. Unlike traditional statistical models, RobustRegressor leverages algebraic and geometric principles from Mathematical Gnostics to deliver deterministic, resilient, and event-level modeling.</p>"},{"location":"poly_reg/#overview","title":"Overview","text":"<p>The Machine Gnostics RobustRegressor fits a polynomial regression function to your data, using a gnostic-based weighting scheme to minimize the influence of outliers and corrupted samples. It iteratively optimizes regression coefficients by minimizing a custom gnostic loss (such as <code>'hi'</code> or <code>'hj'</code>), making it highly robust for real-world applications.</p> <ul> <li>Robust to Outliers: Handles heavy-tailed and non-Gaussian distributions.</li> <li>Polynomial Feature Expansion: Supports configurable polynomial degrees.</li> <li>Gnostic Loss Minimization: Iterative, event-level loss minimization.</li> <li>Custom Weighting: Dynamically adjusts sample influence.</li> <li>Early Stopping &amp; Convergence: Monitors loss and entropy for efficient training.</li> <li>mlflow Integration: For experiment tracking and deployment.</li> <li>Easy Model Persistence: Save and load models with joblib.</li> </ul>"},{"location":"poly_reg/#key-features","title":"Key Features","text":"<ul> <li>Polynomial regression (configurable degree)</li> <li>Gnostic-based iterative loss minimization</li> <li>Custom weighting and scaling strategies</li> <li>Early stopping and convergence control</li> <li>Training history tracking for analysis and visualization</li> <li>Robust to outliers and non-Gaussian noise</li> </ul>"},{"location":"poly_reg/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>degree</code> int 1 Degree of the polynomial for feature expansion (must be &gt;= 1). <code>max_iter</code> int 100 Maximum number of training iterations. <code>tol</code> float 1e-3 Convergence threshold for loss or coefficient changes. <code>mg_loss</code> str 'hi' Type of gnostic loss:<code>'hi'</code> (estimation relevance), <code>'hj'</code> (irrelevance). <code>early_stopping</code> bool or int True Enables early stopping or sets window size. <code>verbose</code> bool False Prints progress and debug information. <code>scale</code> {'auto', float} 'auto' Scaling strategy for the gnostic loss. <code>history</code> bool True Records training history at each iteration. <code>data_form</code> str 'a' Input data form:<code>'a'</code> (additive), <code>'m'</code> (multiplicative)."},{"location":"poly_reg/#attributes","title":"Attributes","text":"<ul> <li>coefficients: <code>ndarray</code>Final learned polynomial regression coefficients.</li> <li>weights: <code>ndarray</code>Final sample weights after convergence.</li> <li>_history: <code>list of dict</code>   Training history, including iteration, loss, coefficients, rentropy, and weights.</li> </ul>"},{"location":"poly_reg/#methods","title":"Methods","text":""},{"location":"poly_reg/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fits the model to training data using polynomial expansion and gnostic loss minimization.</p> <ul> <li>X: array-like, shape (n_samples,) or (n_samples, 1)Input features (numpy, pandas, or pyspark DataFrame).</li> <li>y: array-like, shape (n_samples,)   Target values.</li> </ul>"},{"location":"poly_reg/#predictx","title":"<code>predict(X)</code>","text":"<p>Predicts output values for new input samples using the trained model.</p> <ul> <li>X: array-like, shape (n_samples,) or (n_samples, 1)Input features for prediction.</li> <li>Returns:   <code>y_pred</code>: ndarray, shape (n_samples,)   Predicted target values.</li> </ul>"},{"location":"poly_reg/#save_modelpath","title":"<code>save_model(path)</code>","text":"<p>Saves the trained model to disk using joblib.</p> <ul> <li>path: str   Directory path to save the model.</li> </ul>"},{"location":"poly_reg/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Loads a previously saved model from disk.</p> <ul> <li>path: strDirectory path where the model is saved.</li> <li>Returns:   Instance of <code>RobustRegressor</code> with loaded parameters.</li> </ul>"},{"location":"poly_reg/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.models import RobustRegressor\n\n# Initialize the model\nmodel = RobustRegressor(degree=2, mg_loss='hi', verbose=True)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Access coefficients and weights\nprint(\"Coefficients:\", model.coefficients)\nprint(\"Weights:\", model.weights)\n\n# Save the model\nmodel.save_model(\"./my_model\")\n\n# Load the model\nloaded = RobustRegressor.load_model(\"./my_model\")\ny_pred2 = loaded.predict(X_test)\n</code></pre>"},{"location":"poly_reg/#training-history","title":"Training History","text":"<p>The model records training history at each iteration, accessible via <code>model._history</code>.Each entry contains:</p> <ul> <li><code>iteration</code>: Iteration number</li> <li><code>h_loss</code>: Gnostic loss value</li> <li><code>coefficients</code>: Regression coefficients at this iteration</li> <li><code>rentropy</code>: Rentropy value</li> <li><code>weights</code>: Sample weights at this iteration</li> </ul> <p>This enables detailed analysis and visualization of the training process.</p>"},{"location":"poly_reg/#notes","title":"Notes","text":"<ul> <li>The model is robust to outliers and suitable for datasets with non-Gaussian noise.</li> <li>Supports integration with mlflow for experiment tracking and deployment.</li> <li>For more information, visit: https://machinegnostics.info/</li> <li>Source code: https://github.com/MachineGnostics/machinegnostics</li> </ul>"},{"location":"poly_reg/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p> <p>Author: Nirmal Parmar Date: 2025-10-01</p>"},{"location":"precision/","title":"precision_score: Classification Precision Metric","text":"<p>The <code>precision_score</code> function computes the precision of classification models, supporting both binary and multiclass settings. Precision measures the proportion of positive identifications that were actually correct, making it a key metric for evaluating classifiers, especially when the cost of false positives is high.</p>"},{"location":"precision/#overview","title":"Overview","text":"<p>Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP).</p> <p>This metric is especially important in scenarios where false positives are more costly than false negatives (e.g., spam detection, medical diagnosis).</p>"},{"location":"precision/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred."},{"location":"precision/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the precision for each class as an array.</li> </ul>"},{"location":"precision/#returns","title":"Returns","text":"<ul> <li>precision: <code>float</code> or <code>array of floats</code>   Precision score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of precision values for each class.</li> </ul>"},{"location":"precision/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"precision/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import precision_score\n\n# Example 1: Macro-averaged precision for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(precision_score(y_true, y_pred, average='macro'))  # Output: 0.8333333333333333\n\n# Example 2: Binary precision with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(precision_score(df['true'], df['pred'], average='binary'))  # Output: 0.6666666666666666\n</code></pre>"},{"location":"precision/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"precision/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"r2_score/","title":"robr2: Robust R-squared (RobR2) Metric","text":"<p>The <code>robr2</code> function computes the Robust R-squared (RobR2) value for evaluating the goodness of fit between observed data and model predictions. Unlike the classical R-squared metric, RobR2 is robust to outliers and incorporates sample weights, making it ideal for noisy or irregular datasets.</p>"},{"location":"r2_score/#overview","title":"Overview","text":"<p>Robust R-squared (RobR2) measures the proportion of variance in the observed data explained by the fitted data, while reducing sensitivity to outliers. This is achieved by using a weighted formulation, which allows for more reliable model evaluation in real-world scenarios where data may not be perfectly clean.</p>"},{"location":"r2_score/#formula","title":"Formula","text":"\\[ \\text{RobR2} = 1 - \\frac{\\sum_i w_i (e_i - \\bar{e})^2}{\\sum_i w_i (y_i - \\bar{y})^2} \\] <p>Where:</p> <ul> <li>\\(e_i = y_i - \\hat{y}_i\\) (residuals)</li> <li>\\(\\bar{e}\\) = weighted mean of residuals</li> <li>\\(\\bar{y}\\) = weighted mean of observed data</li> <li>\\(w_i\\) = weight for each data point</li> </ul> <p>If weights are not provided, equal weights are assumed.</p>"},{"location":"r2_score/#parameters","title":"Parameters","text":"Parameter Type Description <code>y</code> np.ndarray Observed data (ground truth). 1D array of numerical values. <code>y_fit</code> np.ndarray Fitted data (model predictions). 1D array, same shape as <code>y</code>. <code>w</code> np.ndarray or None Optional weights for data points. 1D array, same shape as <code>y</code>. If None, equal weights are used."},{"location":"r2_score/#returns","title":"Returns","text":"<ul> <li>float   The computed Robust R-squared (RobR2) value. Ranges from 0 (no explanatory power) to 1 (perfect fit).</li> </ul>"},{"location":"r2_score/#raises","title":"Raises","text":"<ul> <li>ValueError</li> <li>If <code>y</code> and <code>y_fit</code> do not have the same shape.</li> <li>If <code>w</code> is provided and does not have the same shape as <code>y</code>.</li> <li>If <code>y</code> or <code>y_fit</code> are not 1D arrays.</li> </ul>"},{"location":"r2_score/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.metrics import robr2\n\ny = np.array([1.0, 2.0, 3.0, 4.0])\ny_fit = np.array([1.1, 1.9, 3.2, 3.8])\nw = np.array([1.0, 1.0, 1.0, 1.0])\n\nresult = robr2(y, y_fit, w)\nprint(result)  # Example output: 0.98\n</code></pre>"},{"location":"r2_score/#comparison-with-classical-r-squared","title":"Comparison with Classical R-squared","text":"<ul> <li>Classical R-squared: Assumes equal weights and is sensitive to outliers.</li> <li>RobR2: Incorporates weights and is robust to outliers, making it more reliable for datasets with irregularities or noise.</li> </ul>"},{"location":"r2_score/#references","title":"References","text":"<ul> <li>Kovanic P., Humber M.B (2015) The Economics of Information - Mathematical Gnostics for Data Analysis, Chapter 19</li> </ul>"},{"location":"r2_score/#notes","title":"Notes","text":"<ul> <li>If weights are not provided, the metric defaults to equal weighting for all data points.</li> <li>RobR2 is particularly useful for robust regression and model evaluation in the presence of outliers.</li> </ul>"},{"location":"r2_score/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"recall/","title":"recall_score: Classification Recall Metric","text":"<p>The <code>recall_score</code> function computes the recall of classification models, supporting both binary and multiclass settings. Recall measures the proportion of actual positives that were correctly identified, making it a key metric for evaluating classifiers, especially when the cost of false negatives is high.</p>"},{"location":"recall/#overview","title":"Overview","text":"<p>Recall is defined as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).</p> <p>This metric is especially important in scenarios where false negatives are more costly than false positives (e.g., disease screening, fraud detection).</p>"},{"location":"recall/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y_true</code> array-like or pandas Series \u2014 Ground truth (correct) target values. Shape: (n_samples,) <code>y_pred</code> array-like or pandas Series \u2014 Estimated target values as returned by a classifier. Shape: (n_samples,) <code>average</code> {'binary', 'micro', 'macro', 'weighted', None} 'binary' Determines the type of averaging performed on the data. See below for details. <code>labels</code> array-like or None None List of labels to include. If None, uses sorted unique labels from y_true and y_pred."},{"location":"recall/#averaging-options","title":"Averaging Options","text":"<ul> <li>'binary': Only report results for the positive class (default for binary classification).</li> <li>'micro': Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>'macro': Calculate metrics for each label, and find their unweighted mean.</li> <li>'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).</li> <li>None: Return the recall for each class as an array.</li> </ul>"},{"location":"recall/#returns","title":"Returns","text":"<ul> <li>recall: <code>float</code> or <code>array of floats</code>   Recall score(s). Returns a float if <code>average</code> is not None, otherwise returns an array of recall values for each class.</li> </ul>"},{"location":"recall/#raises","title":"Raises","text":"<ul> <li>ValueError </li> <li>If <code>y_true</code> or <code>y_pred</code> is a pandas DataFrame (must select a column).</li> <li>If the shapes of <code>y_true</code> and <code>y_pred</code> do not match.</li> <li>If <code>average='binary'</code> but the problem is not binary classification.</li> <li>If <code>average</code> is not a recognized option.</li> </ul>"},{"location":"recall/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import recall_score\n\n# Example 1: Macro-averaged recall for multiclass\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0]\nprint(recall_score(y_true, y_pred, average='macro'))  # Output: 0.8333333333333333\n\n# Example 2: Binary recall with pandas Series\nimport pandas as pd\ndf = pd.DataFrame({'true': [1, 0, 1], 'pred': [1, 1, 1]})\nprint(recall_score(df['true'], df['pred'], average='binary'))  # Output: 1.0\n</code></pre>"},{"location":"recall/#notes","title":"Notes","text":"<ul> <li>The function supports input as numpy arrays, lists, or pandas Series.</li> <li>If you pass a pandas DataFrame, you must select a column (e.g., <code>df['col']</code>), not the whole DataFrame.</li> <li>For binary classification, by convention, the second label is treated as the positive class.</li> <li>For imbalanced datasets, consider using <code>average='weighted'</code> to account for class support.</li> </ul>"},{"location":"recall/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"rmse/","title":"root_mean_squared_error: Root Mean Squared Error (RMSE) Metric","text":"<p>The <code>root_mean_squared_error</code> function computes the Root Mean Squared Error (RMSE) between true and predicted values. RMSE is a widely used regression metric that measures the square root of the average of the squared differences between predicted and actual values.</p>"},{"location":"rmse/#overview","title":"Overview","text":"<p>Root Mean Squared Error is defined as the square root of the mean squared error.</p> <p>RMSE provides an interpretable measure of prediction error in the same units as the target variable. Lower RMSE values indicate better model performance.</p>"},{"location":"rmse/#parameters","title":"Parameters","text":"Parameter Type Description <code>y_true</code> array-like True values (targets). <code>y_pred</code> array-like Predicted values."},{"location":"rmse/#returns","title":"Returns","text":"<ul> <li>float   The square root of the average of squared errors between actual and predicted values.</li> </ul>"},{"location":"rmse/#raises","title":"Raises","text":"<ul> <li>TypeError   If <code>y_true</code> or <code>y_pred</code> are not array-like (list, tuple, or numpy array).</li> <li>ValueError   If inputs have mismatched shapes or are empty.</li> </ul>"},{"location":"rmse/#example-usage","title":"Example Usage","text":"<pre><code>from machinegnostics.metrics import root_mean_squared_error\n\n# Example 1: Using lists\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(root_mean_squared_error(y_true, y_pred))  # Output: 0.6123724356957945\n\n# Example 2: Using numpy arrays\nimport numpy as np\ny_true = np.array([1, 2, 3])\ny_pred = np.array([1, 2, 2])\nprint(root_mean_squared_error(y_true, y_pred))  # Output: 0.5773502691896257\n</code></pre>"},{"location":"rmse/#notes","title":"Notes","text":"<ul> <li>The function supports input as lists, tuples, or numpy arrays.</li> <li>Both <code>y_true</code> and <code>y_pred</code> must have the same shape and must not be empty.</li> <li>RMSE is sensitive to outliers due to the squaring of errors.</li> <li>RMSE is in the same units as the target variable, making it easy to interpret.</li> </ul>"},{"location":"rmse/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"train_test_split/","title":"train_test_split: Random Train/Test Data Splitter","text":"<p>The <code>train_test_split</code> function provides a simple and flexible way to split your dataset into random training and testing subsets. It is compatible with numpy arrays and can also handle lists or tuples as input. This function is essential for evaluating machine learning models on unseen data and is a core utility in most ML workflows.</p>"},{"location":"train_test_split/#overview","title":"Overview","text":"<p>Splitting your data into training and testing sets is a fundamental step in machine learning. The <code>train_test_split</code> function allows you to:</p> <ul> <li>Randomly partition your data into train and test sets.</li> <li>Specify the proportion or absolute number of test samples.</li> <li>Shuffle your data for unbiased splitting.</li> <li>Use a random seed for reproducibility.</li> <li>Split both features (<code>X</code>) and targets (<code>y</code>) in a consistent manner.</li> </ul>"},{"location":"train_test_split/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>X</code> array-like \u2014 Feature data to be split. Must be indexable and of consistent length. <code>y</code> array-like or None None Target data to be split alongside X. Must be same length as X. <code>test_size</code> float or int 0.25 If float, fraction of data for test set (0.0 &lt; test_size &lt; 1.0). If int, absolute number of test samples. <code>shuffle</code> bool True Whether to shuffle the data before splitting. <code>random_seed</code> int or None None Controls the shuffling for reproducibility."},{"location":"train_test_split/#returns","title":"Returns","text":"<ul> <li> <p>X_train, X_test: <code>np.ndarray</code>   Train-test split of X.</p> </li> <li> <p>y_train, y_test: <code>np.ndarray</code> or <code>None</code>   Train-test split of y. If y is None, these will also be None.</p> </li> </ul>"},{"location":"train_test_split/#raises","title":"Raises","text":"<ul> <li> <p>ValueError   If inputs are invalid or <code>test_size</code> is not appropriate.</p> </li> <li> <p>TypeError   If <code>test_size</code> is not a float or int.</p> </li> </ul>"},{"location":"train_test_split/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom machinegnostics.models import train_test_split\n\n# Create sample data\nX = np.arange(20).reshape(10, 2)\ny = np.arange(10)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, shuffle=True, random_seed=42\n)\n\nprint(\"X_train:\", X_train)\nprint(\"X_test:\", X_test)\nprint(\"y_train:\", y_train)\nprint(\"y_test:\", y_test)\n</code></pre>"},{"location":"train_test_split/#notes","title":"Notes","text":"<ul> <li>If <code>y</code> is not provided, only <code>X</code> will be split and <code>y_train</code>, <code>y_test</code> will be <code>None</code>.</li> <li>If <code>test_size</code> is a float, it must be between 0.0 and 1.0 (exclusive).</li> <li>If <code>test_size</code> is an int, it must be between 1 and <code>len(X) - 1</code>.</li> <li>Setting <code>shuffle=False</code> will split the data in order, without randomization.</li> <li>Use <code>random_seed</code> for reproducible splits.</li> </ul>"},{"location":"train_test_split/#license","title":"License","text":"<p>Machine Gnostics - Machine Gnostics Library Copyright (C) 2025  Machine Gnostics Team</p> <p>This work is licensed under the terms of the GNU General Public License version 3.0.</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide provides a comprehensive overview of how to use the Machine Gnostics library for robust data analysis and machine learning based on Machine Gnostics principles. Machine Gnostics offers robust regression models, gnostic metrics, and alternative statistical tools designed to be resilient to outliers and corrupted data.</p>"},{"location":"usage/#1-importing-machine-gnostics","title":"1. Importing Machine Gnostics","text":"<p>After installation, you can import Machine Gnostics and its modules in your Python scripts or notebooks:</p> <pre><code>import machinegnostics as mg\nfrom machinegnostics.models import RobustRegressor\nfrom machinegnostics.metrics import robr2, gmmfe, divI, evalMet, hc\nfrom machinegnostics.magcal import gmedian, gvariance, gautocovariance, gcorrelation, gcovariance\n</code></pre>"},{"location":"usage/#2-robust-regression","title":"2. Robust Regression","text":"<p>Machine Gnostics provides robust regression models that are less sensitive to outliers.</p> <p>Example: Using <code>RobustRegressor</code></p> <pre><code>from machinegnostics.models import RobustRegressor\n\n# X: feature matrix, y: target vector\nmodel = RobustRegressor()\nmodel.fit(X, y)\ny_pred = model.predict(X_test)\n</code></pre>"},{"location":"usage/#3-gnostic-metrics","title":"3. Gnostic Metrics","text":"<p>Evaluate your models with robust, gnostic metrics:</p> <pre><code>from machinegnostics.metrics import robr2, gmmfe\n\nscore = robr2(y_true, y_pred)\ngmmfe = gmmfe(y_true, y_pred)\nhc = hc(y_true, y_pred, case='i') # estimating case\n</code></pre> <p>Other available metrics:</p> <ul> <li><code>divI</code>: Divergence Index</li> <li><code>evalMet</code>: General evaluation metric</li> <li><code>hc</code>: Relavance of the given data samples</li> </ul>"},{"location":"usage/#4-gnostic-statistical-tools","title":"4. Gnostic Statistical Tools","text":"<p>Machine Gnostics includes robust alternatives to classical statistics:</p> <pre><code>gmed = gmedian(data)\ngmod = gmodulus(data)\ngvar = gvariance(data)\ngacov = gautocovariance(data1, data2)\ngcor = gcorrelation(data1, data2)\ngcov = gcovariance(data1, data2)\n</code></pre>"},{"location":"usage/#5-example-workflow","title":"5. Example Workflow","text":"<pre><code>import numpy as np\nfrom machinegnostics.models import RobustRegressor\nfrom machinegnostics.metrics import robr2\n\n# Generate synthetic data\nX = np.random.randn(100, 3)\ny = 2 * X[:, 0] - X[:, 1] + np.random.randn(100) * 0.5\n\n# Fit robust regression\nmodel = RobustRegressor()\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\n# Evaluate\nscore = robr2(y, y_pred)\nprint(\"Robust R2:\", score)\n</code></pre>"},{"location":"usage/#6-troubleshooting","title":"6. Troubleshooting","text":"<ul> <li>ImportError: Ensure Machine Gnostics is installed and your <code>PYTHONPATH</code> includes the <code>src</code> directory.</li> <li>Unexpected Results: Check for outliers or corrupted data in your input.</li> </ul> <p>For further help, open an issue on GitHub or contact the maintainers.</p>"}]}